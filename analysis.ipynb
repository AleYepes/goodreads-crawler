{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import numpy as np\n",
    "\n",
    "# book_df = pd.read_csv(\"data/books.csv\")\n",
    "# book_df['book_id'] = book_df['book_id'].astype('UInt32')\n",
    "# book_df['review_count'] = book_df['review_count'].astype('UInt32')\n",
    "# book_df['num_pages'] = book_df['num_pages'].astype('UInt16')\n",
    "# book_df['year'] = book_df['year'].astype('Int16')\n",
    "# star_cols = [col for col in book_df.columns if col.endswith('star')]\n",
    "# for col in star_cols:\n",
    "#     book_df[col] = book_df[col].astype('UInt32')\n",
    "\n",
    "# test = book_df[~book_df['similar_books'].isna()]\n",
    "# test = test[~test['genres'].isna()]\n",
    "# test.to_csv('data/books.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "books_df = pd.read_csv(\"data/books.csv\")\n",
    "numeric_map = {\n",
    "    'book_id': 'UInt32',\n",
    "    'review_count': 'UInt32',\n",
    "    'num_pages': 'UInt16',\n",
    "    'year': 'Int16',\n",
    "}\n",
    "for col, dtype in numeric_map.items():\n",
    "    books_df[col] = books_df[col].astype(dtype)\n",
    "\n",
    "star_cols = [c for c in books_df.columns if c.endswith('star')]\n",
    "for col in star_cols:\n",
    "    books_df[col] = books_df[col].astype('UInt32')    \n",
    "# books_df['rating_count'] = books_df[star_cols].sum(axis=1)\n",
    "\n",
    "\n",
    "# Todo: Fix the crawler to split langs with \"|\" by default\n",
    "books_df['lang'] = [\n",
    "    \"|\".join(item.strip() for item in x.split(\";\")) if isinstance(x, str) else x \n",
    "    for x in books_df['lang']\n",
    "]\n",
    "books_df['description'] = books_df['description'].str.replace('\\n\\n', '\\n')\n",
    "books_df['description'] = books_df['description'].str.replace('\\n', ' ')\n",
    "\n",
    "# Goodreads export\n",
    "goodreads_df = pd.read_csv('data/goodreads_library_export.csv')\n",
    "goodreads_df['book_id'] = goodreads_df['book_id'].astype('UInt32')\n",
    "goodreads_df['my_rating'] = goodreads_df['my_rating'].astype('UInt8')\n",
    "\n",
    "books_df = books_df.merge(goodreads_df[['book_id', 'my_rating']], on='book_id', how='left')\n",
    "books_df['my_rating'] = books_df['my_rating'].replace(0, np.nan)\n",
    "\n",
    "# books_df = books_df[~books_df['genres'].isna()].reset_index(drop=True)\n",
    "books_df = books_df[~books_df['similar_books'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep embedding strings\n",
    "def format_string_for_embedding(items, kind=None, truncate=0):\n",
    "    if not isinstance(items, (list)) or len(items) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    n = len(items)\n",
    "    if n == 1:\n",
    "        res = items[0]\n",
    "    elif n > truncate > 1:\n",
    "        res = f\"{', '.join(items[:truncate])}, and {items[truncate]}\"\n",
    "    else:\n",
    "        res = f\"{', '.join(items[:-1])}{',' if n > 2 else ''} and {items[-1]}\"\n",
    "    \n",
    "    prefix = f\"{kind.capitalize()}{'s' if n > 1 else ''}: \" if kind else \"\"\n",
    "    return f\"{prefix}{res}\"\n",
    "\n",
    "books_df['authors_post'] = books_df['authors'].str.split('|')\n",
    "books_df['authors_post'] = books_df['authors_post'].apply(lambda x:format_string_for_embedding(x, truncate=4))\n",
    "\n",
    "books_df['genres_post'] = books_df['genres'].str.split('|')\n",
    "books_df['genres_post'] = books_df['genres_post'].apply(lambda x:format_string_for_embedding(x, kind='genre'))\n",
    "\n",
    "# books_df['lang_post'] = books_df['lang'].str.split('|')\n",
    "# books_df['lang_post'] = books_df['lang_post'].apply(lambda x:format_string_for_embedding(x, kind='language'))\n",
    "\n",
    "books_df['desc_post'] = [[desc] if isinstance(desc, str) else [] for desc in books_df['description']]\n",
    "books_df['desc_post'] = books_df['desc_post'].apply(lambda x:format_string_for_embedding(x, kind='description'))\n",
    "\n",
    "def join_embedding_parts(title, authors, genres, desc):\n",
    "    text = f\"Book: {title}\\n\"\n",
    "    if authors:\n",
    "        text += f\"Written by: {authors}\\n\"\n",
    "    if genres:\n",
    "        text += f\"{genres}\\n\"\n",
    "    if desc:\n",
    "        text += f\"{desc}\" \n",
    "    return text\n",
    "\n",
    "embedding_strings = [\n",
    "    join_embedding_parts(t, a, g, d) \n",
    "    for t, a, g, d in zip(books_df['title'], books_df['authors_post'], books_df['genres_post'], books_df['desc_post'])\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Genre embeddings\n",
    "# import ollama\n",
    "# from tqdm import tqdm\n",
    "# import torch\n",
    "\n",
    "# OLLAMA_MODEL = \"qwen3-embedding:0.6b\"\n",
    "# MIN_DIMENSIONS = 32\n",
    "# all_genres = books_df['genres'].str.split('|').explode().str.strip().unique()\n",
    "# all_genres = [g for g in all_genres if isinstance(g, str) and len(g) > 0]\n",
    "\n",
    "# my_rating_count = (~books_df['my_rating'].isna()).sum()\n",
    "# sent_embedding_dimensions = MIN_DIMENSIONS\n",
    "# while sent_embedding_dimensions*2 < my_rating_count:\n",
    "#     sent_embedding_dimensions*=2\n",
    "\n",
    "# BATCH_SIZE = 64\n",
    "# genre_vectors = {}\n",
    "# for i in tqdm(range(0, len(all_genres), BATCH_SIZE)):\n",
    "#     batch = all_genres[i : i + BATCH_SIZE]\n",
    "#     response = ollama.embed(model=OLLAMA_MODEL, input=batch)\n",
    "#     for genre, vector in zip(batch, response['embeddings']):\n",
    "#         genre_vectors[genre] = np.array(vector)[:sent_embedding_dimensions]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed sentences\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "OLLAMA_MODEL = \"qwen3-embedding:0.6b\"\n",
    "MIN_DIMENSIONS = 32\n",
    "\n",
    "my_rating_count = (~books_df['my_rating'].isna()).sum()\n",
    "sent_embedding_dimensions = MIN_DIMENSIONS\n",
    "while sent_embedding_dimensions*2 < my_rating_count:\n",
    "    sent_embedding_dimensions*=2\n",
    "\n",
    "def get_batch_embeddings(text_list, model, batch_size=64):\n",
    "    embeddings = []\n",
    "    for i in tqdm(range(0, len(text_list), batch_size)):\n",
    "        batch = text_list[i : i + batch_size]\n",
    "        response = ollama.embed(model=model, input=batch)\n",
    "        embeddings.extend(response['embeddings'])\n",
    "    return np.array(embeddings)\n",
    "\n",
    "embeddings = get_batch_embeddings(embedding_strings, OLLAMA_MODEL)\n",
    "embeddings = torch.tensor(embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency matrix\n",
    "id_to_idx = {id: i for i, id in enumerate(books_df['book_id'])}\n",
    "\n",
    "edge_indices = []\n",
    "for idx, row in tqdm(books_df.iterrows(), total=len(books_df)):\n",
    "    current_idx = id_to_idx[row['book_id']]\n",
    "    if pd.isna(row['similar_books']):\n",
    "        continue\n",
    "    for item in row['similar_books'].split('|'):\n",
    "        try:\n",
    "            target_id = int(item.split(':')[0])\n",
    "            if target_id in books_df['book_id'].values:\n",
    "                target_idx = id_to_idx[target_id]\n",
    "                edge_indices.append([current_idx, target_idx])\n",
    "                edge_indices.append([target_idx, current_idx])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "\n",
    "if not edge_indices:\n",
    "    edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "else:\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multiplying embeddings with adjacency matrix\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=embeddings.size(0))\n",
    "edge_index_norm, edge_weight_norm = gcn_norm(edge_index_with_loops, num_nodes=embeddings.size(0))\n",
    "adj_matrix = torch.sparse_coo_tensor(edge_index_norm, edge_weight_norm, (embeddings.size(0), embeddings.size(0)))\n",
    "\n",
    "final_embeddings = embeddings\n",
    "num_propagations = 1\n",
    "for _ in range(num_propagations):\n",
    "    final_embeddings = torch.sparse.mm(adj_matrix, final_embeddings)\n",
    "final_embeddings = final_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "import umap.umap_ as umap\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "\n",
    "# Global score\n",
    "C = books_df['avg_rating'].mean()\n",
    "m = books_df['rating_count'].quantile(0.10) \n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = float(x['rating_count'])\n",
    "    R = float(x['avg_rating'])\n",
    "    if v == 0: \n",
    "        return C\n",
    "    return (v / (v + m) * R) + (m / (v + m) * C)\n",
    "books_df['global_score'] = books_df.apply(weighted_rating, axis=1)\n",
    "\n",
    "# Dimensionality Reduction\n",
    "y_umap = books_df['my_rating'].fillna(-1).values\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(final_embeddings)\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=sent_embedding_dimensions//2,\n",
    "    metric='cosine', \n",
    "    target_metric='l1',\n",
    "    target_weight=0.5,\n",
    "    random_state=42\n",
    ")\n",
    "X_reduced = reducer.fit_transform(X_scaled, y=y_umap)\n",
    "\n",
    "# Gaussian Process Regression\n",
    "train_mask = books_df['my_rating'].notna() & (books_df['my_rating'] > 0)\n",
    "if train_mask.sum() < 5:\n",
    "    print(\"Warning: Not enough ratings for GPR. Need at least 5.\")\n",
    "else:\n",
    "    X_train = X_reduced[train_mask]\n",
    "    y_train = books_df.loc[train_mask, 'my_rating'].values\n",
    "    kernel = Matern(nu=1.5) + WhiteKernel(noise_level=0.1)\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True, n_restarts_optimizer=2)\n",
    "    gpr.fit(X_train, y_train)\n",
    "    means, stds = gpr.predict(X_reduced, return_std=True)\n",
    "    books_df['pred_rating'] = means\n",
    "    books_df['uncertainty'] = stds\n",
    "\n",
    "\n",
    "books_df['safe_personal_score'] = books_df['pred_rating'] - (books_df['uncertainty'] * 0.5)\n",
    "books_df['final_score'] = (0.5 * books_df['safe_personal_score']) + (0.5 * books_df['global_score'])    \n",
    "books_df[~train_mask][['book_id', 'title', 'my_rating', 'final_score', 'pred_rating', 'uncertainty', 'genres']].sort_values(by='final_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df[~train_mask][['book_id', 'title', 'my_rating', 'final_score', 'pred_rating', 'uncertainty', 'genres']].sort_values(by='final_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from node2vec import Node2Vec\n",
    "# import networkx as nx\n",
    "\n",
    "# G = nx.Graph()\n",
    "# G.add_nodes_from(books_df['book_id'].tolist())\n",
    "\n",
    "# for idx, row in books_df.iterrows():\n",
    "#     if pd.isna(row['similar_books']): continue\n",
    "#     for item in row['similar_books'].split('|'):\n",
    "#         try:\n",
    "#             target_id = item.split(':')[0]\n",
    "#             if target_id in books_df['book_id'].values:\n",
    "#                 G.add_edge(row['book_id'], target_id, weight=1.0)\n",
    "#         except ValueError: continue\n",
    "        \n",
    "# node2vec = Node2Vec(G, dimensions=64, walk_length=80, num_walks=100, workers=8, quiet=True)\n",
    "# node2vec = Node2Vec(\n",
    "#     G, \n",
    "#     dimensions=max(MIN_DIMENSIONS, sent_embedding_dimensions//2),\n",
    "#     walk_length=len(books_df)//50,\n",
    "#     num_walks=100,\n",
    "#     p=1.0,\n",
    "#     q=0.5,\n",
    "#     workers=8, \n",
    "#     quiet=True\n",
    "# )\n",
    "# model_n2v = node2vec.fit(window=10, min_count=1)\n",
    "\n",
    "# graph_embeddings = np.array([\n",
    "#     model_n2v.wv[bid] if bid in model_n2v.wv else np.zeros(64) \n",
    "#     for bid in books_df['book_id']\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(books_df['book_id'].tolist())\n",
    "\n",
    "# 2. Build edges with correct type casting\n",
    "edge_list = []\n",
    "for idx, row in books_df.iterrows():\n",
    "    if pd.isna(row['similar_books']): continue\n",
    "    \n",
    "    for item in row['similar_books'].split('|'):\n",
    "        target_id_str = item.split(':')[0]\n",
    "        target_id = int(target_id_str)\n",
    "        \n",
    "        if target_id in books_df['book_id'].values:\n",
    "            edge_list.append((row['book_id'], target_id))\n",
    "\n",
    "G.add_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# Prepare Node Features (Genre Embeddings) and Edge Index\n",
    "node_features = torch.tensor(np.stack(genre_embeddings.values), dtype=torch.float)\n",
    "node_mapping = {node: i for i, node in enumerate(books_df['book_id'].tolist())}\n",
    "edge_indices = []\n",
    "\n",
    "for u, v in G.edges():\n",
    "    if u in node_mapping and v in node_mapping:\n",
    "        edge_indices.append([node_mapping[u], node_mapping[v]])\n",
    "        edge_indices.append([node_mapping[v], node_mapping[u]]) # Undirected\n",
    "\n",
    "if len(edge_indices) == 0:\n",
    "    edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "else:\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Define GraphSAGE Model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Layer 2\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # Optional: L2 Normalize final embeddings\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "# Generate Embeddings\n",
    "model = GraphSAGE(in_channels=node_features.shape[1], hidden_channels=sent_embedding_dimensions, out_channels=sent_embedding_dimensions)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    graph_embeddings_tensor = model(node_features, edge_index)\n",
    "\n",
    "final_embeddings = graph_embeddings_tensor.numpy()\n",
    "print(\"Embeddings shape:\", final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "import umap\n",
    "\n",
    "C = books_df['avg_rating'].mean()\n",
    "m = books_df['review_count'].quantile(0.10) # 10th percentile as minimum votes\n",
    "\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['review_count']\n",
    "    R = x['avg_rating']\n",
    "    if v == 0: return C\n",
    "    return (v/(v+m) * R) + (m/(v+m) * C)\n",
    "\n",
    "books_df['global_score'] = books_df.apply(weighted_rating, axis=1)\n",
    "\n",
    "# PART C: The Personal Score (UMAP + GPR)\n",
    "\n",
    "# 1. Semi-Supervised UMAP\n",
    "# We need a target array. -1 signals \"unlabeled\" to UMAP.\n",
    "# This warps the space so your rated books clump together based on score.\n",
    "y_umap = books_df['my_rating'].fillna(-1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(final_embeddings)\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=sent_embedding_dimensions,\n",
    "    metric='cosine', \n",
    "    target_metric='l1', # Use L1 for the ratings\n",
    "    # target_weight=0.5,  # Balance structural shape vs rating shape\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Running UMAP...\")\n",
    "X_reduced = reducer.fit_transform(X_scaled, y=y_umap)\n",
    "\n",
    "# 2. Gaussian Process Regression\n",
    "# Identify Training set (Books you read) vs Prediction set (Unread)\n",
    "train_mask = books_df['my_rating'].notna() & (books_df['my_rating'] > 0)\n",
    "X_train = X_reduced[train_mask]\n",
    "y_train = books_df.loc[train_mask, 'my_rating'].values\n",
    "\n",
    "# Kernel: Matern handles irregularities better than RBF. WhiteKernel handles noise.\n",
    "kernel = Matern(nu=1.5) + WhiteKernel(noise_level=0.1)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)\n",
    "\n",
    "print(\"Fitting GPR...\")\n",
    "gpr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on ALL books\n",
    "means, stds = gpr.predict(X_reduced, return_std=True)\n",
    "\n",
    "books_df['pred_rating'] = means\n",
    "books_df['uncertainty'] = stds\n",
    "\n",
    "# PART D: Final Hybrid Scoring\n",
    "\n",
    "# Conservative Personal Score: Prediction minus Uncertainty\n",
    "# If model thinks it's 5.0 but is unsure (std=1.0), treats it as 4.0\n",
    "books_df['safe_personal_score'] = books_df['pred_rating'] - (books_df['uncertainty'] * 0.5)\n",
    "\n",
    "# Hybrid: 70% Personal Taste, 30% Global Quality\n",
    "books_df['final_score'] = (0.7 * books_df['safe_personal_score']) + (0.3 * books_df['global_score'])\n",
    "\n",
    "# Filter out books you've already read\n",
    "recs = books_df[~train_mask].sort_values('final_score', ascending=False)\n",
    "\n",
    "# Export\n",
    "recs[['title', 'authors', 'global_score', 'pred_rating', 'uncertainty', 'final_score']].head(20).to_csv(\"data/final_recommendations.csv\", index=False)\n",
    "print(\"Done. Saved top 20 to data/final_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_recommendations(title, embeddings, df, top_k=5):\n",
    "    idx = df[df['title'] == title].index[0]\n",
    "    sim_scores = cosine_similarity([embeddings[idx]], embeddings).flatten()\n",
    "    top_indices = sim_scores.argsort()[-(top_k+1):-1][::-1]\n",
    "    \n",
    "    return df.iloc[top_indices][['title', 'authors', 'genres']]\n",
    "\n",
    "# Test it out\n",
    "# print(get_recommendations(\"The Way of Kings\", final_embeddings, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_export = pd.read_csv('data/goodreads_library_export.csv')\n",
    "# goodreads_export = pd.read_csv('data/20-01-2025_goodreads_library_export.csv')\n",
    "# goodreads_export = goodreads_export[['Book Id', 'Author', 'My Rating', 'Number of Pages', 'Original Publication Year']]\n",
    "# goodreads_export = goodreads_export.rename(columns={'Book Id':'book_id',\n",
    "#                                                     'Author': 'author',\n",
    "#                                                     'My Rating': 'my_rating',\n",
    "#                                                     'Number of Pages': 'num_pages',\n",
    "#                                                     'Original Publication Year': 'year'})\n",
    "threshold = (goodreads_export['number_of_pages'].mean() - goodreads_export['number_of_pages'].std())\n",
    "goodreads_export.loc[goodreads_export['number_of_pages'] < threshold, 'number_of_pages'] = np.nan\n",
    "\n",
    "book_df = pd.read_csv('data/books.csv')\n",
    "# book_df = pd.read_csv('data/01-2025_goodreads_scraped.csv')\n",
    "df = goodreads_export.merge(book_df, on='book_id')\n",
    "\n",
    "# Drop competing columns\n",
    "df['author'] = df['author_x'].fillna(df['author_y'])\n",
    "df['num_pages'] = df['num_pages_x'].fillna(df['num_pages_y'])\n",
    "df['year'] = df['year_x'].fillna(df['year_y'])\n",
    "df.drop(columns=['author_x', 'author_y', 'num_pages_x', 'num_pages_y', 'year_x', 'year_y'], inplace=True)\n",
    "\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df['year'] = df['year'].fillna(df['year'].mean()).round().astype(int)\n",
    "df['num_pages'] = df['num_pages'].fillna(df['num_pages'].mean()).round().astype(int)\n",
    "df['num_reviews'] = df['num_reviews'].fillna(0).round().astype(int)\n",
    "df['my_rating'] = df['my_rating'].replace(0,np.nan)\n",
    "\n",
    "df['age'] = int(datetime.now().strftime('%Y')) - df['year']\n",
    "df['average_rating'] = ((df['5 stars'] * 5) + (df['4 stars'] * 4) + (df['3 stars'] * 3) + (df['2 stars'] * 2) + df['1 star']) / df['num_ratings']\n",
    "df = df[['book_id', 'title', 'author', 'year', 'age', 'series', 'num_pages', 'genres', 'num_ratings', 'num_reviews', 'my_rating', 'average_rating', '5 stars', '4 stars', '3 stars', '2 stars', '1 star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quadratic(row):\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    a, b, c = np.polyfit(x, row, 2)\n",
    "    return pd.Series([a, b, c])\n",
    "\n",
    "# Calculating quadrdic modeling coefficients\n",
    "df['1_star_percentage'] = df['1 star'] / df['num_ratings']\n",
    "df['2_star_percentage'] = df['2 stars'] / df['num_ratings']\n",
    "df['3_star_percentage'] = df['3 stars'] / df['num_ratings']\n",
    "df['4_star_percentage'] = df['4 stars'] / df['num_ratings']\n",
    "df['5_star_percentage'] = df['5 stars'] / df['num_ratings']\n",
    "coefficients = df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "df['a'], df['b'], df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "\n",
    "# Pre-processing columns for rankings\n",
    "df['num_ratings_ln'] = np.log1p(df['num_ratings'])\n",
    "df['num_pages_ln'] = np.log1p(df['num_pages'])\n",
    "df['2a_shifted'] = df['a'] - df['a'].min()\n",
    "df['2a_shifted'] = df['2a_shifted'] * (1 / df['2a_shifted'].max()) + 1\n",
    "df['b_shifted'] = df['b'] - df['b'].min()\n",
    "df['b_shifted'] = df['b_shifted'] * (1 / df['b_shifted'].max()) + 1\n",
    "df['c_shifted'] = df['c'] - df['c'].min()\n",
    "df['c_shifted'] = df['c_shifted'] * (1 / df['c_shifted'].max()) + 1\n",
    "\n",
    "# Types of rankings\n",
    "df['num_adjusted_rating'] = df['average_rating'] - (df['average_rating'] - df['average_rating'].mean()) / df['num_ratings_ln']\n",
    "df['coeff_2a_rating'] = (df['num_adjusted_rating'] * df['2a_shifted'])\n",
    "df['coeff_b_rating'] = (df['num_adjusted_rating']) / (df['b_shifted'])\n",
    "df['coeff_c_rating'] = (df['num_adjusted_rating'] * df['c_shifted'])\n",
    "df['joined_rating'] = (df['num_adjusted_rating'] * df['c_shifted'] * df['2a_shifted']) / df['b_shifted']\n",
    "df['final_rating'] = df['joined_rating'] - (df['joined_rating'] - df['joined_rating'].mean()) / df['num_ratings_ln']\n",
    "\n",
    "df['num_adjusted_page_rating'] = df['num_adjusted_rating'] / (df['num_pages_ln'])\n",
    "df['coeff_2a_page_rating'] = df['coeff_2a_rating'] / df['num_pages_ln']\n",
    "df['coeff_b_page_rating'] = df['coeff_b_rating'] / df['num_pages_ln']\n",
    "df['coeff_c_page_rating'] = df['coeff_c_rating'] / df['num_pages_ln']\n",
    "df['joined_page_rating'] = df['joined_rating'] / df['num_pages_ln']\n",
    "df['final_page_rating'] = df['joined_page_rating'] - (df['joined_page_rating'] - df['joined_page_rating'].mean()) / df['num_ratings_ln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['age', 'num_pages', 'num_pages_ln', 'num_ratings', 'num_ratings_ln', 'num_reviews', 'my_rating', 'average_rating', '1 star', '2 stars', '3 stars', '4 stars', '5 stars', '1_star_percentage', '2_star_percentage', '3_star_percentage', '4_star_percentage', '5_star_percentage', 'a', 'b', 'c', 'num_adjusted_rating', 'coeff_2a_rating', 'coeff_b_rating', 'coeff_c_rating', 'joined_rating', 'final_rating', 'num_adjusted_page_rating', 'coeff_2a_page_rating', 'coeff_b_page_rating', 'coeff_c_page_rating', 'joined_page_rating', 'final_page_rating']\n",
    "corr_df= df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 15)) \n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Correlation Heatmap') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = df.sort_values(by='final_page_rating', ascending=False).reset_index().drop('index', axis=1)\n",
    "fresh = fresh[fresh['my_rating'].isna()]\n",
    "fresh[['Fiction' in genre_list for genre_list in fresh['genres']]] # Fiction, Nonfiction, Memoir, Classics, History, Politics, Philosophy, Business"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
