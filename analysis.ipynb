{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import os\n",
    "\n",
    "def get_expected_score(ra, rb):\n",
    "    return 1 / (1 + 10 ** ((rb - ra) / 400))\n",
    "\n",
    "def update_elo(ra, rb, result, k=32):\n",
    "    expected_a = get_expected_score(ra, rb)\n",
    "    return ra + k * (result - expected_a), rb + k * ((1 - result) - (1 - expected_a))\n",
    "\n",
    "def run_interactive_ranking(elo_df, titles, star_rating):\n",
    "    bucket = elo_df[elo_df['original_rating'] == star_rating].index.tolist()\n",
    "    if len(bucket) < 2: return elo_df\n",
    "\n",
    "    print(f\"--- Ranking {star_rating}-Star Books ({len(bucket)} books) ---\")\n",
    "    while True:\n",
    "        try:\n",
    "            # Weighted random choice favoring fewer matches\n",
    "            weights = 1 / (elo_df.loc[bucket, 'matches_played'] + 1)\n",
    "            idx_a, idx_b = random.choices(bucket, weights=weights, k=2)\n",
    "            if idx_a == idx_b: continue\n",
    "            \n",
    "            choice = input(f\"[1]{titles.get(elo_df.at[idx_a, 'book_id'], 'Unknown')} > [2]{titles.get(elo_df.at[idx_b, 'book_id'], 'Unknown')}\").strip().lower()\n",
    "\n",
    "            if choice == 'q': break\n",
    "            if choice not in ['1', '2']: continue\n",
    "\n",
    "            # Update State\n",
    "            elo_df.loc[[idx_a, idx_b], 'matches_played'] += 1\n",
    "            ra, rb = elo_df.at[idx_a, 'elo_score'], elo_df.at[idx_b, 'elo_score']\n",
    "            new_ra, new_rb = update_elo(ra, rb, 1 if choice == '1' else 0)\n",
    "            elo_df.at[idx_a, 'elo_score'], elo_df.at[idx_b, 'elo_score'] = new_ra, new_rb\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            break\n",
    "    return elo_df\n",
    "\n",
    "def compute_continuous(elo_df):\n",
    "    \"\"\"Normalizes Elo within star buckets to 0.0-0.99 range.\"\"\"\n",
    "    results = pd.Series(index=elo_df['book_id'], dtype=float)\n",
    "    for stars in elo_df['original_rating'].dropna().unique():\n",
    "        mask = elo_df['original_rating'] == stars\n",
    "        subset = elo_df.loc[mask, 'elo_score']\n",
    "\n",
    "        target_ids = elo_df.loc[mask, 'book_id']\n",
    "\n",
    "        if len(subset) > 1 and subset.max() > subset.min():\n",
    "            norm = (subset - subset.min()) / (subset.max() - subset.min())\n",
    "            results.loc[target_ids] = stars + (norm * 0.99).values\n",
    "        else:\n",
    "            results.loc[target_ids] = stars + 0.5\n",
    "    return results\n",
    "\n",
    "def refine_ratings(target_df, rating_col, title_col='title', elo_path='data/elo_ratings.csv'):\n",
    "    target_clean = target_df.dropna(subset=[rating_col]).copy()\n",
    "    if os.path.exists(elo_path):\n",
    "        elo_df = pd.read_csv(elo_path)\n",
    "    else:\n",
    "        elo_df = pd.DataFrame(columns=['book_id', 'original_rating', 'elo_score', 'matches_played'])\n",
    "    elo_df = elo_df.set_index('book_id')\n",
    "    \n",
    "    common_books = elo_df.index.intersection(target_clean['book_id'])\n",
    "    elo_df.loc[common_books, 'original_rating'] = target_clean.set_index('book_id').loc[common_books, rating_col]\n",
    "\n",
    "    # Add new books\n",
    "    new_books = target_clean[~target_clean['book_id'].isin(elo_df.index)]\n",
    "    new_entries = pd.DataFrame({\n",
    "        'original_rating': new_books[rating_col].values,\n",
    "        'elo_score': 1200.0,\n",
    "        'matches_played': 0\n",
    "    }, index=new_books['book_id'])\n",
    "    elo_df = pd.concat([elo_df, new_entries]).reset_index().rename(columns={'index': 'book_id'})\n",
    "\n",
    "    # 3. Interactive Ranking\n",
    "    if input(f\"Rank '{rating_col}' buckets? (y/n): \").lower() == 'y':\n",
    "        titles = dict(zip(target_df['book_id'], target_df[title_col]))\n",
    "        for star in sorted(target_clean[rating_col].unique(), reverse=True):\n",
    "            elo_df = run_interactive_ranking(elo_df, titles, star)\n",
    "        elo_df.to_csv(elo_path, index=False)\n",
    "\n",
    "    return compute_continuous(elo_df)\n",
    "\n",
    "books_df = pd.read_csv(\"data/books.csv\")\n",
    "books_df['description'] = books_df['description'].fillna('').astype(str).str.replace(r'\\s+', ' ', regex=True).str.strip()\n",
    "books_df['lang'] = books_df['lang'].apply(lambda x: \"|\".join(i.strip() for i in x.split(\";\")) if isinstance(x, str) else x)\n",
    "\n",
    "gr_export = pd.read_csv('data/goodreads_library_export.csv')\n",
    "gr_export['my_rating'] = gr_export['my_rating'].astype('UInt8').replace(0, np.nan)\n",
    "\n",
    "my_refined = refine_ratings(gr_export, 'my_rating')\n",
    "books_df = books_df.merge(my_refined.rename('my_rating'), left_on='book_id', right_index=True, how='left')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep embedding strings\n",
    "def format_string_for_embedding(items, kind=None, truncate=0):\n",
    "    if not isinstance(items, (list)) or len(items) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    n = len(items)\n",
    "    if n == 1:\n",
    "        res = items[0]\n",
    "    elif n > truncate > 1:\n",
    "        res = f\"{', '.join(items[:truncate])}, and {items[truncate]}\"\n",
    "    else:\n",
    "        res = f\"{', '.join(items[:-1])}{',' if n > 2 else ''} and {items[-1]}\"\n",
    "    \n",
    "    prefix = f\"{kind.capitalize()}{'s' if n > 1 else ''}: \" if kind else \"\"\n",
    "    return f\"{prefix}{res}\"\n",
    "\n",
    "books_df['authors_post'] = books_df['authors'].str.split('|')\n",
    "books_df['authors_post'] = books_df['authors_post'].apply(lambda x:format_string_for_embedding(x, truncate=4))\n",
    "\n",
    "books_df['genres_post'] = books_df['genres'].str.split('|')\n",
    "books_df['genres_post'] = books_df['genres_post'].apply(lambda x:format_string_for_embedding(x, kind='genre'))\n",
    "\n",
    "books_df['desc_post'] = [[desc] if isinstance(desc, str) else [] for desc in books_df['description']]\n",
    "books_df['desc_post'] = books_df['desc_post'].apply(lambda x:format_string_for_embedding(x, kind='description'))\n",
    "\n",
    "def join_embedding_parts(title, authors, genres, desc):\n",
    "    text = f\"Book: {title}\\n\"\n",
    "    if authors:\n",
    "        text += f\"Written by: {authors}\\n\"\n",
    "    if genres:\n",
    "        text += f\"{genres}\\n\"\n",
    "    if desc:\n",
    "        text += f\"{desc}\" \n",
    "    return text\n",
    "\n",
    "books_df['embedding_input'] = [\n",
    "    join_embedding_parts(t, a, g, d) \n",
    "    for t, a, g, d in zip(books_df['title'], books_df['authors_post'], books_df['genres_post'], books_df['desc_post'])\n",
    "]\n",
    "id_to_string = books_df.set_index('book_id')['embedding_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed sentences\n",
    "import os\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "PARAMS = 8\n",
    "OLLAMA_MODEL = f\"qwen3-embedding:{PARAMS}b\"\n",
    "MIN_DIMENSIONS = 32\n",
    "\n",
    "embeddings_path = f'data/{PARAMS}b_embeddings.csv'\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "if os.path.exists(embeddings_path):\n",
    "    embeddings = pd.read_csv(embeddings_path, index_col='book_id')\n",
    "    embedding_cols = embeddings.columns.astype(int)\n",
    "    loaded_book_ids = embeddings.index.values\n",
    "    embeddings = torch.tensor(embeddings.values, dtype=torch.float32, device=device)\n",
    "else:\n",
    "    loaded_book_ids = np.array([])\n",
    "    embeddings = None\n",
    "\n",
    "current_book_ids = books_df['book_id'].values\n",
    "missing_mask = ~np.isin(current_book_ids, loaded_book_ids)\n",
    "missing_ids = current_book_ids[missing_mask]\n",
    "if len(missing_ids) > 0:\n",
    "    missing_strings = id_to_string.loc[missing_ids].tolist()\n",
    "    batch_size = 128\n",
    "    new_embeddings = []\n",
    "    for i in tqdm(range(0, len(missing_strings), batch_size)):\n",
    "        batch = missing_strings[i : i + batch_size]\n",
    "        response = ollama.embed(model=OLLAMA_MODEL, input=batch)\n",
    "        batch_embeddings = torch.tensor(response['embeddings'], dtype=torch.float32, device=device)\n",
    "        new_embeddings.append(batch_embeddings)\n",
    "\n",
    "    new_embeddings = torch.cat(new_embeddings, dim=0)\n",
    "    if embeddings is not None:\n",
    "        dim = max(embeddings.shape[1], new_embeddings.shape[1])\n",
    "        if embeddings.shape[1] < dim:\n",
    "            pad = torch.zeros((embeddings.shape[0], dim - embeddings.shape[1]), device=device)\n",
    "            embeddings = torch.cat([embeddings, pad], dim=1)\n",
    "        \n",
    "        if new_embeddings.shape[1] < dim:\n",
    "            pad = torch.zeros((new_embeddings.shape[0], dim - new_embeddings.shape[1]), device=device)\n",
    "            new_embeddings = torch.cat([new_embeddings, pad], dim=1)\n",
    "\n",
    "        full_embeddings = torch.empty((len(current_book_ids), dim), dtype=torch.float32, device=device)\n",
    "        existing_idx_map = {book_id: pos for pos, book_id in enumerate(loaded_book_ids)}\n",
    "        for i, book_id in enumerate(current_book_ids):\n",
    "            if book_id in existing_idx_map:\n",
    "                old_pos = existing_idx_map[book_id]\n",
    "                full_embeddings[i] = embeddings[old_pos]\n",
    "        \n",
    "        missing_positions = np.where(missing_mask)[0]\n",
    "        full_embeddings[missing_positions] = new_embeddings\n",
    "    else:\n",
    "        full_embeddings = new_embeddings\n",
    "\n",
    "    save_df = pd.DataFrame(full_embeddings.cpu().numpy(), index=current_book_ids)\n",
    "    save_df.index.name = 'book_id'\n",
    "    save_df.to_csv(embeddings_path)\n",
    "\n",
    "    embeddings = full_embeddings\n",
    "    del full_embeddings, new_embeddings, save_df, batch_embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "def get_similar_friend_ratings(books_df, friends_path, embeddings, z_score=2):  \n",
    "    friends = pd.read_csv(friends_path)\n",
    "    friends['rating'] = friends['rating'].astype('UInt8').replace(0, np.nan)\n",
    "    friends['list_id'] = friends['list_id'].astype(str)\n",
    "\n",
    "    friends_pivot = friends.pivot(index='book_id', columns='list_id', values='rating')\n",
    "    common_book_ratings = friends_pivot.merge(\n",
    "        books_df[['book_id', 'my_rating']].set_index('book_id'),\n",
    "        left_index=True,\n",
    "        right_index=True,\n",
    "        how='outer'\n",
    "    )\n",
    "    rating_columns = [col for col in common_book_ratings.columns if col != 'book_id']\n",
    "    common_book_ratings = common_book_ratings.dropna(subset=rating_columns, how='all')\n",
    "    common_book_ratings = common_book_ratings.loc[[id for id in common_book_ratings.index if id in current_book_ids]]\n",
    "    common_book_ratings = common_book_ratings.dropna(axis='columns', how='all')\n",
    "\n",
    "    common_book_ids = common_book_ratings.index.values\n",
    "    book_id_to_global_idx = {book_id: i for i, book_id in enumerate(books_df['book_id'])}\n",
    "    common_global_indices = torch.tensor([book_id_to_global_idx[bid] for bid in common_book_ids], device=device)\n",
    "    common_embeddings = embeddings[common_global_indices].to(device)\n",
    "\n",
    "    # Predict missing ratings to reduce following correlation sparsity\n",
    "    for list_id in common_book_ratings.columns:\n",
    "        books_rated = ~(common_book_ratings[list_id].isna())\n",
    "        y_train = common_book_ratings[books_rated][list_id]\n",
    "\n",
    "        mask_array = books_rated.values\n",
    "        X_train = common_embeddings[mask_array].cpu().numpy()\n",
    "        y_train = y_train.values\n",
    "\n",
    "        knn = KNeighborsRegressor(\n",
    "            metric='cosine',\n",
    "            weights='distance',\n",
    "            n_jobs=-1\n",
    "            )\n",
    "        knn.fit(X_train, y_train)\n",
    "        pred_rating = knn.predict(common_embeddings.cpu().numpy())\n",
    "        common_book_ratings[list_id] = pred_rating\n",
    "\n",
    "    corr_matrix = common_book_ratings.corr(method='pearson')\n",
    "    np.fill_diagonal(corr_matrix.values, np.nan)\n",
    "    mean_corr = corr_matrix.mean().mean()\n",
    "    std_corr = corr_matrix.std().mean()\n",
    "\n",
    "    similar_tastes = corr_matrix[corr_matrix['my_rating'] >= mean_corr + std_corr * z_score].index\n",
    "    similar_tastes = [p for p in similar_tastes if p != 'my_rating']\n",
    "\n",
    "    similar_friend_ratings = friends[friends['list_id'].isin(similar_tastes)]\n",
    "    return similar_friend_ratings.groupby('book_id')['rating'].mean().dropna()\n",
    "\n",
    "similar_friend_ratings = get_similar_friend_ratings(books_df, \"data/friend_ratings.csv\", embeddings, z_score=2)\n",
    "books_df['training_ratings'] = books_df['my_rating'].fillna(books_df['book_id'].map(similar_friend_ratings))\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # MRL dim reduction\n",
    "train_size = (~books_df['training_ratings'].isna()).sum()\n",
    "mrl_dimensions = MIN_DIMENSIONS\n",
    "while mrl_dimensions*2 < train_size:\n",
    "    mrl_dimensions*=2\n",
    "\n",
    "mrl_dimensions\n",
    "\n",
    "# embeddings = embeddings[:, :mrl_dimensions*2] # qwen-3 embedding supports MRL\n",
    "# norm = Normalizer(norm='l2')\n",
    "# embeddings = norm.fit_transform(embeddings)\n",
    "\n",
    "\n",
    "# embeddings = embeddings.loc[books_df['book_id']].values\n",
    "\n",
    "# embeddings = embeddings[:, :mrl_dimensions*2] # qwen-3 embedding supports MRL\n",
    "# norm = Normalizer(norm='l2')\n",
    "# embeddings = norm.fit_transform(embeddings)\n",
    "\n",
    "# embeddings = torch.tensor(embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency matrix\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "id_to_idx = {id: i for i, id in enumerate(books_df['book_id'])}\n",
    "\n",
    "edge_indices = []\n",
    "for idx, row in tqdm(books_df.iterrows(), total=len(books_df)):\n",
    "    current_idx = id_to_idx[row['book_id']]\n",
    "    if pd.isna(row['similar_books']):\n",
    "        continue\n",
    "    for item in row['similar_books'].split('|'):\n",
    "        try:\n",
    "            target_id = int(item.split(':')[0])\n",
    "            if target_id in books_df['book_id'].values:\n",
    "                target_idx = id_to_idx[target_id]\n",
    "                edge_indices.append([current_idx, target_idx])\n",
    "                edge_indices.append([target_idx, current_idx])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "\n",
    "if not edge_indices:\n",
    "    edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "else:\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=embeddings.size(0))\n",
    "edge_index_norm, edge_weight_norm = gcn_norm(edge_index_with_loops, num_nodes=embeddings.size(0))\n",
    "\n",
    "edge_index_norm = edge_index_norm\n",
    "edge_weight_norm = edge_weight_norm\n",
    "\n",
    "adj_matrix = torch.sparse_coo_tensor(edge_index_norm, edge_weight_norm, (embeddings.size(0), embeddings.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nevergrad as ng\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def objective(num_propagations, \n",
    "              knn_neighbors,\n",
    "              brr_alpha_1, brr_alpha_2, brr_lambda_1, brr_lambda_2, brr_uncertainty_penalty,\n",
    "              svr_C, svr_epsilon,\n",
    "              knn_weight, brr_weight\n",
    "              ):\n",
    "    \n",
    "    all_embeddings = precomputed_embeddings[num_propagations]\n",
    "    X = all_embeddings[training_mask]\n",
    "    y = my_ratings\n",
    "\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, test_idx in skf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        knn = KNeighborsRegressor(\n",
    "            n_neighbors=knn_neighbors,\n",
    "            metric='cosine', \n",
    "            weights='distance',\n",
    "            n_jobs=-1,\n",
    "            )\n",
    "        knn.fit(X_train, y_train)\n",
    "        knn_pred = knn.predict(X_test)\n",
    "\n",
    "        brr = BayesianRidge(\n",
    "            alpha_1=brr_alpha_1,\n",
    "            alpha_2=brr_alpha_2,\n",
    "            lambda_1=brr_lambda_1,\n",
    "            lambda_2=brr_lambda_2,\n",
    "            compute_score=True\n",
    "            )\n",
    "        brr.fit(X_train, y_train)\n",
    "        brr_mu, brr_std = brr.predict(X_test, return_std=True)\n",
    "        brr_pred = brr_mu - (brr_uncertainty_penalty * brr_std)\n",
    "\n",
    "        svr = SVR(\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            C=svr_C, \n",
    "            epsilon=svr_epsilon\n",
    "            ) \n",
    "        svr.fit(X_train, y_train)\n",
    "        svr_pred = svr.predict(X_test)\n",
    "\n",
    "        remaining_weight = 1 - knn_weight\n",
    "        brr_weight *= remaining_weight\n",
    "        svr_weight = remaining_weight - brr_weight\n",
    "        final_pred = (knn_weight * knn_pred) + (brr_weight * brr_pred) + (svr_weight * svr_pred)\n",
    "\n",
    "        y_trues.append(y_test)\n",
    "        y_preds.append(final_pred)\n",
    "\n",
    "    y_trues = np.concatenate(y_trues)\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "    \n",
    "    mse = mean_squared_error(y_trues, y_preds)\n",
    "    ndcg = ndcg_score([y_trues], [y_preds])\n",
    "    if np.std(y_preds) < 1e-9:\n",
    "        spearman = 0.0\n",
    "    else:\n",
    "        spearman, _ = spearmanr(y_trues, y_preds)\n",
    "        if np.isnan(spearman): spearman = 0.0\n",
    "\n",
    "    return mse + (1.0 - ndcg) + (1.0 - spearman)\n",
    "    # return ndcg - spearman\n",
    "\n",
    "MAX_PROPAGATIONS = 3\n",
    "propagated = embeddings.clone()\n",
    "norm_l2 = Normalizer(norm='l2')\n",
    "precomputed_embeddings = [norm_l2.transform(propagated.numpy())]\n",
    "\n",
    "for _ in range(MAX_PROPAGATIONS):\n",
    "    propagated = torch.sparse.mm(adj_matrix, propagated)\n",
    "    precomputed_embeddings.append(norm_l2.transform(propagated.numpy()))\n",
    "del propagated\n",
    "\n",
    "training_mask = ~books_df['training_ratings'].isna()\n",
    "my_ratings = books_df.loc[training_mask, 'training_ratings'].values\n",
    "\n",
    "parametrization = ng.p.Instrumentation(\n",
    "    num_propagations = ng.p.Scalar(lower=0, upper=MAX_PROPAGATIONS).set_integer_casting(),\n",
    "\n",
    "    knn_neighbors = ng.p.Scalar(lower=1, upper=mrl_dimensions//3).set_integer_casting(),\n",
    "    brr_alpha_1=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    brr_alpha_2=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    brr_lambda_1=ng.p.Log(lower=1e-6, upper=1e-1),\n",
    "    brr_lambda_2=ng.p.Log(lower=1e-6, upper=1e-1),\n",
    "    brr_uncertainty_penalty=ng.p.Scalar(lower=0, upper=2.0),\n",
    "    svr_C = ng.p.Log(lower=1e-3, upper=1e2),\n",
    "    svr_epsilon = ng.p.Scalar(lower=0.0, upper=1.0),\n",
    "\n",
    "    knn_weight=ng.p.Scalar(lower=0, upper=1), \n",
    "    brr_weight=ng.p.Scalar(lower=0, upper=1), \n",
    ")\n",
    "\n",
    "BUDGET = 300\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=BUDGET)\n",
    "best_loss = float('inf')\n",
    "with tqdm(total=BUDGET) as pbar:\n",
    "    for i in range(BUDGET):\n",
    "        x = optimizer.ask()\n",
    "        loss = objective(*x.args, **x.kwargs)\n",
    "        optimizer.tell(x, loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "        pbar.update(1)\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_description(f\"Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "best_params = optimizer.provide_recommendation().kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"8 MRL sliced\n",
    "Best Loss: 1.5714: 100%|██████████| 300/300 [11:49<00:00,  2.37s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 17,\n",
    " 'brr_alpha_1': 0.0006421576956932416,\n",
    " 'brr_alpha_2': 0.0004509358497670652,\n",
    " 'brr_lambda_1': 8.24637589893822e-06,\n",
    " 'brr_lambda_2': 0.0001968595842801841,\n",
    " 'brr_uncertainty_penalty': 1.3679433872638418,\n",
    " 'svr_C': 0.004026523432673821,\n",
    " 'svr_epsilon': 0.3118081208045217,\n",
    " 'knn_weight': 0.5805897543886746,\n",
    " 'brr_weight': 0.7793002684038448}\"\"\"\n",
    "\n",
    "\"\"\"8 full\n",
    "Best Loss: 1.6065: 100%|██████████| 300/300 [16:57<00:00,  3.39s/it]\n",
    " {'num_propagations': 0,\n",
    " 'knn_neighbors': 36,\n",
    " 'brr_alpha_1': 0.0006880608281295565,\n",
    " 'brr_alpha_2': 0.0003014766980578955,\n",
    " 'brr_lambda_1': 1.3770534826263817e-05,\n",
    " 'brr_lambda_2': 0.005746291033999427,\n",
    " 'brr_uncertainty_penalty': 1.8570930538472121,\n",
    " 'svr_C': 0.03383255010384741,\n",
    " 'svr_epsilon': 0.44291277801367485,\n",
    " 'knn_weight': 0.5848794696920626,\n",
    " 'brr_weight': 0.5411997331122358}\"\"\"\n",
    "\n",
    "\"\"\"0.6 MRL sliced\n",
    "Best Loss: 1.5748: 100%|██████████| 300/300 [10:29<00:00,  2.10s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 37,\n",
    " 'brr_alpha_1': 0.0002978589454364143,\n",
    " 'brr_alpha_2': 0.0005679816409316726,\n",
    " 'brr_lambda_1': 0.0003994602308099561,\n",
    " 'brr_lambda_2': 0.003659493431608586,\n",
    " 'brr_uncertainty_penalty': 1.3409360588302992,\n",
    " 'svr_C': 0.009413204523151376,\n",
    " 'svr_epsilon': 0.5861865063902226,\n",
    " 'knn_weight': 0.6849389456290866,\n",
    " 'brr_weight': 0.7766436222538672}\"\"\"\n",
    "\n",
    "\"\"\"0.6 full\n",
    "Best Loss: 1.5926: 100%|██████████| 300/300 [12:22<00:00,  2.48s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 55,\n",
    " 'brr_alpha_1': 0.0007517429857762481,\n",
    " 'brr_alpha_2': 0.0003941767071639492,\n",
    " 'brr_lambda_1': 8.604754211823616e-06,\n",
    " 'brr_lambda_2': 0.008083208576838542,\n",
    " 'brr_uncertainty_penalty': 1.048016018935488,\n",
    " 'svr_C': 0.013400702917698843,\n",
    " 'svr_epsilon': 0.4149283525703265,\n",
    " 'knn_weight': 0.6826121601964188,\n",
    " 'brr_weight': 0.8414845392757134}\"\"\"\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run optimized hyperparams\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "\n",
    "all_embeddings = precomputed_embeddings[best_params['num_propagations']]\n",
    "\n",
    "X_train = all_embeddings[training_mask]\n",
    "y_train = my_ratings\n",
    "\n",
    "knn = KNeighborsRegressor(\n",
    "    n_neighbors=best_params['knn_neighbors'],\n",
    "    metric='cosine', \n",
    "    weights='distance',\n",
    "    n_jobs=-1,\n",
    "    )\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(all_embeddings)\n",
    "\n",
    "brr = BayesianRidge(\n",
    "    alpha_1=best_params['brr_alpha_1'],\n",
    "    alpha_2=best_params['brr_alpha_2'],\n",
    "    lambda_1=best_params['brr_lambda_1'],\n",
    "    lambda_2=best_params['brr_lambda_2'],\n",
    "    compute_score=True\n",
    "    )\n",
    "brr.fit(X_train, y_train)\n",
    "brr_mu, brr_std = brr.predict(all_embeddings, return_std=True)\n",
    "brr_pred = brr_mu - (best_params['brr_uncertainty_penalty'] * brr_std)\n",
    "\n",
    "svr = SVR(\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    C=best_params['svr_C'], \n",
    "    epsilon=best_params['svr_epsilon']\n",
    "    ) \n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(all_embeddings)\n",
    "\n",
    "knn_weight = best_params['knn_weight']\n",
    "brr_weight = best_params['brr_weight']\n",
    "\n",
    "remaining_weight = 1 - knn_weight\n",
    "brr_weight *= remaining_weight\n",
    "svr_weight = remaining_weight - brr_weight\n",
    "final_pred = (knn_weight * knn_pred) + (brr_weight * brr_pred) + (svr_weight * svr_pred)\n",
    "\n",
    "books_df['knn_pred'] = knn_pred\n",
    "books_df['brr_pred'] = brr_pred\n",
    "books_df['svr_pred'] = svr_pred\n",
    "books_df['final_pred'] = final_pred\n",
    "\n",
    "# cols = ['title'] + [col for col in books_df.columns if col.endswith('pred')]\n",
    "# books_df[books_df['my_rating'].isna()].sort_values(by='final_pred', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Count adjusted rating\n",
    "C = books_df['avg_rating'].mean()\n",
    "m = books_df['rating_count'].quantile(0.10) \n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = float(x['rating_count'])\n",
    "    R = float(x['avg_rating'])\n",
    "    if v == 0: \n",
    "        return C\n",
    "    return (v / (v + m) * R) + (m / (v + m) * C)\n",
    "books_df['count_adjusted_rating'] = books_df.apply(weighted_rating, axis=1)\n",
    "\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "scaler = MinMaxScaler()\n",
    "books_df[['count_adjusted_rating', 'final_pred']] = scaler.fit_transform(books_df[['count_adjusted_rating', 'final_pred']])\n",
    "\n",
    "# # Quadratic model coefficients\n",
    "# from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "# def fit_quadratic(row):\n",
    "#     x = np.array([1, 2, 3, 4, 5])\n",
    "#     a, b, c = np.polyfit(x, row, 2)\n",
    "#     return pd.Series([a, b, c])\n",
    "\n",
    "# books_df['1_star_percentage'] = books_df['1_star'] / books_df['rating_count']\n",
    "# books_df['2_star_percentage'] = books_df['2_star'] / books_df['rating_count']\n",
    "# books_df['3_star_percentage'] = books_df['3_star'] / books_df['rating_count']\n",
    "# books_df['4_star_percentage'] = books_df['4_star'] / books_df['rating_count']\n",
    "# books_df['5_star_percentage'] = books_df['5_star'] / books_df['rating_count']\n",
    "# coefficients = books_df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "# books_df.drop(['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage'], axis=1, inplace=True)\n",
    "\n",
    "# books_df['a'], books_df['b'], books_df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "# scaler = MinMaxScaler()\n",
    "# books_df[['a', 'b', 'c']] = scaler.fit_transform(books_df[['a', 'b', 'c']]) + 1\n",
    "# books_df['coeff_rating'] = books_df['a'] * books_df['c']\n",
    "# books_df['final_rating'] = books_df[['coeff_rating']] * books_df['count_adjusted_rating'] * books_df['final_pred'] / np.log1p(books_df['num_pages'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['final_rating'] = (books_df['count_adjusted_rating'] * books_df['final_pred']) #/ (np.log1p(books_df['num_pages']))\n",
    "books_df[books_df['my_rating'].isna()].sort_values(by='final_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "books_df['final_rating'] = (books_df['count_adjusted_rating'] * books_df['final_pred']) #/ (np.log1p(books_df['num_pages']))\n",
    "books_df[books_df['my_rating'].isna()].sort_values(by='final_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Types of rankings\n",
    "books_df['num_adjusted_rating'] = books_df['average_rating'] - (books_df['average_rating'] - books_df['average_rating'].mean()) / books_df['num_ratings_ln']\n",
    "books_df['coeff_2a_rating'] = (books_df['num_adjusted_rating'] * books_df['2a_shifted'])\n",
    "books_df['coeff_b_rating'] = (books_df['num_adjusted_rating']) / (books_df['b_shifted'])\n",
    "books_df['coeff_c_rating'] = (books_df['num_adjusted_rating'] * books_df['c_shifted'])\n",
    "books_df['joined_rating'] = (books_df['num_adjusted_rating'] * books_df['c_shifted'] * books_df['2a_shifted']) / books_df['b_shifted']\n",
    "books_df['final_rating'] = books_df['joined_rating'] - (books_df['joined_rating'] - books_df['joined_rating'].mean()) / books_df['num_ratings_ln']\n",
    "\n",
    "books_df['num_adjusted_page_rating'] = books_df['num_adjusted_rating'] / (books_df['num_pages_ln'])\n",
    "books_df['coeff_2a_page_rating'] = books_df['coeff_2a_rating'] / books_df['num_pages_ln']\n",
    "books_df['coeff_b_page_rating'] = books_df['coeff_b_rating'] / books_df['num_pages_ln']\n",
    "books_df['coeff_c_page_rating'] = books_df['coeff_c_rating'] / books_df['num_pages_ln']\n",
    "books_df['joined_page_rating'] = books_df['joined_rating'] / books_df['num_pages_ln']\n",
    "books_df['final_page_rating'] = books_df['joined_page_rating'] - (books_df['joined_page_rating'] - books_df['joined_page_rating'].mean()) / books_df['num_ratings_ln']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_export = pd.read_csv('data/goodreads_library_export.csv')\n",
    "# goodreads_export = pd.read_csv('data/20-01-2025_goodreads_library_export.csv')\n",
    "# goodreads_export = goodreads_export[['Book Id', 'Author', 'My Rating', 'Number of Pages', 'Original Publication Year']]\n",
    "# goodreads_export = goodreads_export.rename(columns={'Book Id':'book_id',\n",
    "#                                                     'Author': 'author',\n",
    "#                                                     'My Rating': 'my_rating',\n",
    "#                                                     'Number of Pages': 'num_pages',\n",
    "#                                                     'Original Publication Year': 'year'})\n",
    "threshold = (goodreads_export['number_of_pages'].mean() - goodreads_export['number_of_pages'].std())\n",
    "goodreads_export.loc[goodreads_export['number_of_pages'] < threshold, 'number_of_pages'] = np.nan\n",
    "\n",
    "book_df = pd.read_csv('data/books.csv')\n",
    "# book_df = pd.read_csv('data/01-2025_goodreads_scraped.csv')\n",
    "df = goodreads_export.merge(book_df, on='book_id')\n",
    "\n",
    "# Drop competing columns\n",
    "df['author'] = df['author_x'].fillna(df['author_y'])\n",
    "df['num_pages'] = df['num_pages_x'].fillna(df['num_pages_y'])\n",
    "df['year'] = df['year_x'].fillna(df['year_y'])\n",
    "df.drop(columns=['author_x', 'author_y', 'num_pages_x', 'num_pages_y', 'year_x', 'year_y'], inplace=True)\n",
    "\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df['year'] = df['year'].fillna(df['year'].mean()).round().astype(int)\n",
    "df['num_pages'] = df['num_pages'].fillna(df['num_pages'].mean()).round().astype(int)\n",
    "df['num_reviews'] = df['num_reviews'].fillna(0).round().astype(int)\n",
    "df['my_rating'] = df['my_rating'].replace(0,np.nan)\n",
    "\n",
    "df['age'] = int(datetime.now().strftime('%Y')) - df['year']\n",
    "df['average_rating'] = ((df['5 stars'] * 5) + (df['4 stars'] * 4) + (df['3 stars'] * 3) + (df['2 stars'] * 2) + df['1 star']) / df['num_ratings']\n",
    "df = df[['book_id', 'title', 'author', 'year', 'age', 'series', 'num_pages', 'genres', 'num_ratings', 'num_reviews', 'my_rating', 'average_rating', '5 stars', '4 stars', '3 stars', '2 stars', '1 star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quadratic(row):\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    a, b, c = np.polyfit(x, row, 2)\n",
    "    return pd.Series([a, b, c])\n",
    "\n",
    "# Calculating quadrdic modeling coefficients\n",
    "df['1_star_percentage'] = df['1 star'] / df['num_ratings']\n",
    "df['2_star_percentage'] = df['2 stars'] / df['num_ratings']\n",
    "df['3_star_percentage'] = df['3 stars'] / df['num_ratings']\n",
    "df['4_star_percentage'] = df['4 stars'] / df['num_ratings']\n",
    "df['5_star_percentage'] = df['5 stars'] / df['num_ratings']\n",
    "coefficients = df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "df['a'], df['b'], df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "\n",
    "# Pre-processing columns for rankings\n",
    "df['num_ratings_ln'] = np.log1p(df['num_ratings'])\n",
    "df['num_pages_ln'] = np.log1p(df['num_pages'])\n",
    "df['2a_shifted'] = df['a'] - df['a'].min()\n",
    "df['2a_shifted'] = df['2a_shifted'] * (1 / df['2a_shifted'].max()) + 1\n",
    "df['b_shifted'] = df['b'] - df['b'].min()\n",
    "df['b_shifted'] = df['b_shifted'] * (1 / df['b_shifted'].max()) + 1\n",
    "df['c_shifted'] = df['c'] - df['c'].min()\n",
    "df['c_shifted'] = df['c_shifted'] * (1 / df['c_shifted'].max()) + 1\n",
    "\n",
    "# Types of rankings\n",
    "df['num_adjusted_rating'] = df['average_rating'] - (df['average_rating'] - df['average_rating'].mean()) / df['num_ratings_ln']\n",
    "df['coeff_2a_rating'] = (df['num_adjusted_rating'] * df['2a_shifted'])\n",
    "df['coeff_b_rating'] = (df['num_adjusted_rating']) / (df['b_shifted'])\n",
    "df['coeff_c_rating'] = (df['num_adjusted_rating'] * df['c_shifted'])\n",
    "df['joined_rating'] = (df['num_adjusted_rating'] * df['c_shifted'] * df['2a_shifted']) / df['b_shifted']\n",
    "df['final_rating'] = df['joined_rating'] - (df['joined_rating'] - df['joined_rating'].mean()) / df['num_ratings_ln']\n",
    "\n",
    "df['num_adjusted_page_rating'] = df['num_adjusted_rating'] / (df['num_pages_ln'])\n",
    "df['coeff_2a_page_rating'] = df['coeff_2a_rating'] / df['num_pages_ln']\n",
    "df['coeff_b_page_rating'] = df['coeff_b_rating'] / df['num_pages_ln']\n",
    "df['coeff_c_page_rating'] = df['coeff_c_rating'] / df['num_pages_ln']\n",
    "df['joined_page_rating'] = df['joined_rating'] / df['num_pages_ln']\n",
    "df['final_page_rating'] = df['joined_page_rating'] - (df['joined_page_rating'] - df['joined_page_rating'].mean()) / df['num_ratings_ln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['age', 'num_pages', 'num_pages_ln', 'num_ratings', 'num_ratings_ln', 'num_reviews', 'my_rating', 'average_rating', '1 star', '2 stars', '3 stars', '4 stars', '5 stars', '1_star_percentage', '2_star_percentage', '3_star_percentage', '4_star_percentage', '5_star_percentage', 'a', 'b', 'c', 'num_adjusted_rating', 'coeff_2a_rating', 'coeff_b_rating', 'coeff_c_rating', 'joined_rating', 'final_rating', 'num_adjusted_page_rating', 'coeff_2a_page_rating', 'coeff_b_page_rating', 'coeff_c_page_rating', 'joined_page_rating', 'final_page_rating']\n",
    "corr_df= df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 15)) \n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Correlation Heatmap') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = df.sort_values(by='final_page_rating', ascending=False).reset_index().drop('index', axis=1)\n",
    "fresh = fresh[fresh['my_rating'].isna()]\n",
    "fresh[['Fiction' in genre_list for genre_list in fresh['genres']]] # Fiction, Nonfiction, Memoir, Classics, History, Politics, Philosophy, Business"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
