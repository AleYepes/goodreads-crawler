{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "books_df = pd.read_csv(\"data/books.csv\")\n",
    "star_cols = [c for c in books_df.columns if c.endswith('star')]\n",
    "books_df['rating_count'] = books_df[star_cols].sum(axis=1)\n",
    "\n",
    "# Todo: Fix the crawler to split langs with \"|\" by default\n",
    "books_df['lang'] = [\n",
    "    \"|\".join(item.strip() for item in x.split(\";\")) if isinstance(x, str) else x \n",
    "    for x in books_df['lang']\n",
    "]\n",
    "books_df['description'] = books_df['description'].str.replace('\\n\\n', '\\n')\n",
    "books_df['description'] = books_df['description'].str.replace('\\n', ' ')\n",
    "books_df['description'] = books_df['description'].str.replace('   ', ' ')\n",
    "books_df['description'] = books_df['description'].str.replace('  ', ' ')\n",
    "\n",
    "goodreads_df = pd.read_csv('data/goodreads_library_export.csv')\n",
    "goodreads_df['my_rating'] = goodreads_df['my_rating'].astype('UInt8')\n",
    "goodreads_df['my_rating'] = goodreads_df['my_rating'].replace(0, np.nan)\n",
    "\n",
    "books_df = books_df.merge(goodreads_df[['book_id', 'my_rating']], on='book_id', how='left')\n",
    "# books_df = books_df[~books_df['similar_books'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep embedding strings\n",
    "def format_string_for_embedding(items, kind=None, truncate=0):\n",
    "    if not isinstance(items, (list)) or len(items) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    n = len(items)\n",
    "    if n == 1:\n",
    "        res = items[0]\n",
    "    elif n > truncate > 1:\n",
    "        res = f\"{', '.join(items[:truncate])}, and {items[truncate]}\"\n",
    "    else:\n",
    "        res = f\"{', '.join(items[:-1])}{',' if n > 2 else ''} and {items[-1]}\"\n",
    "    \n",
    "    prefix = f\"{kind.capitalize()}{'s' if n > 1 else ''}: \" if kind else \"\"\n",
    "    return f\"{prefix}{res}\"\n",
    "\n",
    "books_df['authors_post'] = books_df['authors'].str.split('|')\n",
    "books_df['authors_post'] = books_df['authors_post'].apply(lambda x:format_string_for_embedding(x, truncate=4))\n",
    "\n",
    "books_df['genres_post'] = books_df['genres'].str.split('|')\n",
    "books_df['genres_post'] = books_df['genres_post'].apply(lambda x:format_string_for_embedding(x, kind='genre'))\n",
    "\n",
    "books_df['desc_post'] = [[desc] if isinstance(desc, str) else [] for desc in books_df['description']]\n",
    "books_df['desc_post'] = books_df['desc_post'].apply(lambda x:format_string_for_embedding(x, kind='description'))\n",
    "\n",
    "def join_embedding_parts(title, authors, genres, desc):\n",
    "    text = f\"Book: {title}\\n\"\n",
    "    if authors:\n",
    "        text += f\"Written by: {authors}\\n\"\n",
    "    if genres:\n",
    "        text += f\"{genres}\\n\"\n",
    "    if desc:\n",
    "        text += f\"{desc}\" \n",
    "    return text\n",
    "\n",
    "books_df['embedding_input'] = [\n",
    "    join_embedding_parts(t, a, g, d) \n",
    "    for t, a, g, d in zip(books_df['title'], books_df['authors_post'], books_df['genres_post'], books_df['desc_post'])\n",
    "]\n",
    "id_to_string = books_df.set_index('book_id')['embedding_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed sentences\n",
    "import os\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "from sklearn.preprocessing import Normalizer\n",
    "\n",
    "PARAMS = 8\n",
    "OLLAMA_MODEL = f\"qwen3-embedding:{PARAMS}b\"\n",
    "MIN_DIMENSIONS = 32\n",
    "\n",
    "train_size = (~books_df['my_rating'].isna()).sum()\n",
    "mrl_dimensions = MIN_DIMENSIONS\n",
    "while mrl_dimensions*2 < train_size:\n",
    "    mrl_dimensions*=2\n",
    "\n",
    "embeddings_path = f'data/{PARAMS}b_embeddings.csv'\n",
    "if os.path.exists(embeddings_path):\n",
    "    embeddings = pd.read_csv(embeddings_path).set_index('book_id')\n",
    "    embeddings.columns = embeddings.columns.astype(int)\n",
    "    embeddings = pd.DataFrame(embeddings).dropna(subset=[0])\n",
    "else:\n",
    "    embeddings = pd.DataFrame()\n",
    "\n",
    "current_ids = books_df['book_id'].values\n",
    "missing_ids = [idx for idx in current_ids if idx not in embeddings.index]\n",
    "if missing_ids:\n",
    "    missing_strings = id_to_string.loc[missing_ids].tolist()\n",
    "    batch_size = 128\n",
    "    new_embeddings = []\n",
    "    for i in tqdm(range(0, len(missing_strings), batch_size)):\n",
    "        batch = missing_strings[i : i + batch_size]\n",
    "        response = ollama.embed(model=OLLAMA_MODEL, input=batch)\n",
    "        new_embeddings.extend(response['embeddings'])\n",
    "\n",
    "    new_embeddings = pd.DataFrame(new_embeddings, index=missing_ids)\n",
    "    new_embeddings.index.name = 'book_id'\n",
    "    embeddings = pd.concat([embeddings, new_embeddings])\n",
    "    embeddings.to_csv(embeddings_path)\n",
    "    del new_embeddings, missing_strings, missing_ids, current_ids\n",
    "\n",
    "embeddings = embeddings.loc[books_df['book_id']].values\n",
    "embeddings = embeddings[:, :mrl_dimensions*2] # qwen-3 embedding supports MRL\n",
    "norm = Normalizer(norm='l2')\n",
    "embeddings = norm.fit_transform(embeddings)\n",
    "embeddings = torch.tensor(embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency matrix\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "id_to_idx = {id: i for i, id in enumerate(books_df['book_id'])}\n",
    "\n",
    "edge_indices = []\n",
    "for idx, row in tqdm(books_df.iterrows(), total=len(books_df)):\n",
    "    current_idx = id_to_idx[row['book_id']]\n",
    "    if pd.isna(row['similar_books']):\n",
    "        continue\n",
    "    for item in row['similar_books'].split('|'):\n",
    "        try:\n",
    "            target_id = int(item.split(':')[0])\n",
    "            if target_id in books_df['book_id'].values:\n",
    "                target_idx = id_to_idx[target_id]\n",
    "                edge_indices.append([current_idx, target_idx])\n",
    "                edge_indices.append([target_idx, current_idx])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "\n",
    "if not edge_indices:\n",
    "    edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "else:\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=embeddings.size(0))\n",
    "edge_index_norm, edge_weight_norm = gcn_norm(edge_index_with_loops, num_nodes=embeddings.size(0))\n",
    "adj_matrix = torch.sparse_coo_tensor(edge_index_norm, edge_weight_norm, (embeddings.size(0), embeddings.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nevergrad as ng\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "def objective(num_propagations, \n",
    "              knn_neighbors,\n",
    "              brr_alpha_1, brr_alpha_2, brr_lambda_1, brr_lambda_2, brr_uncertainty_penalty,\n",
    "              svr_C, svr_epsilon,\n",
    "              knn_weight, brr_weight\n",
    "              ):\n",
    "    \n",
    "    all_embeddings = precomputed_embeddings[num_propagations]\n",
    "    X = all_embeddings[my_ratings_mask]\n",
    "    y = my_ratings\n",
    "\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    skf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    for train_idx, test_idx in skf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        knn = KNeighborsRegressor(\n",
    "            n_neighbors=knn_neighbors,\n",
    "            metric='cosine', \n",
    "            weights='distance',\n",
    "            n_jobs=-1,\n",
    "            )\n",
    "        knn.fit(X_train, y_train)\n",
    "        knn_pred = knn.predict(X_test)\n",
    "\n",
    "        brr = BayesianRidge(\n",
    "            alpha_1=brr_alpha_1,\n",
    "            alpha_2=brr_alpha_2,\n",
    "            lambda_1=brr_lambda_1,\n",
    "            lambda_2=brr_lambda_2,\n",
    "            compute_score=True\n",
    "            )\n",
    "        brr.fit(X_train, y_train)\n",
    "        brr_mu, brr_std = brr.predict(X_test, return_std=True)\n",
    "        brr_pred = brr_mu - (brr_uncertainty_penalty * brr_std)\n",
    "\n",
    "        svr = SVR(\n",
    "            kernel='rbf',\n",
    "            gamma='scale',\n",
    "            C=svr_C, \n",
    "            epsilon=svr_epsilon\n",
    "            ) \n",
    "        svr.fit(X_train, y_train)\n",
    "        svr_pred = svr.predict(X_test)\n",
    "\n",
    "        remaining_weight = 1 - knn_weight\n",
    "        brr_weight *= remaining_weight\n",
    "        svr_weight = remaining_weight - brr_weight\n",
    "        final_pred = (knn_weight * knn_pred) + (brr_weight * brr_pred) + (svr_weight * svr_pred)\n",
    "\n",
    "        y_trues.append(y_test)\n",
    "        y_preds.append(final_pred)\n",
    "\n",
    "    y_trues = np.concatenate(y_trues)\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "    y_preds = np.clip(y_preds, 1, 5)\n",
    "    \n",
    "    mse = mean_squared_error(y_trues, y_preds)\n",
    "    ndcg = ndcg_score([y_trues], [y_preds])\n",
    "    if np.std(y_preds) < 1e-9:\n",
    "        spearman = 0.0\n",
    "    else:\n",
    "        spearman, _ = spearmanr(y_trues, y_preds)\n",
    "        if np.isnan(spearman): spearman = 0.0\n",
    "\n",
    "    return mse + (1.0 - ndcg) + (1.0 - spearman)\n",
    "\n",
    "MAX_PROPAGATIONS = 2\n",
    "propagated = embeddings.clone()\n",
    "norm_l2 = Normalizer(norm='l2')\n",
    "precomputed_embeddings = [norm_l2.transform(propagated.numpy())]\n",
    "\n",
    "for _ in range(MAX_PROPAGATIONS):\n",
    "    propagated = torch.sparse.mm(adj_matrix, propagated)\n",
    "    precomputed_embeddings.append(norm_l2.transform(propagated.numpy()))\n",
    "del propagated\n",
    "\n",
    "my_ratings_mask = ~books_df['my_rating'].isna()\n",
    "my_ratings = books_df.loc[my_ratings_mask, 'my_rating'].values\n",
    "\n",
    "parametrization = ng.p.Instrumentation(\n",
    "    num_propagations = ng.p.Scalar(lower=1, upper=MAX_PROPAGATIONS).set_integer_casting(),\n",
    "\n",
    "    knn_neighbors = ng.p.Scalar(lower=3, upper=mrl_dimensions//2).set_integer_casting(),\n",
    "    brr_alpha_1=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    brr_alpha_2=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    brr_lambda_1=ng.p.Log(lower=1e-6, upper=1e-1),\n",
    "    brr_lambda_2=ng.p.Log(lower=1e-6, upper=1e-1),\n",
    "    brr_uncertainty_penalty=ng.p.Scalar(lower=0, upper=2.0),\n",
    "    svr_C = ng.p.Log(lower=1e-3, upper=1e2),\n",
    "    svr_epsilon = ng.p.Scalar(lower=0.0, upper=1.0),\n",
    "\n",
    "    knn_weight=ng.p.Scalar(lower=0, upper=1), \n",
    "    brr_weight=ng.p.Scalar(lower=0, upper=1), \n",
    ")\n",
    "\n",
    "BUDGET = 300\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=BUDGET)\n",
    "best_loss = float('inf')\n",
    "with tqdm(total=BUDGET) as pbar:\n",
    "    for i in range(BUDGET):\n",
    "        x = optimizer.ask()\n",
    "        loss = objective(*x.args, **x.kwargs)\n",
    "        optimizer.tell(x, loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "        pbar.update(1)\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_description(f\"Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "best_params = optimizer.provide_recommendation().kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"8 MRL sliced\n",
    "Best Loss: 1.5714: 100%|██████████| 300/300 [11:49<00:00,  2.37s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 17,\n",
    " 'brr_alpha_1': 0.0006421576956932416,\n",
    " 'brr_alpha_2': 0.0004509358497670652,\n",
    " 'brr_lambda_1': 8.24637589893822e-06,\n",
    " 'brr_lambda_2': 0.0001968595842801841,\n",
    " 'brr_uncertainty_penalty': 1.3679433872638418,\n",
    " 'svr_C': 0.004026523432673821,\n",
    " 'svr_epsilon': 0.3118081208045217,\n",
    " 'knn_weight': 0.5805897543886746,\n",
    " 'brr_weight': 0.7793002684038448}\"\"\"\n",
    "\n",
    "\"\"\"8 full\n",
    "Best Loss: 1.6065: 100%|██████████| 300/300 [16:57<00:00,  3.39s/it]\n",
    " {'num_propagations': 0,\n",
    " 'knn_neighbors': 36,\n",
    " 'brr_alpha_1': 0.0006880608281295565,\n",
    " 'brr_alpha_2': 0.0003014766980578955,\n",
    " 'brr_lambda_1': 1.3770534826263817e-05,\n",
    " 'brr_lambda_2': 0.005746291033999427,\n",
    " 'brr_uncertainty_penalty': 1.8570930538472121,\n",
    " 'svr_C': 0.03383255010384741,\n",
    " 'svr_epsilon': 0.44291277801367485,\n",
    " 'knn_weight': 0.5848794696920626,\n",
    " 'brr_weight': 0.5411997331122358}\"\"\"\n",
    "\n",
    "\"\"\"0.6 MRL sliced\n",
    "Best Loss: 1.5748: 100%|██████████| 300/300 [10:29<00:00,  2.10s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 37,\n",
    " 'brr_alpha_1': 0.0002978589454364143,\n",
    " 'brr_alpha_2': 0.0005679816409316726,\n",
    " 'brr_lambda_1': 0.0003994602308099561,\n",
    " 'brr_lambda_2': 0.003659493431608586,\n",
    " 'brr_uncertainty_penalty': 1.3409360588302992,\n",
    " 'svr_C': 0.009413204523151376,\n",
    " 'svr_epsilon': 0.5861865063902226,\n",
    " 'knn_weight': 0.6849389456290866,\n",
    " 'brr_weight': 0.7766436222538672}\"\"\"\n",
    "\n",
    "\"\"\"0.6 full\n",
    "Best Loss: 1.5926: 100%|██████████| 300/300 [12:22<00:00,  2.48s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 55,\n",
    " 'brr_alpha_1': 0.0007517429857762481,\n",
    " 'brr_alpha_2': 0.0003941767071639492,\n",
    " 'brr_lambda_1': 8.604754211823616e-06,\n",
    " 'brr_lambda_2': 0.008083208576838542,\n",
    " 'brr_uncertainty_penalty': 1.048016018935488,\n",
    " 'svr_C': 0.013400702917698843,\n",
    " 'svr_epsilon': 0.4149283525703265,\n",
    " 'knn_weight': 0.6826121601964188,\n",
    " 'brr_weight': 0.8414845392757134}\"\"\"\n",
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "import time\n",
    "\n",
    "all_embeddings = precomputed_embeddings[best_params['num_propagations']]\n",
    "\n",
    "X_train = all_embeddings[my_ratings_mask]\n",
    "y_train = my_ratings\n",
    "\n",
    "knn = KNeighborsRegressor(\n",
    "    n_neighbors=best_params['knn_neighbors'],\n",
    "    metric='cosine', \n",
    "    weights='distance',\n",
    "    n_jobs=-1,\n",
    "    )\n",
    "knn.fit(X_train, y_train)\n",
    "knn_pred = knn.predict(all_embeddings)\n",
    "\n",
    "brr = BayesianRidge(\n",
    "    alpha_1=best_params['brr_alpha_1'],\n",
    "    alpha_2=best_params['brr_alpha_2'],\n",
    "    lambda_1=best_params['brr_lambda_1'],\n",
    "    lambda_2=best_params['brr_lambda_2'],\n",
    "    compute_score=True\n",
    "    )\n",
    "brr.fit(X_train, y_train)\n",
    "brr_mu, brr_std = brr.predict(all_embeddings, return_std=True)\n",
    "brr_pred = brr_mu - (best_params['brr_uncertainty_penalty'] * brr_std)\n",
    "\n",
    "svr = SVR(\n",
    "    kernel='rbf',\n",
    "    gamma='scale',\n",
    "    C=best_params['svr_C'], \n",
    "    epsilon=best_params['svr_epsilon']\n",
    "    ) \n",
    "svr.fit(X_train, y_train)\n",
    "svr_pred = svr.predict(all_embeddings)\n",
    "\n",
    "knn_weight = best_params['knn_weight']\n",
    "brr_weight = best_params['brr_weight']\n",
    "\n",
    "remaining_weight = 1 - knn_weight\n",
    "brr_weight *= remaining_weight\n",
    "svr_weight = remaining_weight - brr_weight\n",
    "final_pred = (knn_weight * knn_pred) + (brr_weight * brr_pred) + (svr_weight * svr_pred)\n",
    "\n",
    "# start = time.time()\n",
    "# knn = KNeighborsRegressor(\n",
    "#     n_neighbors=best_params['knn_neighbors'],\n",
    "#     metric='cosine', \n",
    "#     weights='distance',\n",
    "#     n_jobs=-1,\n",
    "#     )\n",
    "# knn.fit(X_train, y_train)\n",
    "# knn_pred = knn.predict(all_embeddings)\n",
    "# print(f'KNN: {time.time() - start}')\n",
    "\n",
    "# start = time.time()\n",
    "# brr = BayesianRidge(\n",
    "#     alpha_1 = best_params['brr_alpha_1'],\n",
    "#     alpha_2 = best_params['brr_alpha_2'],\n",
    "#     lambda_1 = best_params['brr_lambda_1'],\n",
    "#     lambda_2 = best_params['brr_lambda_2'],\n",
    "#     )\n",
    "# brr.fit(X_train, y_train)\n",
    "# brr_pred = brr.predict(all_embeddings)\n",
    "# print(f'BRR: {time.time() - start}')\n",
    "# \n",
    "# brr_weight = 1 - best_params['knn_weight']\n",
    "# final_pred = ((best_params['knn_weight'] * knn_pred) + (brr_weight * brr_pred))\n",
    "\n",
    "books_df['knn_rating'] = knn_pred\n",
    "books_df['brr_rating'] = brr_pred\n",
    "books_df['svr_rating'] = svr_pred\n",
    "books_df['final_rating'] = final_pred\n",
    "# books_df.sort_values(by='final_rating', ascending=False)\n",
    "\n",
    "cols = ['title'] + [col for col in books_df.columns if col.endswith('rating')]\n",
    "books_df[cols].sort_values(by='final_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"0.6 MRL sliced\n",
    "Best Loss: 1.5748: 100%|██████████| 300/300 [10:29<00:00,  2.10s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 37,\n",
    " 'brr_alpha_1': 0.0002978589454364143,\n",
    " 'brr_alpha_2': 0.0005679816409316726,\n",
    " 'brr_lambda_1': 0.0003994602308099561,\n",
    " 'brr_lambda_2': 0.003659493431608586,\n",
    " 'brr_uncertainty_penalty': 1.3409360588302992,\n",
    " 'svr_C': 0.009413204523151376,\n",
    " 'svr_epsilon': 0.5861865063902226,\n",
    " 'knn_weight': 0.6849389456290866,\n",
    " 'brr_weight': 0.7766436222538672}\"\"\"\n",
    "cols = ['title'] + [col for col in books_df.columns if col.endswith('rating')]\n",
    "books_df[cols].sort_values(by='final_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"0.6 full\n",
    "Best Loss: 1.5926: 100%|██████████| 300/300 [12:22<00:00,  2.48s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 55,\n",
    " 'brr_alpha_1': 0.0007517429857762481,\n",
    " 'brr_alpha_2': 0.0003941767071639492,\n",
    " 'brr_lambda_1': 8.604754211823616e-06,\n",
    " 'brr_lambda_2': 0.008083208576838542,\n",
    " 'brr_uncertainty_penalty': 1.048016018935488,\n",
    " 'svr_C': 0.013400702917698843,\n",
    " 'svr_epsilon': 0.4149283525703265,\n",
    " 'knn_weight': 0.6826121601964188,\n",
    " 'brr_weight': 0.8414845392757134}\"\"\"\n",
    "cols = ['title'] + [col for col in books_df.columns if col.endswith('rating')]\n",
    "books_df[cols].sort_values(by='final_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"8 MRL sliced\n",
    "Best Loss: 1.5714: 100%|██████████| 300/300 [11:49<00:00,  2.37s/it]\n",
    "{'num_propagations': 0,\n",
    " 'knn_neighbors': 17,\n",
    " 'brr_alpha_1': 0.0006421576956932416,\n",
    " 'brr_alpha_2': 0.0004509358497670652,\n",
    " 'brr_lambda_1': 8.24637589893822e-06,\n",
    " 'brr_lambda_2': 0.0001968595842801841,\n",
    " 'brr_uncertainty_penalty': 1.3679433872638418,\n",
    " 'svr_C': 0.004026523432673821,\n",
    " 'svr_epsilon': 0.3118081208045217,\n",
    " 'knn_weight': 0.5805897543886746,\n",
    " 'brr_weight': 0.7793002684038448}\"\"\"\n",
    "cols = ['title'] + [col for col in books_df.columns if col.endswith('rating')]\n",
    "books_df[cols].sort_values(by='final_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"8 full\n",
    "Best Loss: 1.6065: 100%|██████████| 300/300 [16:57<00:00,  3.39s/it]\n",
    " {'num_propagations': 0,\n",
    " 'knn_neighbors': 36,\n",
    " 'brr_alpha_1': 0.0006880608281295565,\n",
    " 'brr_alpha_2': 0.0003014766980578955,\n",
    " 'brr_lambda_1': 1.3770534826263817e-05,\n",
    " 'brr_lambda_2': 0.005746291033999427,\n",
    " 'brr_uncertainty_penalty': 1.8570930538472121,\n",
    " 'svr_C': 0.03383255010384741,\n",
    " 'svr_epsilon': 0.44291277801367485,\n",
    " 'knn_weight': 0.5848794696920626,\n",
    " 'brr_weight': 0.5411997331122358}\"\"\"\n",
    "cols = ['title'] + [col for col in books_df.columns if col.endswith('rating')]\n",
    "books_df[cols].sort_values(by='final_rating', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop\n",
    "import nevergrad as ng\n",
    "from sklearn.preprocessing import Normalizer, StandardScaler\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.svm import SVR\n",
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "\n",
    "def objective(num_propagations, \n",
    "              pls_n_components, \n",
    "              knn_neighbors, knn_leaf_size,\n",
    "              svr_C, svr_epsilon,\n",
    "              brr_alpha_1, brr_alpha_2, brr_lambda_1, brr_lambda_2,\n",
    "              ensemble_logits,\n",
    "              ):\n",
    "    \n",
    "    all_embeddings = precomputed_embeddings[num_propagations]\n",
    "    X = all_embeddings[my_ratings_mask]\n",
    "    y = my_ratings\n",
    "\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    skf = KFold(n_splits=10, shuffle=True, random_state=42)\n",
    "    for train_idx, test_idx in skf.split(X):\n",
    "        X_train, X_test = X[train_idx], X[test_idx]\n",
    "        y_train, y_test = y[train_idx], y[test_idx]\n",
    "\n",
    "        knn = KNeighborsRegressor(\n",
    "            n_neighbors=knn_neighbors,\n",
    "            leaf_size=knn_leaf_size,\n",
    "            metric='cosine', \n",
    "            weights='distance',\n",
    "            n_jobs=-1,\n",
    "            )\n",
    "        knn.fit(X_train, y_train)\n",
    "        knn_pred = knn.predict(X_test)\n",
    "\n",
    "        svr = SVR(kernel='rbf', \n",
    "            C=svr_C, \n",
    "            epsilon=svr_epsilon\n",
    "            ) \n",
    "        svr.fit(X_train, y_train)\n",
    "        svr_pred = svr.predict(X_test)\n",
    "\n",
    "        brr = BayesianRidge(\n",
    "            alpha_1=brr_alpha_1,\n",
    "            alpha_2=brr_alpha_2,\n",
    "            lambda_1=brr_lambda_1,\n",
    "            lambda_2=brr_lambda_2,\n",
    "            compute_score=True\n",
    "            )\n",
    "        brr.fit(X_train, y_train)\n",
    "        brr_pred = brr.predict(X_test)\n",
    "\n",
    "        n_comps = min(pls_n_components, len(X_train) - 1)\n",
    "        pls = PLSRegression(\n",
    "            n_components=n_comps,\n",
    "            scale=False,\n",
    "            )\n",
    "        pls.fit(X_train, y_train)\n",
    "        pls_pred = pls.predict(X_test).flatten()\n",
    "\n",
    "\n",
    "        full_logits = np.concatenate(([0.0], ensemble_logits))\n",
    "        exp_vals = np.exp(full_logits - np.max(full_logits))\n",
    "        weights = exp_vals / np.sum(exp_vals)\n",
    "        knn_weight, svr_weight, brr_weight, pls_weight = weights\n",
    "\n",
    "        final_pred = (knn_weight * knn_pred)+ (svr_weight * svr_pred) + (brr_weight * brr_pred) + (pls_weight * pls_pred) \n",
    "\n",
    "        y_trues.append(y_test)\n",
    "        y_preds.append(final_pred)\n",
    "\n",
    "    y_trues = np.concatenate(y_trues)\n",
    "    y_preds = np.concatenate(y_preds)\n",
    "    y_preds = np.clip(y_preds, 1, 5)\n",
    "    \n",
    "    mse = mean_squared_error(y_trues, y_preds)\n",
    "    ndcg = ndcg_score([y_trues], [y_preds])\n",
    "    if np.std(y_preds) < 1e-9:\n",
    "        spearman = 0.0\n",
    "    else:\n",
    "        spearman, _ = spearmanr(y_trues, y_preds)\n",
    "        if np.isnan(spearman): spearman = 0.0\n",
    "\n",
    "    return mse + (1.0 - spearman) + (1.0 - ndcg)\n",
    "\n",
    "MAX_PROPAGATIONS = 3\n",
    "propagated = embeddings.clone()\n",
    "norm_l2 = Normalizer(norm='l2')\n",
    "precomputed_embeddings = [norm_l2.transform(propagated.numpy())]\n",
    "\n",
    "for _ in range(MAX_PROPAGATIONS):\n",
    "    propagated = torch.sparse.mm(adj_matrix, propagated)\n",
    "    precomputed_embeddings.append(norm_l2.transform(propagated.numpy()))\n",
    "del propagated\n",
    "\n",
    "my_ratings_mask = ~books_df['my_rating'].isna()\n",
    "my_ratings = books_df.loc[my_ratings_mask, 'my_rating'].values\n",
    "\n",
    "parametrization = ng.p.Instrumentation(\n",
    "    num_propagations = ng.p.Scalar(lower=0, upper=MAX_PROPAGATIONS).set_integer_casting(),\n",
    "\n",
    "    knn_neighbors = ng.p.Scalar(lower=3, upper=mrl_dimensions//2).set_integer_casting(),\n",
    "    knn_leaf_size = ng.p.Scalar(lower=3, upper=mrl_dimensions//2).set_integer_casting(),\n",
    "    pls_n_components = ng.p.Scalar(lower=1, upper=mrl_dimensions//2).set_integer_casting(),\n",
    "    svr_C = ng.p.Log(lower=1e-3, upper=1e3),\n",
    "    svr_epsilon = ng.p.Scalar(lower=0.0, upper=1.0),\n",
    "    brr_alpha_1=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    brr_alpha_2=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    brr_lambda_1=ng.p.Log(lower=1e-6, upper=1e-1),\n",
    "    brr_lambda_2=ng.p.Log(lower=1e-6, upper=1e-1),\n",
    "\n",
    "    ensemble_logits = ng.p.Array(shape=(3,)), \n",
    ")\n",
    "\n",
    "BUDGET = 300\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=BUDGET)\n",
    "best_loss = float('inf')\n",
    "with tqdm(total=BUDGET) as pbar:\n",
    "    for i in range(BUDGET):\n",
    "        x = optimizer.ask()\n",
    "        loss = objective(*x.args, **x.kwargs)\n",
    "        optimizer.tell(x, loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "        pbar.update(1)\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_description(f\"Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "best_params = optimizer.provide_recommendation().kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global score\n",
    "C = books_df['avg_rating'].mean()\n",
    "m = books_df['rating_count'].quantile(0.10) \n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = float(x['rating_count'])\n",
    "    R = float(x['avg_rating'])\n",
    "    if v == 0: \n",
    "        return C\n",
    "    return (v / (v + m) * R) + (m / (v + m) * C)\n",
    "books_df['global_score'] = books_df.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_export = pd.read_csv('data/goodreads_library_export.csv')\n",
    "# goodreads_export = pd.read_csv('data/20-01-2025_goodreads_library_export.csv')\n",
    "# goodreads_export = goodreads_export[['Book Id', 'Author', 'My Rating', 'Number of Pages', 'Original Publication Year']]\n",
    "# goodreads_export = goodreads_export.rename(columns={'Book Id':'book_id',\n",
    "#                                                     'Author': 'author',\n",
    "#                                                     'My Rating': 'my_rating',\n",
    "#                                                     'Number of Pages': 'num_pages',\n",
    "#                                                     'Original Publication Year': 'year'})\n",
    "threshold = (goodreads_export['number_of_pages'].mean() - goodreads_export['number_of_pages'].std())\n",
    "goodreads_export.loc[goodreads_export['number_of_pages'] < threshold, 'number_of_pages'] = np.nan\n",
    "\n",
    "book_df = pd.read_csv('data/books.csv')\n",
    "# book_df = pd.read_csv('data/01-2025_goodreads_scraped.csv')\n",
    "df = goodreads_export.merge(book_df, on='book_id')\n",
    "\n",
    "# Drop competing columns\n",
    "df['author'] = df['author_x'].fillna(df['author_y'])\n",
    "df['num_pages'] = df['num_pages_x'].fillna(df['num_pages_y'])\n",
    "df['year'] = df['year_x'].fillna(df['year_y'])\n",
    "df.drop(columns=['author_x', 'author_y', 'num_pages_x', 'num_pages_y', 'year_x', 'year_y'], inplace=True)\n",
    "\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df['year'] = df['year'].fillna(df['year'].mean()).round().astype(int)\n",
    "df['num_pages'] = df['num_pages'].fillna(df['num_pages'].mean()).round().astype(int)\n",
    "df['num_reviews'] = df['num_reviews'].fillna(0).round().astype(int)\n",
    "df['my_rating'] = df['my_rating'].replace(0,np.nan)\n",
    "\n",
    "df['age'] = int(datetime.now().strftime('%Y')) - df['year']\n",
    "df['average_rating'] = ((df['5 stars'] * 5) + (df['4 stars'] * 4) + (df['3 stars'] * 3) + (df['2 stars'] * 2) + df['1 star']) / df['num_ratings']\n",
    "df = df[['book_id', 'title', 'author', 'year', 'age', 'series', 'num_pages', 'genres', 'num_ratings', 'num_reviews', 'my_rating', 'average_rating', '5 stars', '4 stars', '3 stars', '2 stars', '1 star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quadratic(row):\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    a, b, c = np.polyfit(x, row, 2)\n",
    "    return pd.Series([a, b, c])\n",
    "\n",
    "# Calculating quadrdic modeling coefficients\n",
    "df['1_star_percentage'] = df['1 star'] / df['num_ratings']\n",
    "df['2_star_percentage'] = df['2 stars'] / df['num_ratings']\n",
    "df['3_star_percentage'] = df['3 stars'] / df['num_ratings']\n",
    "df['4_star_percentage'] = df['4 stars'] / df['num_ratings']\n",
    "df['5_star_percentage'] = df['5 stars'] / df['num_ratings']\n",
    "coefficients = df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "df['a'], df['b'], df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "\n",
    "# Pre-processing columns for rankings\n",
    "df['num_ratings_ln'] = np.log1p(df['num_ratings'])\n",
    "df['num_pages_ln'] = np.log1p(df['num_pages'])\n",
    "df['2a_shifted'] = df['a'] - df['a'].min()\n",
    "df['2a_shifted'] = df['2a_shifted'] * (1 / df['2a_shifted'].max()) + 1\n",
    "df['b_shifted'] = df['b'] - df['b'].min()\n",
    "df['b_shifted'] = df['b_shifted'] * (1 / df['b_shifted'].max()) + 1\n",
    "df['c_shifted'] = df['c'] - df['c'].min()\n",
    "df['c_shifted'] = df['c_shifted'] * (1 / df['c_shifted'].max()) + 1\n",
    "\n",
    "# Types of rankings\n",
    "df['num_adjusted_rating'] = df['average_rating'] - (df['average_rating'] - df['average_rating'].mean()) / df['num_ratings_ln']\n",
    "df['coeff_2a_rating'] = (df['num_adjusted_rating'] * df['2a_shifted'])\n",
    "df['coeff_b_rating'] = (df['num_adjusted_rating']) / (df['b_shifted'])\n",
    "df['coeff_c_rating'] = (df['num_adjusted_rating'] * df['c_shifted'])\n",
    "df['joined_rating'] = (df['num_adjusted_rating'] * df['c_shifted'] * df['2a_shifted']) / df['b_shifted']\n",
    "df['final_rating'] = df['joined_rating'] - (df['joined_rating'] - df['joined_rating'].mean()) / df['num_ratings_ln']\n",
    "\n",
    "df['num_adjusted_page_rating'] = df['num_adjusted_rating'] / (df['num_pages_ln'])\n",
    "df['coeff_2a_page_rating'] = df['coeff_2a_rating'] / df['num_pages_ln']\n",
    "df['coeff_b_page_rating'] = df['coeff_b_rating'] / df['num_pages_ln']\n",
    "df['coeff_c_page_rating'] = df['coeff_c_rating'] / df['num_pages_ln']\n",
    "df['joined_page_rating'] = df['joined_rating'] / df['num_pages_ln']\n",
    "df['final_page_rating'] = df['joined_page_rating'] - (df['joined_page_rating'] - df['joined_page_rating'].mean()) / df['num_ratings_ln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['age', 'num_pages', 'num_pages_ln', 'num_ratings', 'num_ratings_ln', 'num_reviews', 'my_rating', 'average_rating', '1 star', '2 stars', '3 stars', '4 stars', '5 stars', '1_star_percentage', '2_star_percentage', '3_star_percentage', '4_star_percentage', '5_star_percentage', 'a', 'b', 'c', 'num_adjusted_rating', 'coeff_2a_rating', 'coeff_b_rating', 'coeff_c_rating', 'joined_rating', 'final_rating', 'num_adjusted_page_rating', 'coeff_2a_page_rating', 'coeff_b_page_rating', 'coeff_c_page_rating', 'joined_page_rating', 'final_page_rating']\n",
    "corr_df= df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 15)) \n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Correlation Heatmap') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = df.sort_values(by='final_page_rating', ascending=False).reset_index().drop('index', axis=1)\n",
    "fresh = fresh[fresh['my_rating'].isna()]\n",
    "fresh[['Fiction' in genre_list for genre_list in fresh['genres']]] # Fiction, Nonfiction, Memoir, Classics, History, Politics, Philosophy, Business"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
