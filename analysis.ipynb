{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "books_df = pd.read_csv(\"data/books.csv\")\n",
    "numeric_map = {\n",
    "    'book_id': 'UInt32',\n",
    "    'review_count': 'UInt32',\n",
    "    'num_pages': 'UInt16',\n",
    "    'year': 'Int16',\n",
    "}\n",
    "for col, dtype in numeric_map.items():\n",
    "    books_df[col] = books_df[col].astype(dtype)\n",
    "\n",
    "star_cols = [c for c in books_df.columns if c.endswith('star')]\n",
    "for col in star_cols:\n",
    "    books_df[col] = books_df[col].astype('UInt32')    \n",
    "books_df['rating_count'] = books_df[star_cols].sum(axis=1)\n",
    "\n",
    "# Todo: Fix the crawler to split langs with \"|\" by default\n",
    "books_df['lang'] = [\n",
    "    \"|\".join(item.strip() for item in x.split(\";\")) if isinstance(x, str) else x \n",
    "    for x in books_df['lang']\n",
    "]\n",
    "books_df['description'] = books_df['description'].str.replace('\\n\\n', '\\n')\n",
    "books_df['description'] = books_df['description'].str.replace('\\n', ' ')\n",
    "books_df['description'] = books_df['description'].str.replace('   ', ' ')\n",
    "books_df['description'] = books_df['description'].str.replace('  ', ' ')\n",
    "\n",
    "# Goodreads export\n",
    "goodreads_df = pd.read_csv('data/goodreads_library_export.csv')\n",
    "goodreads_df['book_id'] = goodreads_df['book_id'].astype('UInt32')\n",
    "goodreads_df['my_rating'] = goodreads_df['my_rating'].astype('UInt8')\n",
    "\n",
    "books_df = books_df.merge(goodreads_df[['book_id', 'my_rating']], on='book_id', how='left')\n",
    "books_df['my_rating'] = books_df['my_rating'].replace(0, np.nan)\n",
    "\n",
    "books_df = books_df[~books_df['similar_books'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep embedding strings\n",
    "def format_string_for_embedding(items, kind=None, truncate=0):\n",
    "    if not isinstance(items, (list)) or len(items) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    n = len(items)\n",
    "    if n == 1:\n",
    "        res = items[0]\n",
    "    elif n > truncate > 1:\n",
    "        res = f\"{', '.join(items[:truncate])}, and {items[truncate]}\"\n",
    "    else:\n",
    "        res = f\"{', '.join(items[:-1])}{',' if n > 2 else ''} and {items[-1]}\"\n",
    "    \n",
    "    prefix = f\"{kind.capitalize()}{'s' if n > 1 else ''}: \" if kind else \"\"\n",
    "    return f\"{prefix}{res}\"\n",
    "\n",
    "books_df['authors_post'] = books_df['authors'].str.split('|')\n",
    "books_df['authors_post'] = books_df['authors_post'].apply(lambda x:format_string_for_embedding(x, truncate=4))\n",
    "\n",
    "books_df['genres_post'] = books_df['genres'].str.split('|')\n",
    "books_df['genres_post'] = books_df['genres_post'].apply(lambda x:format_string_for_embedding(x, kind='genre'))\n",
    "\n",
    "books_df['desc_post'] = [[desc] if isinstance(desc, str) else [] for desc in books_df['description']]\n",
    "books_df['desc_post'] = books_df['desc_post'].apply(lambda x:format_string_for_embedding(x, kind='description'))\n",
    "\n",
    "def join_embedding_parts(title, authors, genres, desc):\n",
    "    text = f\"Book: {title}\\n\"\n",
    "    if authors:\n",
    "        text += f\"Written by: {authors}\\n\"\n",
    "    if genres:\n",
    "        text += f\"{genres}\\n\"\n",
    "    if desc:\n",
    "        text += f\"{desc}\" \n",
    "    return text\n",
    "\n",
    "books_df['embedding_input'] = [\n",
    "    join_embedding_parts(t, a, g, d) \n",
    "    for t, a, g, d in zip(books_df['title'], books_df['authors_post'], books_df['genres_post'], books_df['desc_post'])\n",
    "]\n",
    "\n",
    "id_to_string = books_df.set_index('book_id')['embedding_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed sentences\n",
    "import os\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "PARAMS = 0.6\n",
    "OLLAMA_MODEL = f\"qwen3-embedding:{PARAMS}b\"\n",
    "MIN_DIMENSIONS = 32\n",
    "\n",
    "my_rating_count = (~books_df['my_rating'].isna()).sum()\n",
    "sent_embedding_dimensions = MIN_DIMENSIONS\n",
    "while sent_embedding_dimensions*2 < my_rating_count:\n",
    "    sent_embedding_dimensions*=2\n",
    "\n",
    "embeddings_path = f'data/{PARAMS}b_embeddings.csv'\n",
    "if os.path.exists(embeddings_path):\n",
    "    embeddings = pd.read_csv(embeddings_path).set_index('book_id')\n",
    "else:\n",
    "    embeddings = pd.DataFrame()\n",
    "\n",
    "current_ids = books_df['book_id'].values\n",
    "missing_ids = [idx for idx in current_ids if idx not in embeddings.index]\n",
    "if missing_ids:\n",
    "    missing_strings = id_to_string.loc[missing_ids].tolist()\n",
    "    batch_size = 128\n",
    "    new_embeddings = []\n",
    "    for i in tqdm(range(0, len(missing_strings), batch_size)):\n",
    "        batch = missing_strings[i : i + batch_size]\n",
    "        response = ollama.embed(model=OLLAMA_MODEL, input=batch)\n",
    "        new_embeddings.extend(response['embeddings'])\n",
    "\n",
    "    new_embeddings = pd.DataFrame(new_embeddings, index=missing_ids)\n",
    "    new_embeddings.index.name = 'book_id'\n",
    "    embeddings = pd.concat([embeddings, new_embeddings])\n",
    "    embeddings.to_csv(embeddings_path)\n",
    "    del new_embeddings, missing_strings, missing_ids, current_ids\n",
    "\n",
    "embeddings = embeddings.loc[books_df['book_id']].values\n",
    "embeddings = torch.tensor(embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency matrix\n",
    "id_to_idx = {id: i for i, id in enumerate(books_df['book_id'])}\n",
    "\n",
    "edge_indices = []\n",
    "for idx, row in tqdm(books_df.iterrows(), total=len(books_df)):\n",
    "    current_idx = id_to_idx[row['book_id']]\n",
    "    if pd.isna(row['similar_books']):\n",
    "        continue\n",
    "    for item in row['similar_books'].split('|'):\n",
    "        try:\n",
    "            target_id = int(item.split(':')[0])\n",
    "            if target_id in books_df['book_id'].values:\n",
    "                target_idx = id_to_idx[target_id]\n",
    "                edge_indices.append([current_idx, target_idx])\n",
    "                edge_indices.append([target_idx, current_idx])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "\n",
    "if not edge_indices:\n",
    "    edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "else:\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Multiplying/Smoothing embeddings with adjacency matrix\n",
    "# from torch_geometric.utils import add_self_loops\n",
    "# from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "# edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=embeddings.size(0))\n",
    "# edge_index_norm, edge_weight_norm = gcn_norm(edge_index_with_loops, num_nodes=embeddings.size(0))\n",
    "# adj_matrix = torch.sparse_coo_tensor(edge_index_norm, edge_weight_norm, (embeddings.size(0), embeddings.size(0)))\n",
    "\n",
    "# final_embeddings = embeddings\n",
    "# num_propagations = 1\n",
    "# for _ in range(num_propagations):\n",
    "#     final_embeddings = torch.sparse.mm(adj_matrix, final_embeddings)\n",
    "# final_embeddings = final_embeddings.numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Dimensionality Reduction\n",
    "# from sklearn.preprocessing import StandardScaler\n",
    "# import umap.umap_ as umap\n",
    "\n",
    "# y_umap = books_df['my_rating'].fillna(-1).values\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(final_embeddings)\n",
    "# reducer = umap.UMAP(\n",
    "#     n_neighbors=15,\n",
    "#     n_components=MIN_DIMENSIONS,\n",
    "#     metric='cosine', \n",
    "#     target_metric='l1',\n",
    "#     target_weight=0.5,\n",
    "#     random_state=42\n",
    "# )\n",
    "# X_reduced = reducer.fit_transform(X_scaled, y=y_umap)\n",
    "\n",
    "# # Global score\n",
    "# C = books_df['avg_rating'].mean()\n",
    "# m = books_df['rating_count'].quantile(0.10) \n",
    "# def weighted_rating(x, m=m, C=C):\n",
    "#     v = float(x['rating_count'])\n",
    "#     R = float(x['avg_rating'])\n",
    "#     if v == 0: \n",
    "#         return C\n",
    "#     return (v / (v + m) * R) + (m / (v + m) * C)\n",
    "# books_df['global_score'] = books_df.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.linear_model import BayesianRidge\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from sklearn.linear_model import ARDRegression\n",
    "# from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "# train_mask = books_df['my_rating'].notna() & (books_df['my_rating'] > 0)\n",
    "# X_train = X_reduced[train_mask]\n",
    "# y_train = books_df.loc[train_mask, 'my_rating'].values\n",
    "\n",
    "# # Bayesian Ridge Regression\n",
    "# brr = BayesianRidge(compute_score=True)\n",
    "# brr.fit(X_train, y_train)\n",
    "# means, stds = brr.predict(X_reduced, return_std=True)\n",
    "# books_df['brr_pred_rating'] = means\n",
    "# books_df['brr_uncertainty'] = stds\n",
    "# books_df['brr_score'] = books_df['brr_pred_rating'] - (books_df['brr_uncertainty'])\n",
    "\n",
    "# nonzero_weights = np.sum(np.abs(brr.coef_) > 1e-5)\n",
    "# print(f\"BRR used {nonzero_weights} of {X_reduced.shape[1]} dimensions.\")\n",
    "\n",
    "# # Bayesian ARD\n",
    "# ard = ARDRegression(compute_score=True)\n",
    "# ard.fit(X_train, y_train)\n",
    "# means, stds = ard.predict(X_reduced, return_std=True)\n",
    "# books_df['brr_pred_rating'] = means\n",
    "# books_df['brr_uncertainty'] = stds\n",
    "# books_df['brr_score'] = books_df['brr_pred_rating'] - (books_df['brr_uncertainty'])\n",
    "\n",
    "# nonzero_weights = np.sum(np.abs(ard.coef_) > 1e-5)\n",
    "# print(f\"ARD used {nonzero_weights} of {X_reduced.shape[1]} dimensions.\")\n",
    "\n",
    "# # Random Forest\n",
    "# rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "# rf.fit(X_train, y_train)\n",
    "# means = rf.predict(X_reduced)\n",
    "# per_tree_preds = np.stack([tree.predict(X_reduced) for tree in rf.estimators_])\n",
    "# stds = np.std(per_tree_preds, axis=0)\n",
    "# books_df['rf_pred_rating'] = means\n",
    "# books_df['rf_uncertainty'] = stds\n",
    "# books_df['rf_score'] = books_df['rf_pred_rating'] - (books_df['rf_uncertainty'])\n",
    "\n",
    "# # Weighted KNN Regressor\n",
    "# knn = KNeighborsRegressor(n_neighbors=15, weights='distance', metric='cosine')\n",
    "# knn.fit(X_train, y_train)\n",
    "# means = knn.predict(X_reduced)\n",
    "# neighbor_indices = knn.kneighbors(X_reduced, return_distance=False)\n",
    "# neighbor_ratings = y_train[neighbor_indices] \n",
    "# stds = np.std(neighbor_ratings, axis=1)\n",
    "# books_df['knn_pred_rating'] = means\n",
    "# books_df['knn_uncertainty'] = stds\n",
    "# books_df['knn_score'] = books_df['knn_pred_rating'] - (books_df['knn_uncertainty'])\n",
    "\n",
    "# books_df\n",
    "# # books_df['final_score'] = (books_df['knn_score']/6) + (books_df['brr_score']/6)  + (books_df['gpr_score']/6) + (0.5 * books_df['global_score'])    \n",
    "# # books_df[~train_mask][['book_id', 'title', 'my_rating', 'final_score', 'knn_pred_rating', 'knn_score', 'knn_uncertainty', 'brr_pred_rating', 'brr_score', 'brr_uncertainty', 'gpr_pred_rating', 'gpr_score', 'gpr_uncertainty']].sort_values(by='final_score', ascending=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nevergrad as ng\n",
    "import numpy as np\n",
    "import umap\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from scipy.stats import rankdata\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def objective(\n",
    "    # GCN feature blending (now with 4 levels)\n",
    "    gcn_weight_0, gcn_weight_1, gcn_weight_2, gcn_weight_3,\n",
    "    \n",
    "    # UMAP hyperparameters\n",
    "    umap_neighbors,\n",
    "    umap_min_dist,\n",
    "    umap_components,\n",
    "    umap_learning_rate,\n",
    "    umap_negative_sample_rate,\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    knn_k,\n",
    "    knn_weights,\n",
    "    \n",
    "    ard_alpha_1,\n",
    "    ard_alpha_2,\n",
    "    ard_lambda_1,\n",
    "    ard_lambda_2,\n",
    "    ard_threshold_lambda,\n",
    "    \n",
    "    # Ensemble strategy\n",
    "    ensemble_method,\n",
    "    uncertainty_transform,\n",
    "    \n",
    "    # Ensemble weights\n",
    "    w_ard, w_knn,\n",
    "    \n",
    "    # Uncertainty penalties\n",
    "    u_ard, u_knn,\n",
    "):\n",
    "    \n",
    "    # Cast to appropriate types\n",
    "    umap_neighbors = max(5, int(umap_neighbors))\n",
    "    umap_components = max(2, int(umap_components))\n",
    "    knn_k = max(3, int(knn_k))\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "    for _, (train_idx, val_idx) in enumerate(kf.split(X_raw_emb_train)):\n",
    "        try:\n",
    "            # 1. Blend GCN features at multiple depths\n",
    "            w_sum = gcn_weight_0 + gcn_weight_1 + gcn_weight_2 + gcn_weight_3\n",
    "            X_emb_blended = (\n",
    "                gcn_weight_0 * X_raw_emb_train + \n",
    "                gcn_weight_1 * X_gcn_1_train + \n",
    "                gcn_weight_2 * X_gcn_2_train +\n",
    "                gcn_weight_3 * X_gcn_3_train\n",
    "            ) / w_sum\n",
    "            \n",
    "            X_emb_tr, X_emb_val = X_emb_blended[train_idx], X_emb_blended[val_idx]\n",
    "            y_tr, y_val = y_train_base[train_idx], y_train_base[val_idx]\n",
    "            \n",
    "            max_components = min(umap_components, len(y_tr) - 2)\n",
    "\n",
    "            # 2. Normalize and scale embeddings\n",
    "            l2_norm = Normalizer(norm='l2')\n",
    "            X_emb_tr = l2_norm.transform(X_emb_tr)\n",
    "            X_emb_val = l2_norm.transform(X_emb_val)\n",
    "            # scaler = StandardScaler()\n",
    "            # X_emb_tr_scaled = scaler.fit_transform(X_emb_tr)\n",
    "            # X_emb_val_scaled = scaler.transform(X_emb_val)\n",
    "            \n",
    "            # 3. UMAP\n",
    "            reducer = umap.UMAP(\n",
    "                n_neighbors=min(umap_neighbors, len(y_tr) - 1),\n",
    "                min_dist=umap_min_dist,\n",
    "                n_components=max_components,\n",
    "                metric='cosine',\n",
    "                learning_rate=umap_learning_rate,\n",
    "                negative_sample_rate=int(umap_negative_sample_rate),\n",
    "                random_state=42,\n",
    "                n_jobs=1,\n",
    "                verbose=False\n",
    "            )\n",
    "            X_tr_final = reducer.fit_transform(X_emb_tr)\n",
    "            X_val_final = reducer.transform(X_emb_val)\n",
    "            \n",
    "            # 4. Train models\n",
    "            # ARD with hyperparameters\n",
    "            ard = ARDRegression(\n",
    "                alpha_1=ard_alpha_1,\n",
    "                alpha_2=ard_alpha_2,\n",
    "                lambda_1=ard_lambda_1,\n",
    "                lambda_2=ard_lambda_2,\n",
    "                threshold_lambda=ard_threshold_lambda,\n",
    "                compute_score=True\n",
    "            )\n",
    "            ard.fit(X_tr_final, y_tr)\n",
    "            mu_ard, std_ard = ard.predict(X_val_final, return_std=True)\n",
    "            \n",
    "            # KNN with extended options\n",
    "            knn = KNeighborsRegressor(\n",
    "                n_neighbors=min(knn_k, len(y_tr) - 1), \n",
    "                weights=knn_weights,\n",
    "                metric='cosine',\n",
    "            )\n",
    "            knn.fit(X_tr_final, y_tr)\n",
    "            mu_knn = knn.predict(X_val_final)\n",
    "            neigh_idx = knn.kneighbors(X_val_final, return_distance=False)\n",
    "            std_knn = np.std(y_tr[neigh_idx], axis=1)\n",
    "            \n",
    "            # 5. Transform uncertainties\n",
    "            if uncertainty_transform == 'sqrt':\n",
    "                std_ard = np.sqrt(std_ard)\n",
    "                std_knn = np.sqrt(std_knn)\n",
    "            elif uncertainty_transform == 'square':\n",
    "                std_ard = std_ard ** 2\n",
    "                std_knn = std_knn ** 2\n",
    "            \n",
    "            # 6. Ensemble predictions\n",
    "            if ensemble_method == 'weighted':\n",
    "                final_pred = (w_ard * (mu_ard - u_ard * std_ard) + \n",
    "              w_knn * (mu_knn - u_knn * std_knn)) / (w_ard + w_knn + 1e-6)\n",
    "                # final_pred = np.sum(preds, axis=0)\n",
    "            \n",
    "            elif ensemble_method == 'rank':\n",
    "                # # Rank-based combination\n",
    "                # ranks = [\n",
    "                #     w_ard * rankdata(mu_ard - u_ard * std_ard),\n",
    "                #     w_knn * rankdata(mu_knn - u_knn * std_knn),\n",
    "                # ]\n",
    "                # final_pred = np.sum(ranks, axis=0)\n",
    "\n",
    "                # Rank-based combination\n",
    "                r_ard = rankdata(mu_ard - u_ard * std_ard)\n",
    "                r_knn = rankdata(mu_knn - u_knn * std_knn)\n",
    "                \n",
    "                # Normalize ranks to 0-1 range to be comparable to y_val for MSE check\n",
    "                r_ard = r_ard / len(r_ard)\n",
    "                r_knn = r_knn / len(r_knn)\n",
    "                \n",
    "                final_pred = (w_ard * r_ard + w_knn * r_knn) / (w_ard + w_knn + 1e-6)\n",
    "            \n",
    "            else:  # 'average'\n",
    "                final_pred = (mu_ard + mu_knn) / 2\n",
    "            \n",
    "            # 7. Multi-objective scoring: NDCG + MSE\n",
    "            ndcg = ndcg_score([y_val], [final_pred], k=min(20, len(y_val)))\n",
    "            mse = mean_squared_error(y_val, final_pred)\n",
    "            \n",
    "            # Combined score (NDCG primary, MSE secondary)\n",
    "            score = ndcg - 0.1 * np.sqrt(mse)\n",
    "            fold_scores.append(score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 1e6\n",
    "    \n",
    "    if len(fold_scores) == 0:\n",
    "        return 1e6\n",
    "    \n",
    "    return -np.mean(fold_scores)\n",
    "\n",
    "\n",
    "# DATA PREPARATION\n",
    "if isinstance(embeddings, torch.Tensor):\n",
    "    X_raw_emb = embeddings.detach().cpu().numpy()\n",
    "else:\n",
    "    X_raw_emb = np.array(embeddings)\n",
    "\n",
    "# Pre-calculate multiple GCN propagation depths\n",
    "X_gcn_1 = torch.sparse.mm(adj_matrix, torch.tensor(X_raw_emb, dtype=torch.float32)).numpy()\n",
    "X_gcn_2 = torch.sparse.mm(adj_matrix, torch.tensor(X_gcn_1, dtype=torch.float32)).numpy()\n",
    "X_gcn_3 = torch.sparse.mm(adj_matrix, torch.tensor(X_gcn_2, dtype=torch.float32)).numpy()\n",
    "\n",
    "# Extract training data\n",
    "y_full = books_df['my_rating'].values.astype(float)\n",
    "valid_mask = (~np.isnan(y_full)) & (y_full > 0)\n",
    "y_train_base = y_full[valid_mask]\n",
    "scaler = MinMaxScaler()\n",
    "y_train_base = scaler.fit_transform(y_train_base.reshape(-1, 1)).ravel()\n",
    "\n",
    "X_raw_emb_train = X_raw_emb[valid_mask]\n",
    "X_gcn_1_train = X_gcn_1[valid_mask]\n",
    "X_gcn_2_train = X_gcn_2[valid_mask]\n",
    "X_gcn_3_train = X_gcn_3[valid_mask]\n",
    "n_train_samples = len(y_train_base)\n",
    "\n",
    "max_umap_components = min(int((n_train_samples * 2 / 3)), sent_embedding_dimensions)\n",
    "parametrization = ng.p.Instrumentation(\n",
    "    # GCN blending (4 depths now)\n",
    "    gcn_weight_0=ng.p.Scalar(lower=0, upper=10),\n",
    "    gcn_weight_1=ng.p.Scalar(lower=0, upper=10),\n",
    "    gcn_weight_2=ng.p.Scalar(lower=0, upper=10),\n",
    "    gcn_weight_3=ng.p.Scalar(lower=0, upper=10),\n",
    "    \n",
    "    # UMAP with more hyperparameters\n",
    "    umap_neighbors=ng.p.Scalar(lower=5, upper=50).set_integer_casting(),\n",
    "    umap_min_dist=ng.p.Scalar(lower=0.0, upper=0.99),\n",
    "    umap_components=ng.p.Scalar(lower=3, upper=max_umap_components).set_integer_casting(),\n",
    "    umap_learning_rate=ng.p.Scalar(lower=0.1, upper=2.0),\n",
    "    umap_negative_sample_rate=ng.p.Scalar(lower=3, upper=20).set_integer_casting(),\n",
    "    \n",
    "    # KNN hyperparameters\n",
    "    knn_k=ng.p.Scalar(lower=3, upper=30).set_integer_casting(),\n",
    "    knn_weights=ng.p.Choice(['uniform', 'distance']),\n",
    "    \n",
    "    # ARD hyperparameters\n",
    "    ard_alpha_1=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_alpha_2=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_lambda_1=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_lambda_2=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_threshold_lambda=ng.p.Scalar(lower=1e3, upper=1e6),\n",
    "    \n",
    "    # Ensemble strategy\n",
    "    ensemble_method=ng.p.Choice(['weighted', 'rank', 'average']),\n",
    "    uncertainty_transform=ng.p.Choice(['none', 'sqrt']),\n",
    "    \n",
    "    # Ensemble weights (6 models now)\n",
    "    w_ard=ng.p.Scalar(lower=0, upper=2),\n",
    "    w_knn=ng.p.Scalar(lower=0, upper=2),\n",
    "    \n",
    "    # Uncertainty penalties\n",
    "    u_ard=ng.p.Scalar(lower=0, upper=5),\n",
    "    u_knn=ng.p.Scalar(lower=0, upper=5),\n",
    ")\n",
    "\n",
    "BUDGET = 300\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=BUDGET)\n",
    "with tqdm(total=BUDGET) as pbar:\n",
    "    for i in range(BUDGET):\n",
    "        x = optimizer.ask()\n",
    "        loss = objective(*x.args, **x.kwargs)\n",
    "        optimizer.tell(x, loss)\n",
    "        pbar.update(1)\n",
    "        if i % 20 == 0:\n",
    "            pbar.set_description(f\"Best Score: {-optimizer.current_bests['minimum'].mean:.4f}\")\n",
    "\n",
    "best_params = optimizer.provide_recommendation().kwargs\n",
    "param_groups = {\n",
    "    'GCN Blending': ['gcn_weight_0', 'gcn_weight_1', 'gcn_weight_2', 'gcn_weight_3'],\n",
    "    'UMAP': [k for k in best_params.keys() if k.startswith('umap_')],\n",
    "    'KNN': [k for k in best_params.keys() if k.startswith('knn_')],\n",
    "    'ARD': [k for k in best_params.keys() if k.startswith('ard_')],\n",
    "    'Ensemble': ['ensemble_method', 'uncertainty_transform'] + [k for k in best_params.keys() if k.startswith('w_') or k.startswith('u_')]\n",
    "}\n",
    "\n",
    "# # Save best parameters\n",
    "# import json\n",
    "# with open('best_hyperparameters.json', 'w') as f:\n",
    "#     params_to_save = {k: (float(v) if isinstance(v, (np.floating, np.integer)) else v) \n",
    "#                       for k, v in best_params.items()}\n",
    "#     json.dump(params_to_save, f, indent=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.model_selection import cross_val_score, KFold\n",
    "# from sklearn.linear_model import BayesianRidge\n",
    "# from sklearn.neighbors import KNeighborsRegressor\n",
    "# from tqdm import auto\n",
    "\n",
    "# # Setup\n",
    "# train_mask = books_df['my_rating'].notna() & (books_df['my_rating'] > 0)\n",
    "# y_train_full = books_df.loc[train_mask, 'my_rating'].values\n",
    "\n",
    "# brr_scores = []\n",
    "# knn_scores = []\n",
    "# candidates = np.arange(10,61,2)\n",
    "# for dim in (candidates):\n",
    "#     reducer = umap.UMAP(\n",
    "#         n_neighbors=15, \n",
    "#         n_components=dim, \n",
    "#         metric='cosine', \n",
    "#         target_metric='l1', \n",
    "#         target_weight=0.5, \n",
    "#         # random_state=42\n",
    "#     )\n",
    "#     X_reduced_cv = reducer.fit_transform(X_scaled, y=y_umap)\n",
    "#     X_train_cv = X_reduced_cv[train_mask]\n",
    "    \n",
    "#     brr = BayesianRidge()\n",
    "#     score_brr = -cross_val_score(brr, X_train_cv, y_train_full, cv=5, scoring='neg_root_mean_squared_error').mean()\n",
    "#     brr_scores.append(score_brr)\n",
    "    \n",
    "#     knn = KNeighborsRegressor(n_neighbors=10, weights='distance', metric='cosine')\n",
    "#     score_knn = -cross_val_score(knn, X_train_cv, y_train_full, cv=5, scoring='neg_root_mean_squared_error').mean()\n",
    "#     knn_scores.append(score_knn)\n",
    "\n",
    "#     score_both = score_brr + score_knn\n",
    "    \n",
    "#     print(f\"D:{dim:<5}, BRR:{score_brr:.4f}, KNN:{score_knn:.4f}, Comb:{score_both:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(books_df['book_id'].tolist())\n",
    "\n",
    "# 2. Build edges with correct type casting\n",
    "edge_list = []\n",
    "for idx, row in books_df.iterrows():\n",
    "    if pd.isna(row['similar_books']): continue\n",
    "    \n",
    "    for item in row['similar_books'].split('|'):\n",
    "        target_id_str = item.split(':')[0]\n",
    "        target_id = int(target_id_str)\n",
    "        \n",
    "        if target_id in books_df['book_id'].values:\n",
    "            edge_list.append((row['book_id'], target_id))\n",
    "\n",
    "G.add_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import SAGEConv\n",
    "\n",
    "# Prepare Node Features (Genre Embeddings) and Edge Index\n",
    "node_features = torch.tensor(np.stack(genre_embeddings.values), dtype=torch.float)\n",
    "node_mapping = {node: i for i, node in enumerate(books_df['book_id'].tolist())}\n",
    "edge_indices = []\n",
    "\n",
    "for u, v in G.edges():\n",
    "    if u in node_mapping and v in node_mapping:\n",
    "        edge_indices.append([node_mapping[u], node_mapping[v]])\n",
    "        edge_indices.append([node_mapping[v], node_mapping[u]]) # Undirected\n",
    "\n",
    "if len(edge_indices) == 0:\n",
    "    edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "else:\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# Define GraphSAGE Model\n",
    "class GraphSAGE(torch.nn.Module):\n",
    "    def __init__(self, in_channels, hidden_channels, out_channels):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(in_channels, hidden_channels)\n",
    "        self.conv2 = SAGEConv(hidden_channels, out_channels)\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        # Layer 1\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = F.relu(x)\n",
    "        # Layer 2\n",
    "        x = self.conv2(x, edge_index)\n",
    "        # Optional: L2 Normalize final embeddings\n",
    "        x = F.normalize(x, p=2, dim=1)\n",
    "        return x\n",
    "\n",
    "# Generate Embeddings\n",
    "model = GraphSAGE(in_channels=node_features.shape[1], hidden_channels=sent_embedding_dimensions, out_channels=sent_embedding_dimensions)\n",
    "model.eval()\n",
    "\n",
    "with torch.no_grad():\n",
    "    graph_embeddings_tensor = model(node_features, edge_index)\n",
    "\n",
    "final_embeddings = graph_embeddings_tensor.numpy()\n",
    "print(\"Embeddings shape:\", final_embeddings.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import normalize, StandardScaler\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import Matern, WhiteKernel\n",
    "import umap\n",
    "\n",
    "C = books_df['avg_rating'].mean()\n",
    "m = books_df['review_count'].quantile(0.10) # 10th percentile as minimum votes\n",
    "\n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = x['review_count']\n",
    "    R = x['avg_rating']\n",
    "    if v == 0: return C\n",
    "    return (v/(v+m) * R) + (m/(v+m) * C)\n",
    "\n",
    "books_df['global_score'] = books_df.apply(weighted_rating, axis=1)\n",
    "\n",
    "# PART C: The Personal Score (UMAP + GPR)\n",
    "\n",
    "# 1. Semi-Supervised UMAP\n",
    "# We need a target array. -1 signals \"unlabeled\" to UMAP.\n",
    "# This warps the space so your rated books clump together based on score.\n",
    "y_umap = books_df['my_rating'].fillna(-1).values\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X_scaled = scaler.fit_transform(final_embeddings)\n",
    "\n",
    "reducer = umap.UMAP(\n",
    "    n_neighbors=15,\n",
    "    n_components=sent_embedding_dimensions,\n",
    "    metric='cosine', \n",
    "    target_metric='l1', # Use L1 for the ratings\n",
    "    # target_weight=0.5,  # Balance structural shape vs rating shape\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "print(\"Running UMAP...\")\n",
    "X_reduced = reducer.fit_transform(X_scaled, y=y_umap)\n",
    "\n",
    "# 2. Gaussian Process Regression\n",
    "# Identify Training set (Books you read) vs Prediction set (Unread)\n",
    "train_mask = books_df['my_rating'].notna() & (books_df['my_rating'] > 0)\n",
    "X_train = X_reduced[train_mask]\n",
    "y_train = books_df.loc[train_mask, 'my_rating'].values\n",
    "\n",
    "# Kernel: Matern handles irregularities better than RBF. WhiteKernel handles noise.\n",
    "kernel = Matern(nu=1.5) + WhiteKernel(noise_level=0.1)\n",
    "gpr = GaussianProcessRegressor(kernel=kernel, normalize_y=True)\n",
    "\n",
    "print(\"Fitting GPR...\")\n",
    "gpr.fit(X_train, y_train)\n",
    "\n",
    "# Predict on ALL books\n",
    "means, stds = gpr.predict(X_reduced, return_std=True)\n",
    "\n",
    "books_df['pred_rating'] = means\n",
    "books_df['uncertainty'] = stds\n",
    "\n",
    "# PART D: Final Hybrid Scoring\n",
    "\n",
    "# Conservative Personal Score: Prediction minus Uncertainty\n",
    "# If model thinks it's 5.0 but is unsure (std=1.0), treats it as 4.0\n",
    "books_df['safe_personal_score'] = books_df['pred_rating'] - (books_df['uncertainty'] * 0.5)\n",
    "\n",
    "# Hybrid: 70% Personal Taste, 30% Global Quality\n",
    "books_df['final_score'] = (0.7 * books_df['safe_personal_score']) + (0.3 * books_df['global_score'])\n",
    "\n",
    "# Filter out books you've already read\n",
    "recs = books_df[~train_mask].sort_values('final_score', ascending=False)\n",
    "\n",
    "# Export\n",
    "recs[['title', 'authors', 'global_score', 'pred_rating', 'uncertainty', 'final_score']].head(20).to_csv(\"data/final_recommendations.csv\", index=False)\n",
    "print(\"Done. Saved top 20 to data/final_recommendations.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "\n",
    "def get_recommendations(title, embeddings, df, top_k=5):\n",
    "    idx = df[df['title'] == title].index[0]\n",
    "    sim_scores = cosine_similarity([embeddings[idx]], embeddings).flatten()\n",
    "    top_indices = sim_scores.argsort()[-(top_k+1):-1][::-1]\n",
    "    \n",
    "    return df.iloc[top_indices][['title', 'authors', 'genres']]\n",
    "\n",
    "# Test it out\n",
    "# print(get_recommendations(\"The Way of Kings\", final_embeddings, df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_export = pd.read_csv('data/goodreads_library_export.csv')\n",
    "# goodreads_export = pd.read_csv('data/20-01-2025_goodreads_library_export.csv')\n",
    "# goodreads_export = goodreads_export[['Book Id', 'Author', 'My Rating', 'Number of Pages', 'Original Publication Year']]\n",
    "# goodreads_export = goodreads_export.rename(columns={'Book Id':'book_id',\n",
    "#                                                     'Author': 'author',\n",
    "#                                                     'My Rating': 'my_rating',\n",
    "#                                                     'Number of Pages': 'num_pages',\n",
    "#                                                     'Original Publication Year': 'year'})\n",
    "threshold = (goodreads_export['number_of_pages'].mean() - goodreads_export['number_of_pages'].std())\n",
    "goodreads_export.loc[goodreads_export['number_of_pages'] < threshold, 'number_of_pages'] = np.nan\n",
    "\n",
    "book_df = pd.read_csv('data/books.csv')\n",
    "# book_df = pd.read_csv('data/01-2025_goodreads_scraped.csv')\n",
    "df = goodreads_export.merge(book_df, on='book_id')\n",
    "\n",
    "# Drop competing columns\n",
    "df['author'] = df['author_x'].fillna(df['author_y'])\n",
    "df['num_pages'] = df['num_pages_x'].fillna(df['num_pages_y'])\n",
    "df['year'] = df['year_x'].fillna(df['year_y'])\n",
    "df.drop(columns=['author_x', 'author_y', 'num_pages_x', 'num_pages_y', 'year_x', 'year_y'], inplace=True)\n",
    "\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df['year'] = df['year'].fillna(df['year'].mean()).round().astype(int)\n",
    "df['num_pages'] = df['num_pages'].fillna(df['num_pages'].mean()).round().astype(int)\n",
    "df['num_reviews'] = df['num_reviews'].fillna(0).round().astype(int)\n",
    "df['my_rating'] = df['my_rating'].replace(0,np.nan)\n",
    "\n",
    "df['age'] = int(datetime.now().strftime('%Y')) - df['year']\n",
    "df['average_rating'] = ((df['5 stars'] * 5) + (df['4 stars'] * 4) + (df['3 stars'] * 3) + (df['2 stars'] * 2) + df['1 star']) / df['num_ratings']\n",
    "df = df[['book_id', 'title', 'author', 'year', 'age', 'series', 'num_pages', 'genres', 'num_ratings', 'num_reviews', 'my_rating', 'average_rating', '5 stars', '4 stars', '3 stars', '2 stars', '1 star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quadratic(row):\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    a, b, c = np.polyfit(x, row, 2)\n",
    "    return pd.Series([a, b, c])\n",
    "\n",
    "# Calculating quadrdic modeling coefficients\n",
    "df['1_star_percentage'] = df['1 star'] / df['num_ratings']\n",
    "df['2_star_percentage'] = df['2 stars'] / df['num_ratings']\n",
    "df['3_star_percentage'] = df['3 stars'] / df['num_ratings']\n",
    "df['4_star_percentage'] = df['4 stars'] / df['num_ratings']\n",
    "df['5_star_percentage'] = df['5 stars'] / df['num_ratings']\n",
    "coefficients = df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "df['a'], df['b'], df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "\n",
    "# Pre-processing columns for rankings\n",
    "df['num_ratings_ln'] = np.log1p(df['num_ratings'])\n",
    "df['num_pages_ln'] = np.log1p(df['num_pages'])\n",
    "df['2a_shifted'] = df['a'] - df['a'].min()\n",
    "df['2a_shifted'] = df['2a_shifted'] * (1 / df['2a_shifted'].max()) + 1\n",
    "df['b_shifted'] = df['b'] - df['b'].min()\n",
    "df['b_shifted'] = df['b_shifted'] * (1 / df['b_shifted'].max()) + 1\n",
    "df['c_shifted'] = df['c'] - df['c'].min()\n",
    "df['c_shifted'] = df['c_shifted'] * (1 / df['c_shifted'].max()) + 1\n",
    "\n",
    "# Types of rankings\n",
    "df['num_adjusted_rating'] = df['average_rating'] - (df['average_rating'] - df['average_rating'].mean()) / df['num_ratings_ln']\n",
    "df['coeff_2a_rating'] = (df['num_adjusted_rating'] * df['2a_shifted'])\n",
    "df['coeff_b_rating'] = (df['num_adjusted_rating']) / (df['b_shifted'])\n",
    "df['coeff_c_rating'] = (df['num_adjusted_rating'] * df['c_shifted'])\n",
    "df['joined_rating'] = (df['num_adjusted_rating'] * df['c_shifted'] * df['2a_shifted']) / df['b_shifted']\n",
    "df['final_rating'] = df['joined_rating'] - (df['joined_rating'] - df['joined_rating'].mean()) / df['num_ratings_ln']\n",
    "\n",
    "df['num_adjusted_page_rating'] = df['num_adjusted_rating'] / (df['num_pages_ln'])\n",
    "df['coeff_2a_page_rating'] = df['coeff_2a_rating'] / df['num_pages_ln']\n",
    "df['coeff_b_page_rating'] = df['coeff_b_rating'] / df['num_pages_ln']\n",
    "df['coeff_c_page_rating'] = df['coeff_c_rating'] / df['num_pages_ln']\n",
    "df['joined_page_rating'] = df['joined_rating'] / df['num_pages_ln']\n",
    "df['final_page_rating'] = df['joined_page_rating'] - (df['joined_page_rating'] - df['joined_page_rating'].mean()) / df['num_ratings_ln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['age', 'num_pages', 'num_pages_ln', 'num_ratings', 'num_ratings_ln', 'num_reviews', 'my_rating', 'average_rating', '1 star', '2 stars', '3 stars', '4 stars', '5 stars', '1_star_percentage', '2_star_percentage', '3_star_percentage', '4_star_percentage', '5_star_percentage', 'a', 'b', 'c', 'num_adjusted_rating', 'coeff_2a_rating', 'coeff_b_rating', 'coeff_c_rating', 'joined_rating', 'final_rating', 'num_adjusted_page_rating', 'coeff_2a_page_rating', 'coeff_b_page_rating', 'coeff_c_page_rating', 'joined_page_rating', 'final_page_rating']\n",
    "corr_df= df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 15)) \n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Correlation Heatmap') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = df.sort_values(by='final_page_rating', ascending=False).reset_index().drop('index', axis=1)\n",
    "fresh = fresh[fresh['my_rating'].isna()]\n",
    "fresh[['Fiction' in genre_list for genre_list in fresh['genres']]] # Fiction, Nonfiction, Memoir, Classics, History, Politics, Philosophy, Business"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
