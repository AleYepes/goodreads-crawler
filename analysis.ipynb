{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prep data\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "books_df = pd.read_csv(\"data/books.csv\")\n",
    "star_cols = [c for c in books_df.columns if c.endswith('star')]\n",
    "books_df['rating_count'] = books_df[star_cols].sum(axis=1)\n",
    "\n",
    "# Todo: Fix the crawler to split langs with \"|\" by default\n",
    "books_df['lang'] = [\n",
    "    \"|\".join(item.strip() for item in x.split(\";\")) if isinstance(x, str) else x \n",
    "    for x in books_df['lang']\n",
    "]\n",
    "books_df['description'] = books_df['description'].str.replace('\\n\\n', '\\n')\n",
    "books_df['description'] = books_df['description'].str.replace('\\n', ' ')\n",
    "books_df['description'] = books_df['description'].str.replace('   ', ' ')\n",
    "books_df['description'] = books_df['description'].str.replace('  ', ' ')\n",
    "\n",
    "goodreads_df = pd.read_csv('data/goodreads_library_export.csv')\n",
    "goodreads_df['my_rating'] = goodreads_df['my_rating'].astype('UInt8')\n",
    "books_df['my_rating'] = books_df['my_rating'].replace(0, np.nan)\n",
    "\n",
    "books_df = books_df.merge(goodreads_df[['book_id', 'my_rating']], on='book_id', how='left')\n",
    "books_df = books_df[~books_df['similar_books'].isna()].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Prep embedding strings\n",
    "def format_string_for_embedding(items, kind=None, truncate=0):\n",
    "    if not isinstance(items, (list)) or len(items) == 0:\n",
    "        return \"\"\n",
    "\n",
    "    n = len(items)\n",
    "    if n == 1:\n",
    "        res = items[0]\n",
    "    elif n > truncate > 1:\n",
    "        res = f\"{', '.join(items[:truncate])}, and {items[truncate]}\"\n",
    "    else:\n",
    "        res = f\"{', '.join(items[:-1])}{',' if n > 2 else ''} and {items[-1]}\"\n",
    "    \n",
    "    prefix = f\"{kind.capitalize()}{'s' if n > 1 else ''}: \" if kind else \"\"\n",
    "    return f\"{prefix}{res}\"\n",
    "\n",
    "books_df['authors_post'] = books_df['authors'].str.split('|')\n",
    "books_df['authors_post'] = books_df['authors_post'].apply(lambda x:format_string_for_embedding(x, truncate=4))\n",
    "\n",
    "books_df['genres_post'] = books_df['genres'].str.split('|')\n",
    "books_df['genres_post'] = books_df['genres_post'].apply(lambda x:format_string_for_embedding(x, kind='genre'))\n",
    "\n",
    "books_df['desc_post'] = [[desc] if isinstance(desc, str) else [] for desc in books_df['description']]\n",
    "books_df['desc_post'] = books_df['desc_post'].apply(lambda x:format_string_for_embedding(x, kind='description'))\n",
    "\n",
    "def join_embedding_parts(title, authors, genres, desc):\n",
    "    text = f\"Book: {title}\\n\"\n",
    "    if authors:\n",
    "        text += f\"Written by: {authors}\\n\"\n",
    "    if genres:\n",
    "        text += f\"{genres}\\n\"\n",
    "    if desc:\n",
    "        text += f\"{desc}\" \n",
    "    return text\n",
    "\n",
    "books_df['embedding_input'] = [\n",
    "    join_embedding_parts(t, a, g, d) \n",
    "    for t, a, g, d in zip(books_df['title'], books_df['authors_post'], books_df['genres_post'], books_df['desc_post'])\n",
    "]\n",
    "\n",
    "id_to_string = books_df.set_index('book_id')['embedding_input']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Embed sentences\n",
    "import os\n",
    "import ollama\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "\n",
    "PARAMS = 0.6\n",
    "OLLAMA_MODEL = f\"qwen3-embedding:{PARAMS}b\"\n",
    "MIN_DIMENSIONS = 32\n",
    "\n",
    "my_rating_count = (~books_df['my_rating'].isna()).sum()\n",
    "\n",
    "embeddings_path = f'data/{PARAMS}b_embeddings.csv'\n",
    "if os.path.exists(embeddings_path):\n",
    "    embeddings = pd.read_csv(embeddings_path).set_index('book_id')\n",
    "else:\n",
    "    embeddings = pd.DataFrame()\n",
    "\n",
    "current_ids = books_df['book_id'].values\n",
    "missing_ids = [idx for idx in current_ids if idx not in embeddings.index]\n",
    "if missing_ids:\n",
    "    missing_strings = id_to_string.loc[missing_ids].tolist()\n",
    "    batch_size = 128\n",
    "    new_embeddings = []\n",
    "    for i in tqdm(range(0, len(missing_strings), batch_size)):\n",
    "        batch = missing_strings[i : i + batch_size]\n",
    "        response = ollama.embed(model=OLLAMA_MODEL, input=batch)\n",
    "        new_embeddings.extend(response['embeddings'])\n",
    "\n",
    "    new_embeddings = pd.DataFrame(new_embeddings, index=missing_ids)\n",
    "    new_embeddings.index.name = 'book_id'\n",
    "    embeddings = pd.concat([embeddings, new_embeddings])\n",
    "    embeddings.to_csv(embeddings_path)\n",
    "    del new_embeddings, missing_strings, missing_ids, current_ids\n",
    "\n",
    "embeddings = embeddings.loc[books_df['book_id']].values\n",
    "embeddings = torch.tensor(embeddings, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build adjacency matrix\n",
    "from torch_geometric.utils import add_self_loops\n",
    "from torch_geometric.nn.conv.gcn_conv import gcn_norm\n",
    "\n",
    "id_to_idx = {id: i for i, id in enumerate(books_df['book_id'])}\n",
    "\n",
    "edge_indices = []\n",
    "for idx, row in tqdm(books_df.iterrows(), total=len(books_df)):\n",
    "    current_idx = id_to_idx[row['book_id']]\n",
    "    if pd.isna(row['similar_books']):\n",
    "        continue\n",
    "    for item in row['similar_books'].split('|'):\n",
    "        try:\n",
    "            target_id = int(item.split(':')[0])\n",
    "            if target_id in books_df['book_id'].values:\n",
    "                target_idx = id_to_idx[target_id]\n",
    "                edge_indices.append([current_idx, target_idx])\n",
    "                edge_indices.append([target_idx, current_idx])\n",
    "        except (ValueError, IndexError):\n",
    "            continue\n",
    "\n",
    "if not edge_indices:\n",
    "    edge_index = torch.tensor([[], []], dtype=torch.long)\n",
    "else:\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "\n",
    "# GCN normalization to balance message passing\n",
    "edge_index_with_loops, _ = add_self_loops(edge_index, num_nodes=embeddings.size(0))\n",
    "edge_index_norm, edge_weight_norm = gcn_norm(edge_index_with_loops, num_nodes=embeddings.size(0))\n",
    "adj_matrix = torch.sparse_coo_tensor(edge_index_norm, edge_weight_norm, (embeddings.size(0), embeddings.size(0)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import Normalizer\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.cross_decomposition import PLSRegression\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from scipy.stats import spearmanr\n",
    "import nevergrad as ng\n",
    "\n",
    "\n",
    "def objective(num_propagations, \n",
    "              pls_n_components, \n",
    "              knn_neighbors, knn_leaf_size,\n",
    "              knn_weight, pls_weight,\n",
    "              ):\n",
    "    \n",
    "    all_embeddings = precomputed_embeddings[num_propagations]\n",
    "    my_ratings_embeddings = all_embeddings[my_ratings_mask]\n",
    "\n",
    "    loo = LeaveOneOut()\n",
    "    y_trues = []\n",
    "    y_preds = []\n",
    "    for train_idx, test_idx in loo.split(my_ratings_embeddings):\n",
    "        X_train, X_test = my_ratings_embeddings[train_idx], my_ratings_embeddings[test_idx]\n",
    "        y_train, y_test = my_ratings[train_idx], my_ratings[test_idx]\n",
    "\n",
    "        knn = KNeighborsRegressor(\n",
    "            n_neighbors=knn_neighbors,\n",
    "            leaf_size=knn_leaf_size,\n",
    "            metric='cosine', \n",
    "            weights='distance',\n",
    "            n_jobs=-1,\n",
    "            )\n",
    "        knn.fit(X_train, y_train)\n",
    "        knn_pred = knn.predict(X_test)\n",
    "\n",
    "        n_comps = min(pls_n_components, len(X_train) - 1)\n",
    "        pls = PLSRegression(\n",
    "            n_components=n_comps,\n",
    "            scale=False,\n",
    "            )\n",
    "        pls.fit(X_train, y_train)\n",
    "        pls_pred = pls.predict(X_test).flatten()\n",
    "\n",
    "        total_w = knn_weight + pls_weight + 1e-6\n",
    "        final_pred = ((knn_weight * knn_pred) + (pls_weight * pls_pred)) / total_w\n",
    "\n",
    "        y_trues.append(y_test[0])\n",
    "        y_preds.append(final_pred[0])\n",
    "\n",
    "    y_trues = np.array(y_trues)\n",
    "    y_preds = np.array(y_preds)\n",
    "    \n",
    "    mse = mean_squared_error(y_trues, y_preds)\n",
    "    ndcg = ndcg_score([y_trues], [y_preds])\n",
    "    if np.std(y_preds) < 1e-9:\n",
    "        spearman = 0.0\n",
    "    else:\n",
    "        spearman, _ = spearmanr(y_trues, y_preds)\n",
    "        if np.isnan(spearman): spearman = 0.0\n",
    "\n",
    "    return mse + (1.0 - spearman) + (1.0 - ndcg)\n",
    "\n",
    "\n",
    "MAX_PROPAGATIONS = 3\n",
    "propagated = embeddings.clone()\n",
    "norm_l2 = Normalizer(norm='l2')\n",
    "precomputed_embeddings = [norm_l2.transform(propagated.numpy())]\n",
    "\n",
    "for _ in range(MAX_PROPAGATIONS):\n",
    "    propagated = torch.sparse.mm(adj_matrix, propagated)\n",
    "    precomputed_embeddings.append(norm_l2.transform(propagated.numpy()))\n",
    "del propagated\n",
    "\n",
    "my_ratings_mask = ~books_df['my_rating'].isna()\n",
    "my_ratings = books_df.loc[my_ratings_mask, 'my_rating'].values\n",
    "\n",
    "\n",
    "train_size = len(my_ratings)\n",
    "parametrization = ng.p.Instrumentation(\n",
    "    num_propagations = ng.p.Scalar(lower=0, upper=MAX_PROPAGATIONS).set_integer_casting(),\n",
    "    pls_n_components = ng.p.Scalar(lower=1, upper=train_size).set_integer_casting(),\n",
    "    knn_neighbors = ng.p.Scalar(lower=3, upper=train_size//2).set_integer_casting(),\n",
    "    knn_leaf_size = ng.p.Scalar(lower=3, upper=50).set_integer_casting(),\n",
    "    knn_weight = ng.p.Scalar(lower=0, upper=1),\n",
    "    pls_weight = ng.p.Scalar(lower=0, upper=1),\n",
    ")\n",
    "\n",
    "BUDGET = 300\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=BUDGET)\n",
    "best_loss = float('inf')\n",
    "with tqdm(total=BUDGET) as pbar:\n",
    "    for i in range(BUDGET):\n",
    "        x = optimizer.ask()\n",
    "        loss = objective(*x.args, **x.kwargs)\n",
    "        optimizer.tell(x, loss)\n",
    "        if loss < best_loss:\n",
    "            best_loss = loss\n",
    "        pbar.update(1)\n",
    "        if i % 10 == 0:\n",
    "            pbar.set_description(f\"Best Loss: {best_loss:.4f}\")\n",
    "\n",
    "best_params = optimizer.provide_recommendation().kwargs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import BayesianRidge\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "\n",
    "train_mask = books_df['my_rating'].notna() & (books_df['my_rating'] > 0)\n",
    "X_train = X_reduced[train_mask]\n",
    "y_train = books_df.loc[train_mask, 'my_rating'].values\n",
    "\n",
    "# Bayesian Ridge Regression\n",
    "brr = BayesianRidge(compute_score=True)\n",
    "brr.fit(X_train, y_train)\n",
    "means, stds = brr.predict(X_reduced, return_std=True)\n",
    "books_df['brr_pred_rating'] = means\n",
    "books_df['brr_uncertainty'] = stds\n",
    "books_df['brr_score'] = books_df['brr_pred_rating'] - (books_df['brr_uncertainty'])\n",
    "\n",
    "nonzero_weights = np.sum(np.abs(brr.coef_) > 1e-5)\n",
    "print(f\"BRR used {nonzero_weights} of {X_reduced.shape[1]} dimensions.\")\n",
    "\n",
    "# Bayesian ARD\n",
    "ard = ARDRegression(compute_score=True)\n",
    "ard.fit(X_train, y_train)\n",
    "means, stds = ard.predict(X_reduced, return_std=True)\n",
    "books_df['brr_pred_rating'] = means\n",
    "books_df['brr_uncertainty'] = stds\n",
    "books_df['brr_score'] = books_df['brr_pred_rating'] - (books_df['brr_uncertainty'])\n",
    "\n",
    "nonzero_weights = np.sum(np.abs(ard.coef_) > 1e-5)\n",
    "print(f\"ARD used {nonzero_weights} of {X_reduced.shape[1]} dimensions.\")\n",
    "\n",
    "# Random Forest\n",
    "rf = RandomForestRegressor(n_estimators=100, min_samples_leaf=2, random_state=42, n_jobs=-1)\n",
    "rf.fit(X_train, y_train)\n",
    "means = rf.predict(X_reduced)\n",
    "per_tree_preds = np.stack([tree.predict(X_reduced) for tree in rf.estimators_])\n",
    "stds = np.std(per_tree_preds, axis=0)\n",
    "books_df['rf_pred_rating'] = means\n",
    "books_df['rf_uncertainty'] = stds\n",
    "books_df['rf_score'] = books_df['rf_pred_rating'] - (books_df['rf_uncertainty'])\n",
    "\n",
    "# Weighted KNN Regressor\n",
    "knn = KNeighborsRegressor(n_neighbors=15, weights='distance', metric='cosine')\n",
    "knn.fit(X_train, y_train)\n",
    "means = knn.predict(X_reduced)\n",
    "neighbor_indices = knn.kneighbors(X_reduced, return_distance=False)\n",
    "neighbor_ratings = y_train[neighbor_indices] \n",
    "stds = np.std(neighbor_ratings, axis=1)\n",
    "books_df['knn_pred_rating'] = means\n",
    "books_df['knn_uncertainty'] = stds\n",
    "books_df['knn_score'] = books_df['knn_pred_rating'] - (books_df['knn_uncertainty'])\n",
    "\n",
    "books_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Global score\n",
    "C = books_df['avg_rating'].mean()\n",
    "m = books_df['rating_count'].quantile(0.10) \n",
    "def weighted_rating(x, m=m, C=C):\n",
    "    v = float(x['rating_count'])\n",
    "    R = float(x['avg_rating'])\n",
    "    if v == 0: \n",
    "        return C\n",
    "    return (v / (v + m) * R) + (m / (v + m) * C)\n",
    "books_df['global_score'] = books_df.apply(weighted_rating, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nevergrad as ng\n",
    "import numpy as np\n",
    "import umap\n",
    "import torch\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.linear_model import ARDRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler, Normalizer\n",
    "from sklearn.metrics import ndcg_score, mean_squared_error\n",
    "from scipy.stats import rankdata\n",
    "from tqdm import tqdm\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "\n",
    "def objective(\n",
    "    # GCN feature blending (now with 4 levels)\n",
    "    gcn_propagations\n",
    "    \n",
    "    # UMAP hyperparameters\n",
    "    umap_neighbors,\n",
    "    umap_min_dist,\n",
    "    umap_components,\n",
    "    umap_learning_rate,\n",
    "    umap_negative_sample_rate,\n",
    "    \n",
    "    # Model hyperparameters\n",
    "    knn_k,\n",
    "    knn_weights,\n",
    "    \n",
    "    ard_alpha_1,\n",
    "    ard_alpha_2,\n",
    "    ard_lambda_1,\n",
    "    ard_lambda_2,\n",
    "    ard_threshold_lambda,\n",
    "    \n",
    "    # Ensemble strategy\n",
    "    ensemble_method,\n",
    "    uncertainty_transform,\n",
    "    \n",
    "    # Ensemble weights\n",
    "    w_ard, w_knn,\n",
    "    \n",
    "    # Uncertainty penalties\n",
    "    u_ard, u_knn,\n",
    "):\n",
    "    \n",
    "    # Cast to appropriate types\n",
    "    umap_neighbors = max(5, int(umap_neighbors))\n",
    "    umap_components = max(2, int(umap_components))\n",
    "    knn_k = max(3, int(knn_k))\n",
    "    \n",
    "    kf = KFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    fold_scores = []\n",
    "    for _, (train_idx, val_idx) in enumerate(kf.split(X_raw_emb_train)):\n",
    "        try:\n",
    "            # 1. Blend GCN features\n",
    "            for _ in range(gcn_propagations):\n",
    "                final_embeddings = torch.sparse.mm(adj_matrix, final_embeddings)\n",
    "            final_embeddings = final_embeddings.numpy()\n",
    "            \n",
    "            X_emb_tr, X_emb_val = final_embeddings[train_idx], final_embeddings[val_idx]\n",
    "            y_tr, y_val = y_train_base[train_idx], y_train_base[val_idx]\n",
    "            \n",
    "            max_components = min(umap_components, len(y_tr) - 2)\n",
    "\n",
    "            # 2. Normalize and scale embeddings\n",
    "            l2_norm = Normalizer(norm='l2')\n",
    "            X_emb_tr = l2_norm.transform(X_emb_tr)\n",
    "            X_emb_val = l2_norm.transform(X_emb_val)\n",
    "            \n",
    "            # 3. UMAP\n",
    "            reducer = umap.UMAP(\n",
    "                n_neighbors=min(umap_neighbors, len(y_tr) - 1),\n",
    "                min_dist=umap_min_dist,\n",
    "                n_components=max_components,\n",
    "                metric='cosine',\n",
    "                learning_rate=umap_learning_rate,\n",
    "                negative_sample_rate=int(umap_negative_sample_rate),\n",
    "                random_state=42,\n",
    "                n_jobs=1,\n",
    "                verbose=False\n",
    "            )\n",
    "            X_tr_final = reducer.fit_transform(X_emb_tr)\n",
    "            X_val_final = reducer.transform(X_emb_val)\n",
    "            \n",
    "            # 4. Train models\n",
    "            # ARD with hyperparameters\n",
    "            ard = ARDRegression(\n",
    "                alpha_1=ard_alpha_1,\n",
    "                alpha_2=ard_alpha_2,\n",
    "                lambda_1=ard_lambda_1,\n",
    "                lambda_2=ard_lambda_2,\n",
    "                threshold_lambda=ard_threshold_lambda,\n",
    "                compute_score=True\n",
    "            )\n",
    "            ard.fit(X_tr_final, y_tr)\n",
    "            mu_ard, std_ard = ard.predict(X_val_final, return_std=True)\n",
    "            \n",
    "            # KNN with extended options\n",
    "            knn = KNeighborsRegressor(\n",
    "                n_neighbors=min(knn_k, len(y_tr) - 1), \n",
    "                weights=knn_weights,\n",
    "                metric='cosine',\n",
    "            )\n",
    "            knn.fit(X_tr_final, y_tr)\n",
    "            mu_knn = knn.predict(X_val_final)\n",
    "            neigh_idx = knn.kneighbors(X_val_final, return_distance=False)\n",
    "            std_knn = np.std(y_tr[neigh_idx], axis=1)\n",
    "            \n",
    "            # 5. Transform uncertainties\n",
    "            if uncertainty_transform == 'sqrt':\n",
    "                std_ard = np.sqrt(std_ard)\n",
    "                std_knn = np.sqrt(std_knn)\n",
    "            elif uncertainty_transform == 'square':\n",
    "                std_ard = std_ard ** 2\n",
    "                std_knn = std_knn ** 2\n",
    "            \n",
    "            # 6. Ensemble predictions\n",
    "            if ensemble_method == 'weighted':\n",
    "                final_pred = (w_ard * (mu_ard - u_ard * std_ard) + \n",
    "              w_knn * (mu_knn - u_knn * std_knn)) / (w_ard + w_knn + 1e-6)\n",
    "                # final_pred = np.sum(preds, axis=0)\n",
    "            \n",
    "            elif ensemble_method == 'rank':\n",
    "                # # Rank-based combination\n",
    "                # ranks = [\n",
    "                #     w_ard * rankdata(mu_ard - u_ard * std_ard),\n",
    "                #     w_knn * rankdata(mu_knn - u_knn * std_knn),\n",
    "                # ]\n",
    "                # final_pred = np.sum(ranks, axis=0)\n",
    "\n",
    "                # Rank-based combination\n",
    "                r_ard = rankdata(mu_ard - u_ard * std_ard)\n",
    "                r_knn = rankdata(mu_knn - u_knn * std_knn)\n",
    "                \n",
    "                # Normalize ranks to 0-1 range to be comparable to y_val for MSE check\n",
    "                r_ard = r_ard / len(r_ard)\n",
    "                r_knn = r_knn / len(r_knn)\n",
    "                \n",
    "                final_pred = (w_ard * r_ard + w_knn * r_knn) / (w_ard + w_knn + 1e-6)\n",
    "            \n",
    "            else:  # 'average'\n",
    "                final_pred = (mu_ard + mu_knn) / 2\n",
    "            \n",
    "            # 7. Multi-objective scoring: NDCG + MSE\n",
    "            ndcg = ndcg_score([y_val], [final_pred], k=min(20, len(y_val)))\n",
    "            mse = mean_squared_error(y_val, final_pred)\n",
    "            \n",
    "            # Combined score (NDCG primary, MSE secondary)\n",
    "            score = ndcg - 0.1 * np.sqrt(mse)\n",
    "            fold_scores.append(score)\n",
    "            \n",
    "        except Exception as e:\n",
    "            return 1e6\n",
    "    \n",
    "    if len(fold_scores) == 0:\n",
    "        return 1e6\n",
    "    \n",
    "    return -np.mean(fold_scores)\n",
    "\n",
    "\n",
    "# DATA PREPARATION\n",
    "if isinstance(embeddings, torch.Tensor):\n",
    "    X_raw_emb = embeddings.detach().cpu().numpy()\n",
    "else:\n",
    "    X_raw_emb = np.array(embeddings)\n",
    "\n",
    "# Extract training data\n",
    "y_full = books_df['my_rating'].values.astype(float)\n",
    "valid_mask = (~np.isnan(y_full)) & (y_full > 0)\n",
    "y_train_base = y_full[valid_mask]\n",
    "scaler = MinMaxScaler()\n",
    "y_train_base = scaler.fit_transform(y_train_base.reshape(-1, 1)).ravel()\n",
    "n_train_samples = len(y_train_base)\n",
    "\n",
    "max_umap_components = min(int((n_train_samples * 2 / 3)), sent_embedding_dimensions)\n",
    "parametrization = ng.p.Instrumentation(\n",
    "    # GCN blending (4 depths now)\n",
    "    gcn_weight_0=ng.p.Scalar(lower=0, upper=10),\n",
    "    gcn_weight_1=ng.p.Scalar(lower=0, upper=10),\n",
    "    gcn_weight_2=ng.p.Scalar(lower=0, upper=10),\n",
    "    gcn_weight_3=ng.p.Scalar(lower=0, upper=10),\n",
    "    \n",
    "    # UMAP with more hyperparameters\n",
    "    umap_neighbors=ng.p.Scalar(lower=5, upper=50).set_integer_casting(),\n",
    "    umap_min_dist=ng.p.Scalar(lower=0.0, upper=0.99),\n",
    "    umap_components=ng.p.Scalar(lower=3, upper=max_umap_components).set_integer_casting(),\n",
    "    umap_learning_rate=ng.p.Scalar(lower=0.1, upper=2.0),\n",
    "    umap_negative_sample_rate=ng.p.Scalar(lower=3, upper=20).set_integer_casting(),\n",
    "    \n",
    "    # KNN hyperparameters\n",
    "    knn_k=ng.p.Scalar(lower=3, upper=30).set_integer_casting(),\n",
    "    knn_weights=ng.p.Choice(['uniform', 'distance']),\n",
    "    \n",
    "    # ARD hyperparameters\n",
    "    ard_alpha_1=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_alpha_2=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_lambda_1=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_lambda_2=ng.p.Scalar(lower=1e-7, upper=1e-3),\n",
    "    ard_threshold_lambda=ng.p.Scalar(lower=1e3, upper=1e6),\n",
    "    \n",
    "    # Ensemble strategy\n",
    "    ensemble_method=ng.p.Choice(['weighted', 'rank', 'average']),\n",
    "    uncertainty_transform=ng.p.Choice(['none', 'sqrt']),\n",
    "    \n",
    "    # Ensemble weights (6 models now)\n",
    "    w_ard=ng.p.Scalar(lower=0, upper=2),\n",
    "    w_knn=ng.p.Scalar(lower=0, upper=2),\n",
    "    \n",
    "    # Uncertainty penalties\n",
    "    u_ard=ng.p.Scalar(lower=0, upper=5),\n",
    "    u_knn=ng.p.Scalar(lower=0, upper=5),\n",
    ")\n",
    "\n",
    "BUDGET = 300\n",
    "optimizer = ng.optimizers.NGOpt(parametrization=parametrization, budget=BUDGET)\n",
    "with tqdm(total=BUDGET) as pbar:\n",
    "    for i in range(BUDGET):\n",
    "        x = optimizer.ask()\n",
    "        loss = objective(*x.args, **x.kwargs)\n",
    "        optimizer.tell(x, loss)\n",
    "        pbar.update(1)\n",
    "        if i % 20 == 0:\n",
    "            pbar.set_description(f\"Best Score: {-optimizer.current_bests['minimum'].mean:.4f}\")\n",
    "\n",
    "best_params = optimizer.provide_recommendation().kwargs\n",
    "param_groups = {\n",
    "    'GCN Blending': ['gcn_weight_0', 'gcn_weight_1', 'gcn_weight_2', 'gcn_weight_3'],\n",
    "    'UMAP': [k for k in best_params.keys() if k.startswith('umap_')],\n",
    "    'KNN': [k for k in best_params.keys() if k.startswith('knn_')],\n",
    "    'ARD': [k for k in best_params.keys() if k.startswith('ard_')],\n",
    "    'Ensemble': ['ensemble_method', 'uncertainty_transform'] + [k for k in best_params.keys() if k.startswith('w_') or k.startswith('u_')]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import networkx as nx\n",
    "\n",
    "G = nx.Graph()\n",
    "G.add_nodes_from(books_df['book_id'].tolist())\n",
    "\n",
    "# 2. Build edges with correct type casting\n",
    "edge_list = []\n",
    "for idx, row in books_df.iterrows():\n",
    "    if pd.isna(row['similar_books']): continue\n",
    "    \n",
    "    for item in row['similar_books'].split('|'):\n",
    "        target_id_str = item.split(':')[0]\n",
    "        target_id = int(target_id_str)\n",
    "        \n",
    "        if target_id in books_df['book_id'].values:\n",
    "            edge_list.append((row['book_id'], target_id))\n",
    "\n",
    "G.add_edges_from(edge_list)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# old"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_export = pd.read_csv('data/goodreads_library_export.csv')\n",
    "# goodreads_export = pd.read_csv('data/20-01-2025_goodreads_library_export.csv')\n",
    "# goodreads_export = goodreads_export[['Book Id', 'Author', 'My Rating', 'Number of Pages', 'Original Publication Year']]\n",
    "# goodreads_export = goodreads_export.rename(columns={'Book Id':'book_id',\n",
    "#                                                     'Author': 'author',\n",
    "#                                                     'My Rating': 'my_rating',\n",
    "#                                                     'Number of Pages': 'num_pages',\n",
    "#                                                     'Original Publication Year': 'year'})\n",
    "threshold = (goodreads_export['number_of_pages'].mean() - goodreads_export['number_of_pages'].std())\n",
    "goodreads_export.loc[goodreads_export['number_of_pages'] < threshold, 'number_of_pages'] = np.nan\n",
    "\n",
    "book_df = pd.read_csv('data/books.csv')\n",
    "# book_df = pd.read_csv('data/01-2025_goodreads_scraped.csv')\n",
    "df = goodreads_export.merge(book_df, on='book_id')\n",
    "\n",
    "# Drop competing columns\n",
    "df['author'] = df['author_x'].fillna(df['author_y'])\n",
    "df['num_pages'] = df['num_pages_x'].fillna(df['num_pages_y'])\n",
    "df['year'] = df['year_x'].fillna(df['year_y'])\n",
    "df.drop(columns=['author_x', 'author_y', 'num_pages_x', 'num_pages_y', 'year_x', 'year_y'], inplace=True)\n",
    "\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df['year'] = df['year'].fillna(df['year'].mean()).round().astype(int)\n",
    "df['num_pages'] = df['num_pages'].fillna(df['num_pages'].mean()).round().astype(int)\n",
    "df['num_reviews'] = df['num_reviews'].fillna(0).round().astype(int)\n",
    "df['my_rating'] = df['my_rating'].replace(0,np.nan)\n",
    "\n",
    "df['age'] = int(datetime.now().strftime('%Y')) - df['year']\n",
    "df['average_rating'] = ((df['5 stars'] * 5) + (df['4 stars'] * 4) + (df['3 stars'] * 3) + (df['2 stars'] * 2) + df['1 star']) / df['num_ratings']\n",
    "df = df[['book_id', 'title', 'author', 'year', 'age', 'series', 'num_pages', 'genres', 'num_ratings', 'num_reviews', 'my_rating', 'average_rating', '5 stars', '4 stars', '3 stars', '2 stars', '1 star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quadratic(row):\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    a, b, c = np.polyfit(x, row, 2)\n",
    "    return pd.Series([a, b, c])\n",
    "\n",
    "# Calculating quadrdic modeling coefficients\n",
    "df['1_star_percentage'] = df['1 star'] / df['num_ratings']\n",
    "df['2_star_percentage'] = df['2 stars'] / df['num_ratings']\n",
    "df['3_star_percentage'] = df['3 stars'] / df['num_ratings']\n",
    "df['4_star_percentage'] = df['4 stars'] / df['num_ratings']\n",
    "df['5_star_percentage'] = df['5 stars'] / df['num_ratings']\n",
    "coefficients = df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "df['a'], df['b'], df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "\n",
    "# Pre-processing columns for rankings\n",
    "df['num_ratings_ln'] = np.log1p(df['num_ratings'])\n",
    "df['num_pages_ln'] = np.log1p(df['num_pages'])\n",
    "df['2a_shifted'] = df['a'] - df['a'].min()\n",
    "df['2a_shifted'] = df['2a_shifted'] * (1 / df['2a_shifted'].max()) + 1\n",
    "df['b_shifted'] = df['b'] - df['b'].min()\n",
    "df['b_shifted'] = df['b_shifted'] * (1 / df['b_shifted'].max()) + 1\n",
    "df['c_shifted'] = df['c'] - df['c'].min()\n",
    "df['c_shifted'] = df['c_shifted'] * (1 / df['c_shifted'].max()) + 1\n",
    "\n",
    "# Types of rankings\n",
    "df['num_adjusted_rating'] = df['average_rating'] - (df['average_rating'] - df['average_rating'].mean()) / df['num_ratings_ln']\n",
    "df['coeff_2a_rating'] = (df['num_adjusted_rating'] * df['2a_shifted'])\n",
    "df['coeff_b_rating'] = (df['num_adjusted_rating']) / (df['b_shifted'])\n",
    "df['coeff_c_rating'] = (df['num_adjusted_rating'] * df['c_shifted'])\n",
    "df['joined_rating'] = (df['num_adjusted_rating'] * df['c_shifted'] * df['2a_shifted']) / df['b_shifted']\n",
    "df['final_rating'] = df['joined_rating'] - (df['joined_rating'] - df['joined_rating'].mean()) / df['num_ratings_ln']\n",
    "\n",
    "df['num_adjusted_page_rating'] = df['num_adjusted_rating'] / (df['num_pages_ln'])\n",
    "df['coeff_2a_page_rating'] = df['coeff_2a_rating'] / df['num_pages_ln']\n",
    "df['coeff_b_page_rating'] = df['coeff_b_rating'] / df['num_pages_ln']\n",
    "df['coeff_c_page_rating'] = df['coeff_c_rating'] / df['num_pages_ln']\n",
    "df['joined_page_rating'] = df['joined_rating'] / df['num_pages_ln']\n",
    "df['final_page_rating'] = df['joined_page_rating'] - (df['joined_page_rating'] - df['joined_page_rating'].mean()) / df['num_ratings_ln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['age', 'num_pages', 'num_pages_ln', 'num_ratings', 'num_ratings_ln', 'num_reviews', 'my_rating', 'average_rating', '1 star', '2 stars', '3 stars', '4 stars', '5 stars', '1_star_percentage', '2_star_percentage', '3_star_percentage', '4_star_percentage', '5_star_percentage', 'a', 'b', 'c', 'num_adjusted_rating', 'coeff_2a_rating', 'coeff_b_rating', 'coeff_c_rating', 'joined_rating', 'final_rating', 'num_adjusted_page_rating', 'coeff_2a_page_rating', 'coeff_b_page_rating', 'coeff_c_page_rating', 'joined_page_rating', 'final_page_rating']\n",
    "corr_df= df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 15)) \n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Correlation Heatmap') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = df.sort_values(by='final_page_rating', ascending=False).reset_index().drop('index', axis=1)\n",
    "fresh = fresh[fresh['my_rating'].isna()]\n",
    "fresh[['Fiction' in genre_list for genre_list in fresh['genres']]] # Fiction, Nonfiction, Memoir, Classics, History, Politics, Philosophy, Business"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv (3.12.3)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
