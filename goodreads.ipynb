{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "from selenium import webdriver\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "import time\n",
    "from datetime import datetime\n",
    "import os\n",
    "from requests import get\n",
    "import json\n",
    "import bs4\n",
    "import glob\n",
    "import ast\n",
    "\n",
    "pd.set_option('display.max_rows', None)\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.max_colwidth', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mybooks(new_export_path):\n",
    "    # Remove previous \n",
    "    goodreads_export = f'data/goodreads_library_export.csv'\n",
    "    if os.path.isfile(goodreads_export):\n",
    "        os.remove(goodreads_export)\n",
    "\n",
    "    directory = '/Users/alex/Documents/testing/goodreads-ranker/data'\n",
    "    prefs = {'download.default_directory' : directory}\n",
    "    chrome_options = webdriver.ChromeOptions()\n",
    "    chrome_options.add_experimental_option('prefs', prefs)\n",
    "    driver = webdriver.Chrome(options=chrome_options)\n",
    "\n",
    "    login_url = 'https://www.goodreads.com/ap/signin?language=en_US&openid.assoc_handle=amzn_goodreads_web_na&openid.claimed_id=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.identity=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0%2Fidentifier_select&openid.mode=checkid_setup&openid.ns=http%3A%2F%2Fspecs.openid.net%2Fauth%2F2.0&openid.pape.max_auth_age=0&openid.return_to=https%3A%2F%2Fwww.goodreads.com%2Fap-handler%2Fsign-in'\n",
    "    driver.get(login_url)\n",
    "    WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CLASS_NAME, \"homePrimaryColumn\")))\n",
    "\n",
    "    driver.get('https://www.goodreads.com/review/import')\n",
    "    time.sleep(2)\n",
    "\n",
    "    current_date = datetime.now().strftime('%m/%d/%Y')\n",
    "    file_list = driver.find_element(By.CLASS_NAME, 'fileList')\n",
    "    if current_date not in file_list.text:\n",
    "        export_button = driver.find_element(By.CLASS_NAME, 'js-LibraryExport')\n",
    "        export_button.click()\n",
    "    WebDriverWait(driver, 60).until(EC.presence_of_element_located((By.CLASS_NAME, \"fileList\"))) # Maybe useless\n",
    "\n",
    "    while True:\n",
    "        file_list = driver.find_element(By.CLASS_NAME, 'fileList')\n",
    "        if current_date in file_list.text:\n",
    "            # find the link and click it\n",
    "            link = file_list.find_element(By.TAG_NAME, 'a')\n",
    "            link.click()\n",
    "            time.sleep(3)\n",
    "            break\n",
    "        else:\n",
    "            time.sleep(3)\n",
    "\n",
    "    os.rename('data/goodreads_library_export.csv', new_export_path)\n",
    "    goodreads_export = pd.read_csv(new_export_path)\n",
    "    driver.quit()\n",
    "    return goodreads_export"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def check_mybooks(new_export_path, this_months_scrape_path):\n",
    "    books_already_scraped = [int(file_name.replace(f'_metadata.json', '')) for file_name in os.listdir('metadata')]\n",
    "    try:\n",
    "        goodreads_export = pd.read_csv(new_export_path)\n",
    "    except FileNotFoundError:\n",
    "        goodreads_export = get_mybooks(new_export_path)\n",
    "    try:\n",
    "        recent_df = pd.read_csv(this_months_scrape_path)\n",
    "        recent_book_ids = recent_df['book_id'].tolist()\n",
    "        books_already_scraped = set(recent_book_ids + books_already_scraped)\n",
    "        print(books_already_scraped)\n",
    "    except FileNotFoundError:\n",
    "        pass\n",
    "\n",
    "    book_ids = goodreads_export['Book Id'].tolist()\n",
    "    books_to_scrape = [id for id in book_ids if id not in books_already_scraped]\n",
    "\n",
    "    return book_ids, books_to_scrape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = set()\n",
    "test.remove('a')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('data/books.csv')\n",
    "# df = df[~df['similar_books'].isna()]\n",
    "# df['year'] = df['year'].astype(int)\n",
    "# df = df.drop(columns='Unnamed: 0')\n",
    "# df = df.drop_duplicates(subset=['id'])\n",
    "df#.to_csv('data/goodreads_books_data.csv', index=False)\n",
    "\n",
    "# df = pd.read_csv('data/goodreads_library_export.csv')\n",
    "# df.columns = [col.lower().replace(' ','_') for col in df.columns]\n",
    "# df#.to_csv('data/goodreads_library_export.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import asyncio\n",
    "from playwright.async_api import async_playwright\n",
    "from bs4 import BeautifulSoup\n",
    "import json\n",
    "import nest_asyncio\n",
    "\n",
    "# Apply nest_asyncio to allow asyncio to run in Jupyter/IPython\n",
    "# (This is often necessary to run Playwright's async functions in a notebook cell)\n",
    "nest_asyncio.apply()\n",
    "\n",
    "# --- Your original function goes here ---\n",
    "\n",
    "def extract_dom_data(page, book_data):\n",
    "    # Note: If running this in a synchronous environment like a script,\n",
    "    # you would need to await page.content() but Playwright's async_api \n",
    "    # handles this context-appropriately here.\n",
    "    html_content = page.content() # This assumes 'page' is a synchronous wrapper or is awaited elsewhere\n",
    "    \n",
    "    # In the actual async run below, we will pass the awaited content()\n",
    "    # Let's adjust the function signature slightly to accept content instead of the page object\n",
    "    # for cleaner testing in an async environment, or keep it as is if your \n",
    "    # original environment is a Playwright/Scrapy/etc. synchronous runner.\n",
    "    \n",
    "    # *** I will assume your original environment makes page.content() synchronous\n",
    "    # For a *direct* Jupyter test using Playwright, it's safer to pass the HTML string.\n",
    "    # I will modify the function call below, but keep your function as is for clarity.\n",
    "    \n",
    "    # --- Start of your function logic ---\n",
    "    html_content = page \n",
    "    soup = BeautifulSoup(html_content, \"html.parser\")\n",
    "\n",
    "    stars = {}\n",
    "    for i in range(1, 6):\n",
    "        label = soup.find(attrs={\"data-testid\": f\"labelTotal-{i}\"})\n",
    "        if label:\n",
    "            text = label.get_text().strip().split()[0]\n",
    "            text = text.replace(\",\", \"\")\n",
    "            stars[f\"{i}_star\"] = int(text) if text.isdigit() else 0\n",
    "        else:\n",
    "            stars[f\"{i}_star\"] = 0\n",
    "    \n",
    "    book_data.update(stars)\n",
    "\n",
    "    # --- Genres ---\n",
    "    genre_nodes = soup.select(\".BookPageMetadataSection__genreButton .Button__labelItem\")\n",
    "    genres = [node.get_text() for node in genre_nodes if node.get_text() != \"...more\"]\n",
    "    book_data['genres'] = \"|\".join(genres)\n",
    "\n",
    "    # --- Series ---\n",
    "    series_el = soup.select_one(\"h3.Text__italic a\")\n",
    "    if series_el and series_el.get('href'):\n",
    "        book_data['series'] = series_el['href'].split('/')[-1]\n",
    "    else:\n",
    "        book_data['series'] = \"\"\n",
    "\n",
    "    # --- Publication Year ---\n",
    "    pub_el = soup.find(attrs={\"data-testid\": \"publicationInfo\"})\n",
    "    if pub_el:\n",
    "        parts = pub_el.get_text().split(\", \")\n",
    "        book_data['year'] = parts[-1].strip() if parts else \"\"\n",
    "    else:\n",
    "        book_data['year'] = \"\"\n",
    "\n",
    "    # --- Description ---\n",
    "    desc_el = soup.select_one(\"[data-testid='description'] span.Formatted\")\n",
    "    \n",
    "    if not desc_el:\n",
    "        desc_el = soup.select_one(\".DetailsLayoutRightParagraph__widthConstrained span.Formatted\")\n",
    "\n",
    "    if desc_el:\n",
    "        text = desc_el.get_text(separator=\"\\n\", strip=True)\n",
    "        book_data['description'] = text\n",
    "    else:\n",
    "        book_data['description'] = \"\"\n",
    "\n",
    "    return book_data\n",
    "\n",
    "# --- Asynchronous function to run Playwright ---\n",
    "\n",
    "async def run_scraper():\n",
    "    url = \"https://www.goodreads.com/book/show/122449053-biblioteca-de-manualidades-cortinas-y-estores?from_search=true&from_srp=mn95XrkXj6&qid=3\"\n",
    "    \n",
    "    print(\"üöÄ Launching Playwright browser...\")\n",
    "    async with async_playwright() as p:\n",
    "        # Use chromium and launch in headless mode for speed\n",
    "        browser = await p.chromium.launch(headless=True)\n",
    "        page = await browser.new_page()\n",
    "        \n",
    "        # Set a user-agent to mimic a real browser\n",
    "        await page.set_extra_http_headers({\"User-Agent\": \"Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.124 Safari/537.36\"})\n",
    "\n",
    "        print(f\"üåç Navigating to: {url}\")\n",
    "        # Wait until the network is idle, meaning all content has likely loaded\n",
    "        await page.goto(url, wait_until=\"networkidle\") \n",
    "        \n",
    "        # NOTE: Goodreads often uses client-side rendering (JavaScript) for content.\n",
    "        # Playwright handles this automatically by waiting for the page to load.\n",
    "\n",
    "        # Pass the HTML content (string) to your scraping function\n",
    "        html_content = await page.content()\n",
    "        \n",
    "        await browser.close()\n",
    "        print(\"‚úÖ Browser closed.\")\n",
    "\n",
    "        book_data = {}\n",
    "        # Call the modified function: pass html_content directly as the 'page' argument\n",
    "        # since page.content() is the key part that BeautifulSoup needs.\n",
    "        extracted_data = extract_dom_data(html_content, book_data)\n",
    "        \n",
    "        # Print the result nicely\n",
    "        print(\"\\n‚ú® Extracted Data:\")\n",
    "        print(json.dumps(extracted_data, indent=4, ensure_ascii=False))\n",
    "\n",
    "# Run the async function\n",
    "asyncio.run(run_scraper())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# main\n",
    "this_month = datetime.now().strftime('%m-%Y')\n",
    "this_day = datetime.now().strftime('%d-%m-%Y')\n",
    "new_export_path = f'data/{this_day}_goodreads_library_export.csv'\n",
    "this_months_scrape_path = f'data/{this_month}_goodreads_scraped.csv'\n",
    "\n",
    "while True:\n",
    "    book_ids, books_to_scrape = check_mybooks(new_export_path, this_months_scrape_path)\n",
    "    if books_to_scrape:\n",
    "        for i, book_id in enumerate(books_to_scrape):\n",
    "            try:\n",
    "                print(f'\\nScraping book-id:{book_id} ({i+1}/{len(books_to_scrape)})')\n",
    "                start = datetime.now()\n",
    "                book = scrape_book(book_id)\n",
    "                if book:\n",
    "                    json.dump(book, open(f'metadata/{book_id}_metadata.json', 'w'))\n",
    "                else:\n",
    "                    print(f'    scrape_book() returned empty')\n",
    "                print(f'{datetime.now() - start}')\n",
    "            except Exception as e:\n",
    "                print(e)\n",
    "\n",
    "        books = condense_books('metadata')\n",
    "        book_df = pd.DataFrame(books)\n",
    "        \n",
    "        if os.path.isfile(this_months_scrape_path):\n",
    "            old_df = pd.read_csv(this_months_scrape_path)\n",
    "            book_df = pd.concat([old_df, book_df])\n",
    "            book_df = book_df[book_df['book_id'].isin(book_ids)]\n",
    "            book_df = book_df.drop_duplicates(subset=[col for col in book_df.columns if col != 'genres'])\n",
    "\n",
    "        book_df.to_csv(this_months_scrape_path, index=False, encoding='utf-8')\n",
    "    else:\n",
    "        # delete_metadata()\n",
    "        print('ALL BOOKS HAVE BEEN SCRAPED')\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "goodreads_export = pd.read_csv(new_export_path)\n",
    "# goodreads_export = pd.read_csv('data/20-01-2025_goodreads_library_export.csv')\n",
    "goodreads_export['Original Publication Year'] = goodreads_export['Original Publication Year'].fillna(goodreads_export['Year Published'])\n",
    "goodreads_export = goodreads_export[['Book Id', 'Author', 'My Rating', 'Number of Pages', 'Original Publication Year']]\n",
    "goodreads_export = goodreads_export.rename(columns={'Book Id':'book_id',\n",
    "                                                    'Author': 'author',\n",
    "                                                    'My Rating': 'my_rating',\n",
    "                                                    'Number of Pages': 'num_pages',\n",
    "                                                    'Original Publication Year': 'year'})\n",
    "threshold = (goodreads_export['num_pages'].mean() - goodreads_export['num_pages'].std())\n",
    "goodreads_export.loc[goodreads_export['num_pages'] < threshold, 'num_pages'] = np.nan\n",
    "\n",
    "book_df = pd.read_csv(this_months_scrape_path)\n",
    "# book_df = pd.read_csv('data/01-2025_goodreads_scraped.csv')\n",
    "df = goodreads_export.merge(book_df, on='book_id')\n",
    "\n",
    "# Drop competing columns\n",
    "df['author'] = df['author_x'].fillna(df['author_y'])\n",
    "df['num_pages'] = df['num_pages_x'].fillna(df['num_pages_y'])\n",
    "df['year'] = df['year_x'].fillna(df['year_y'])\n",
    "df.drop(columns=['author_x', 'author_y', 'num_pages_x', 'num_pages_y', 'year_x', 'year_y'], inplace=True)\n",
    "\n",
    "df['genres'] = df['genres'].apply(lambda x: ast.literal_eval(x) if pd.notna(x) else [])\n",
    "df['year'] = df['year'].fillna(df['year'].mean()).round().astype(int)\n",
    "df['num_pages'] = df['num_pages'].fillna(df['num_pages'].mean()).round().astype(int)\n",
    "df['num_reviews'] = df['num_reviews'].fillna(0).round().astype(int)\n",
    "df['my_rating'] = df['my_rating'].replace(0,np.nan)\n",
    "\n",
    "df['age'] = int(datetime.now().strftime('%Y')) - df['year']\n",
    "df['average_rating'] = ((df['5 stars'] * 5) + (df['4 stars'] * 4) + (df['3 stars'] * 3) + (df['2 stars'] * 2) + df['1 star']) / df['num_ratings']\n",
    "df = df[['book_id', 'title', 'author', 'year', 'age', 'series', 'num_pages', 'genres', 'num_ratings', 'num_reviews', 'my_rating', 'average_rating', '5 stars', '4 stars', '3 stars', '2 stars', '1 star']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quadratic(row):\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    a, b, c = np.polyfit(x, row, 2)\n",
    "    return pd.Series([a, b, c])\n",
    "\n",
    "# Calculating quadrdic modeling coefficients\n",
    "df['1_star_percentage'] = df['1 star'] / df['num_ratings']\n",
    "df['2_star_percentage'] = df['2 stars'] / df['num_ratings']\n",
    "df['3_star_percentage'] = df['3 stars'] / df['num_ratings']\n",
    "df['4_star_percentage'] = df['4 stars'] / df['num_ratings']\n",
    "df['5_star_percentage'] = df['5 stars'] / df['num_ratings']\n",
    "coefficients = df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "df['a'], df['b'], df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "\n",
    "# Pre-processing columns for rankings\n",
    "df['num_ratings_ln'] = np.log1p(df['num_ratings'])\n",
    "df['num_pages_ln'] = np.log1p(df['num_pages'])\n",
    "df['2a_shifted'] = df['a'] - df['a'].min()\n",
    "df['2a_shifted'] = df['2a_shifted'] * (1 / df['2a_shifted'].max()) + 1\n",
    "df['b_shifted'] = df['b'] - df['b'].min()\n",
    "df['b_shifted'] = df['b_shifted'] * (1 / df['b_shifted'].max()) + 1\n",
    "df['c_shifted'] = df['c'] - df['c'].min()\n",
    "df['c_shifted'] = df['c_shifted'] * (1 / df['c_shifted'].max()) + 1\n",
    "\n",
    "# Types of rankings\n",
    "df['num_adjusted_rating'] = df['average_rating'] - (df['average_rating'] - df['average_rating'].mean()) / df['num_ratings_ln']\n",
    "df['coeff_2a_rating'] = (df['num_adjusted_rating'] * df['2a_shifted'])\n",
    "df['coeff_b_rating'] = (df['num_adjusted_rating']) / (df['b_shifted'])\n",
    "df['coeff_c_rating'] = (df['num_adjusted_rating'] * df['c_shifted'])\n",
    "df['joined_rating'] = (df['num_adjusted_rating'] * df['c_shifted'] * df['2a_shifted']) / df['b_shifted']\n",
    "df['final_rating'] = df['joined_rating'] - (df['joined_rating'] - df['joined_rating'].mean()) / df['num_ratings_ln']\n",
    "\n",
    "df['num_adjusted_page_rating'] = df['num_adjusted_rating'] / (df['num_pages_ln'])\n",
    "df['coeff_2a_page_rating'] = df['coeff_2a_rating'] / df['num_pages_ln']\n",
    "df['coeff_b_page_rating'] = df['coeff_b_rating'] / df['num_pages_ln']\n",
    "df['coeff_c_page_rating'] = df['coeff_c_rating'] / df['num_pages_ln']\n",
    "df['joined_page_rating'] = df['joined_rating'] / df['num_pages_ln']\n",
    "df['final_page_rating'] = df['joined_page_rating'] - (df['joined_page_rating'] - df['joined_page_rating'].mean()) / df['num_ratings_ln']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "numeric_cols = ['age', 'num_pages', 'num_pages_ln', 'num_ratings', 'num_ratings_ln', 'num_reviews', 'my_rating', 'average_rating', '1 star', '2 stars', '3 stars', '4 stars', '5 stars', '1_star_percentage', '2_star_percentage', '3_star_percentage', '4_star_percentage', '5_star_percentage', 'a', 'b', 'c', 'num_adjusted_rating', 'coeff_2a_rating', 'coeff_b_rating', 'coeff_c_rating', 'joined_rating', 'final_rating', 'num_adjusted_page_rating', 'coeff_2a_page_rating', 'coeff_b_page_rating', 'coeff_c_page_rating', 'joined_page_rating', 'final_page_rating']\n",
    "corr_df= df[numeric_cols].corr()\n",
    "\n",
    "plt.figure(figsize=(20, 15)) \n",
    "sns.heatmap(corr_df, annot=True, cmap='coolwarm', linewidths=0.5) \n",
    "plt.title('Correlation Heatmap') \n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fresh = df.sort_values(by='final_page_rating', ascending=False).reset_index().drop('index', axis=1)\n",
    "fresh = fresh[fresh['my_rating'].isna()]\n",
    "fresh[['Fiction' in genre_list for genre_list in fresh['genres']]] # Fiction, Nonfiction, Memoir, Classics, History, Politics, Philosophy, Business"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test = pd.read_csv('books_data.csv')\n",
    "test#['author'].iloc[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
