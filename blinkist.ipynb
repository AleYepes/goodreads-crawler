{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from googlesearch import search\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import json\n",
    "from requests import get\n",
    "import os\n",
    "import bs4\n",
    "from datetime import datetime\n",
    "from time import sleep\n",
    "import traceback\n",
    "import string\n",
    "from unidecode import unidecode\n",
    "import re\n",
    "\n",
    "import jellyfish\n",
    "\n",
    "pd.set_option('display.max_rows', 100)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def scrape_book(book_id):\n",
    "    def get_series_url(soup):\n",
    "        series_elem = soup.find('h3', {'class': 'Text Text__title3 Text__italic Text__regular Text__subdued'})\n",
    "        if series_elem:\n",
    "            series_slug = series_elem.find('a')['href'].split('/')[-1]\n",
    "            return series_slug\n",
    "        else:\n",
    "            print('    Series link not found')\n",
    "\n",
    "\n",
    "    def get_title(soup):\n",
    "        title_elem = soup.find('h1', {'class': 'Text Text__title1'})\n",
    "        if title_elem:\n",
    "            title = ' '.join(title_elem.text.split())\n",
    "            if title:\n",
    "                return title\n",
    "            else:\n",
    "                print('    Title not found')\n",
    "\n",
    "    def get_genres(soup):\n",
    "        genre_elems = soup.find_all('span', {'class': 'BookPageMetadataSection__genreButton'})\n",
    "        if genre_elems:\n",
    "            genres = []\n",
    "            for span in genre_elems:\n",
    "                genre = span.find('span', {'class': 'Button__labelItem'}).text\n",
    "                if genre != '...more':\n",
    "                    genres.append(genre)\n",
    "            return genres\n",
    "        else:\n",
    "            print('    Genres not found')\n",
    "\n",
    "    def get_script_data(soup):\n",
    "        try:\n",
    "            script_tag = soup.find('script', {'type': 'application/ld+json'})\n",
    "            data = json.loads(script_tag.string)\n",
    "            ratingCount = data.get('aggregateRating', {}).get('ratingCount', np.nan)\n",
    "            reviewCount = data.get('aggregateRating', {}).get('reviewCount', np.nan)\n",
    "            ratingValue = data.get('aggregateRating', {}).get('ratingValue', np.nan)\n",
    "        except AttributeError:\n",
    "            print(f\"    Script tag not found\")\n",
    "            ratingCount = np.nan\n",
    "            reviewCount = np.nan\n",
    "            ratingValue = np.nan\n",
    "\n",
    "        return {'num_ratings':          ratingCount,\n",
    "                'num_reviews':          reviewCount,\n",
    "                'average_rating':       ratingValue}\n",
    "\n",
    "    def get_rating_distribution(soup):\n",
    "        rating_bar_elems = soup.find_all('div', {'class': 'RatingsHistogram__bar'})\n",
    "        if rating_bar_elems:\n",
    "            distribution_dict = {}\n",
    "            for bar in rating_bar_elems:\n",
    "                star_label = bar['aria-label']\n",
    "                num_ratings = bar.find('div', {'class': 'RatingsHistogram__labelTotal'}).text.split(' ')[0]\n",
    "                num_ratings = int(num_ratings.replace(',', ''))\n",
    "                distribution_dict[star_label] = num_ratings\n",
    "            return distribution_dict\n",
    "        else:\n",
    "            print('    No rating bars found')\n",
    "\n",
    "    def get_num_pages(soup):\n",
    "        pages_elem = soup.find('p', {'data-testid': 'pagesFormat'})\n",
    "        if pages_elem:\n",
    "            try:\n",
    "                num_pages = int(pages_elem.text.split(' ')[0])\n",
    "            except Exception:\n",
    "                print('    No page number found')\n",
    "                num_pages = np.nan\n",
    "        else:\n",
    "            print('    No page number found')\n",
    "            num_pages = np.nan\n",
    "        return num_pages\n",
    "    \n",
    "    def get_author(soup):\n",
    "        author_elem = soup.find('span', {'class': 'ContributorLink__name'})\n",
    "        if author_elem:\n",
    "            author = author_elem.text.strip()\n",
    "        else:\n",
    "            print('    No author found')\n",
    "            author = np.nan\n",
    "        return author\n",
    "    \n",
    "    def get_year(soup):\n",
    "        year_elem = soup.find('p', {'data-testid': 'publicationInfo'})\n",
    "        if year_elem:\n",
    "            year = year_elem.text.split(',')[-1]\n",
    "        else:\n",
    "            print('    Year not found')\n",
    "            year = np.nan\n",
    "        return year\n",
    "\n",
    "    \n",
    "    url = f'https://www.goodreads.com/book/show/{book_id}'\n",
    "    source = get(url, timeout=30).text\n",
    "    soup = bs4.BeautifulSoup(source, 'html.parser')\n",
    "\n",
    "    return {**{'book_id': book_id,\n",
    "            \"series\": get_series_url(soup),\n",
    "            'title': get_title(soup),\n",
    "            'genres': get_genres(soup),\n",
    "            'author': get_author(soup),\n",
    "            'num_pages': get_num_pages(soup),\n",
    "            'year': get_year(soup)},\n",
    "            **get_script_data(soup),\n",
    "            **get_rating_distribution(soup)}\n",
    "\n",
    "\n",
    "def condense_books(books_directory_path):\n",
    "    books = []\n",
    "    for file_name in os.listdir(books_directory_path):\n",
    "        if f'_metadata.json' in file_name:\n",
    "            book = json.load(open(f'metadata/{file_name}', 'r'))\n",
    "            books.append(book)\n",
    "    return books\n",
    "\n",
    "\n",
    "def delete_metadata():\n",
    "    directory = './metadata/*metadata.json'\n",
    "    files = glob.glob(directory)\n",
    "    for f in files:\n",
    "        os.remove(f)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/blinkist_scraped.csv')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = os.listdir('/Users/alex/Downloads/Blinkist August 2023 SiteRip Collection - BASiQ')\n",
    "list = [name.split('.m4a')[0] for name in list]\n",
    "dir_df = pd.DataFrame([name.split(' - ') for name in list], columns=['authors', 'titles'])\n",
    "dir_df['text_id'] = dir_df['authors'] + ' - ' + dir_df['titles']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_punctuation(text):\n",
    "    return text.translate(str.maketrans('', '', string.punctuation))\n",
    "\n",
    "def remove_accents(text):\n",
    "    return unidecode(text)\n",
    "\n",
    "def remove_parentheses(text):\n",
    "    return re.sub(r'\\(.*?\\)', '', text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['query'] = df['titles2'] + ' by ' + df['authors2'] + ' goodreads.com book'\n",
    "jaro_threshold = 0.8\n",
    "\n",
    "toggle = True\n",
    "while True:\n",
    "    scrape_count = 0\n",
    "    toggle = not toggle\n",
    "    try:\n",
    "        rest = df[df['book_id'] == 0]\n",
    "        if len(rest) == 0:\n",
    "            break\n",
    "        counter = 1\n",
    "        for idx, row in rest.sort_values(by=\"authors\", ascending=toggle).iterrows():\n",
    "            query = row['query']\n",
    "            print(f\"Searching for: {query} ({counter}/{len(rest)})\")\n",
    "            counter += 1\n",
    "            start = datetime.now()\n",
    "            for result in search(query, sleep_interval=5, advanced=True, num_results=50):\n",
    "                print(f'    {result.url}')\n",
    "                if result.url.startswith('https://www.goodreads.com/') and '/book/' in result.url:\n",
    "                    title = result.title\n",
    "                    title = remove_punctuation(remove_accents(title.lower().strip()))\n",
    "                    file = row['titles'] + ' ' + row['authors']\n",
    "                    file = remove_punctuation(remove_accents(file.lower().strip()))\n",
    "                    jaro = jellyfish.jaro_similarity(file, title)\n",
    "                    print(f'    \"{title}\", \"{file}\": {jaro}')\n",
    "                    \n",
    "                    if jellyfish.jaro_similarity(file, title) > jaro_threshold:\n",
    "                        try:\n",
    "                            book_id = result.url.split('/')[-1]\n",
    "                            book_id = book_id.split('-')[0]\n",
    "                            book_id = book_id.split('.')[0]\n",
    "\n",
    "                            book = scrape_book(book_id)\n",
    "                            if book:\n",
    "                                json.dump(book, open(f'metadata/{book_id}_metadata.json', 'w'))\n",
    "                                df.at[idx, 'book_id'] = int(book_id)\n",
    "                                scrape_count += 1\n",
    "                            else:\n",
    "                                print(f'    scrape_book() returned empty')\n",
    "                            break\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            traceback.print_exc()\n",
    "            # sleep(1)\n",
    "            print(f'    {datetime.now() - start}')\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        pass\n",
    "    df.to_csv('data/blinkist.csv', index=False, encoding='utf-8')\n",
    "\n",
    "    if scrape_count == 0 and toggle == True:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('data/blinkist.csv', index=False, encoding='utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/blinkist.csv')\n",
    "books = condense_books('metadata')\n",
    "book_df = pd.DataFrame(books)\n",
    "book_df.book_id = book_df.book_id.astype(int)\n",
    "\n",
    "blinkist = df.merge(book_df, on='book_id', how='left')\n",
    "blinkist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in ['author', 'authors', 'title', 'titles']:\n",
    "    blinkist[f'{col}2'] = blinkist[col].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n",
    "\n",
    "blinkist['title2'] = blinkist['title2'].apply(lambda x: x.split(':')[0] if isinstance(x, str) else x)\n",
    "\n",
    "for col in ['title2', 'titles2']:\n",
    "    blinkist[col] = blinkist[col].apply(lambda x: remove_parentheses(x) if isinstance(x, str) else x)\n",
    "    blinkist[col] = blinkist[col].apply(lambda x: x.lower().strip() if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_authors(df, delimiters):\n",
    "    new_rows = []\n",
    "\n",
    "    for _, row in df.iterrows():\n",
    "        authors = row['authors2']\n",
    "        \n",
    "        if isinstance(authors, str):\n",
    "            for delimiter in delimiters:\n",
    "                if delimiter in authors:\n",
    "                    authors_split = authors.split(delimiter)\n",
    "                    for author in authors_split:\n",
    "                        new_row = row.copy()\n",
    "                        new_row['authors2'] = author.strip()  # Remove any leading/trailing spaces\n",
    "                        new_rows.append(new_row)\n",
    "                    break  # Exit the loop after processing the first valid delimiter\n",
    "            else:\n",
    "                new_rows.append(row)\n",
    "        else:\n",
    "            new_rows.append(row)\n",
    "\n",
    "    return pd.DataFrame(new_rows)\n",
    "\n",
    "delimiters = [' and ', ' with ', ' & ']\n",
    "for i in delimiters:\n",
    "    blinkist = split_authors(blinkist, delimiters)\n",
    "    for col in ['author2', 'authors2', 'title2', 'titles2']:\n",
    "        blinkist[col] = blinkist[col].apply(lambda x: remove_punctuation(remove_accents(x.lower().strip())) if isinstance(x, str) else x)\n",
    "        blinkist[col] = blinkist[col].apply(lambda x: ''.join(x.split(' ')) if isinstance(x, str) else x)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def jaro_similarity(str1, str2):\n",
    "    if pd.isna(str1) or pd.isna(str2):\n",
    "        return np.nan\n",
    "        \n",
    "    # t = jellyfish.levenshtein_distance(str1, str2)\n",
    "    # m = len(set(str1).intersection(set(str2)))\n",
    "    # if m == 0:\n",
    "    #     return 0.0\n",
    "    # else:\n",
    "    #     jaro = ((m/len(str1)) + (m/len(str2)) + ((m-t)/m))/3\n",
    "    #     return jaro\n",
    "    return jellyfish.jaro_similarity(str1, str2)\n",
    "\n",
    "blinkist['author_jaro'] = blinkist.apply(lambda row: jaro_similarity(row['author2'], row['authors2']), axis=1)\n",
    "blinkist['title_jaro'] = blinkist.apply(lambda row: jaro_similarity(row['title2'], row['titles2']), axis=1)\n",
    "blinkist['jaro'] = blinkist['author_jaro'] * blinkist['title_jaro']\n",
    "blinkist['jaro'] = blinkist['jaro'].fillna(0.0)\n",
    "blinkist['text_id'] = blinkist['authors'] + ' - ' + blinkist['titles']\n",
    "blinkist = dir_df.drop(['authors', 'titles'], axis=1).merge(blinkist, on='text_id', how='left')\n",
    "blinkist = blinkist.loc[blinkist.groupby('text_id')['jaro'].idxmax()]\n",
    "blinkist['book_id'] = blinkist['book_id'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = blinkist[~((blinkist['author2'] == blinkist['authors2']) & (blinkist['title2'] == blinkist['titles2']))][['book_id','authors', 'author', 'titles', 'title', 'authors2', 'author2', 'author_jaro', 'titles2', 'title2', 'title_jaro', 'jaro']]\n",
    "filtered_df[filtered_df['book_id'] != 0].sort_values(by='jaro').head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_df = blinkist[~((blinkist['author2'] == blinkist['authors2']) & (blinkist['title2'] == blinkist['titles2']))][['book_id','authors', 'author', 'titles', 'title', 'authors2', 'author2', 'author_jaro', 'titles2', 'title2', 'title_jaro', 'jaro']]\n",
    "filtered_df = filtered_df[filtered_df['jaro'] <= .4]\n",
    "filtered_df.sort_values(by='jaro', ascending=False)#.shape\n",
    "blinkist.loc[filtered_df.index, 'book_id'] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = blinkist[['book_id', 'authors', 'authors2', 'titles', 'titles2']].copy()\n",
    "df = df.drop_duplicates()\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "blinkist['num_ratings'] = blinkist['num_ratings'].fillna(0).round().astype(int)\n",
    "blinkist['num_reviews'] = blinkist['num_reviews'].fillna(0).round().astype(int)\n",
    "blinkist['num_pages'] = blinkist['num_pages'].fillna(blinkist['num_pages'].mean()).round().astype(int)\n",
    "blinkist['average_rating'] = ((blinkist['5 stars'] * 5) + (blinkist['4 stars'] * 4) + (blinkist['3 stars'] * 3) + (blinkist['2 stars'] * 2) + blinkist['1 star']) / blinkist['num_ratings']\n",
    "\n",
    "blinkist.isna().mean()\n",
    "blinkist.to_csv('data/blinkist_scraped.csv', index=False)\n",
    "df = blinkist.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/blinkist_scraped.csv')\n",
    "\n",
    "# Calculating quadrdic modeling coefficients\n",
    "df['1_star_percentage'] = df['1 star'] / df['num_ratings']\n",
    "df['2_star_percentage'] = df['2 stars'] / df['num_ratings']\n",
    "df['3_star_percentage'] = df['3 stars'] / df['num_ratings']\n",
    "df['4_star_percentage'] = df['4 stars'] / df['num_ratings']\n",
    "df['5_star_percentage'] = df['5 stars'] / df['num_ratings']\n",
    "\n",
    "df['5_star_percentage'] = df['5_star_percentage'].fillna(0)\n",
    "df['4_star_percentage'] = df['4_star_percentage'].fillna(0)\n",
    "df['3_star_percentage'] = df['3_star_percentage'].fillna(0)\n",
    "df['2_star_percentage'] = df['2_star_percentage'].fillna(0)\n",
    "df['1_star_percentage'] = df['1_star_percentage'].fillna(0)\n",
    "\n",
    "df.average_rating = df.average_rating.fillna(df.average_rating.mean())\n",
    "df.isna().mean()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def fit_quadratic(row):\n",
    "    x = np.array([1, 2, 3, 4, 5])\n",
    "    a, b, c = np.polyfit(x, row, 2)\n",
    "    return pd.Series([2*a, b, c])\n",
    "\n",
    "coefficients = df[['1_star_percentage','2_star_percentage','3_star_percentage','4_star_percentage','5_star_percentage']].apply(fit_quadratic, axis=1)\n",
    "df['2a'], df['b'], df['c'] = coefficients[0], coefficients[1], coefficients[2]\n",
    "\n",
    "# Pre-processing columns for rankings\n",
    "df['num_ratings_ln'] = np.log1p(df['num_ratings'])\n",
    "df['num_ratings_ln'] = df['num_ratings_ln'].replace(0,1)\n",
    "df['num_pages_ln'] = np.log1p(df['num_pages'])\n",
    "df['2a_shifted'] = df['2a'] - df['2a'].min()\n",
    "\n",
    "# Types of rankings\n",
    "df['num_adjusted_rating'] = df['average_rating'] - (df['average_rating'] - df['average_rating'].mean()) / df['num_ratings_ln']\n",
    "df['page_adjusted_rating'] = df['num_adjusted_rating'] / df['num_pages_ln']\n",
    "df['2nd_derivative_rating'] = df['num_adjusted_rating'] * df['2a_shifted']\n",
    "df['2nd_derivative_page_adjusted_rating'] = df['num_adjusted_rating'] * df['2a_shifted'] / df['num_pages_ln']\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['rating2'] = df['page_adjusted_rating'] + df['2nd_derivative_rating']\n",
    "df = df.sort_values(by='rating2', ascending=False).drop('index', axis=1).reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('data/blinkist_scraped.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df.rating2 > 1.7825086940916708]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df#.head(50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "directory = '/Users/alex/Downloads/Blinkist August 2023 SiteRip Collection - BASiQ'\n",
    "\n",
    "for filename in os.listdir(directory):\n",
    "    text_id = '_'.join(filename.split('_')[1:]).split('.m4a')[0]\n",
    "    \n",
    "    row = df[df['text_id'] == text_id]\n",
    "    \n",
    "    if not row.empty:\n",
    "        old_file = os.path.join(directory, filename)\n",
    "        \n",
    "        new_file = os.path.join(directory, f\"{row['index'].values[0]}_{text_id}.m4a\")\n",
    "        \n",
    "        os.rename(old_file, new_file)\n",
    "\n",
    "print(\"Files have been renamed successfully!\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "list = os.listdir('/Users/alex/Downloads/Blinkist August 2023 SiteRip Collection - BASiQ')\n",
    "list"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
